"{\"title\":\"\",\"courses\":[{\"course_title\":\"Python for Data Science and Machine Learning Essential Training Part 1\",\"course_admin_id\":3006708,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":3006708,\"Project ID\":null,\"Course Name\":\"Python for Data Science and Machine Learning Essential Training Part 1\",\"Course Name EN\":\"Python for Data Science and Machine Learning Essential Training Part 1\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"&lt;p&gt;&lt;i&gt;Python for Data Science and Machine Learning Essential Training&lt;/i&gt; is one of the most popular data science courses at LinkedIn Learning. It has now been updated and expanded to two parts-giving you even more hands-on, real-world Python experience. In part one, instructor Lillian Pierson takes you step by step through a data science and machine learning project: a web scraper that downloads and analyzes data from the web. Along the way, she introduces techniques to clean, reformat, transform, and describe raw data; generate visualizations; remove outliers; perform simple data analysis; and generate web-based graphs using Streamlit. By the end of this course, you'll have acquired basic coding experience that you can take to your organization and quickly apply to your own custom data science and machine learning projects.&lt;/p&gt;&lt;p&gt;This course is integrated with GitHub Codespaces, an instant cloud developer environment that offers all the functionality of your favorite IDE without the need for any local machine setup. With GitHub Codespaces, you can get hands-on practice from any machine, at any time-all while using a tool that you'll likely encounter in the workplace. Check out the Using GitHub Codespaces with this course video to learn how to get started.&lt;/p&gt;\",\"Course Short Description\":\"Learn Python programming skills for data science and machine learning. Discover how to clean, transform, analyze, and visualize data, as you build a practical, real-world project.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":8464985,\"Instructor Name\":\"Lillian Pierson\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Engineer, CEO, and Head of Product at Data-Mania\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2024-03-12T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/python-for-data-science-and-machine-learning-essential-training-part-1,https://www.linkedin.com/learning/python-for-data-science-and-machine-learning-essential-training-part-1-2024-revision,https://www.linkedin.com/learning/python-for-data-science-essential-training-part-1-2022-revision\",\"Series\":\"Essential Training\",\"Limited Series\":null,\"Manager Level\":\"General\",\"LI Level\":\"Intermediate\",\"LI Level EN\":\"Intermediate\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Artificial Intelligence for Technology\",\"Primary Software\":\"Python\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"No\",\"Visible Duration\":27863.0,\"Visible Video Count\":49.0,\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":285,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2715030\",\"duration\":41,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Data science life hacks\",\"fileName\":\"3006708_en_US_00_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":46,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2176232,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Lillian] Have you ever wanted to be able to copy\\nand paste a bunch of data off a website\\nor just get the gist\\nof all the content without actually having to read\\nthrough line by line?\\nIf so, then great,\\nbecause I'm going to show you how\\nto build a web scraper in Python so that you can have\\nthat data written off of the web for you automatically.\\nAnd I'm also going to be introducing you to a Python library\\nthat you can use to visualize that data in a standalone,\\nshareable web application using just a few lines of code.\\n\\nHi, I'm Lillian Pearson.\\nI'm a data and AI strategist with nearly three decades\\nof experience working with data.\\nLet's get going on Python for Data Science.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4589016\",\"duration\":59,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What you should know\",\"fileName\":\"3006708_en_US_00_02_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":103,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1295829,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] As far as what\\nyou should know for this course,\\nwe'll be programming in Python,\\nbut you don't need to have\\nany prior Python experience to take this course.\\nWe are going to be using Jupyter Notebooks\\nas our interactive programming environment.\\nIn case you've never worked with Jupyter Notebooks before,\\nthen, you can go check out\\nthe work with Jupyter Notebooks video,\\nin the course Python programming efficiently\\nin the LinkedIn Learning Library.\\n\\nDon't worry too much about the notebooks\\npart of this course, though,\\nbecause you'll be getting the course notebooks\\nwith the Codespaces environment.\\nMore on Codespaces in the next video, by the way.\\nLastly, as for the math and statistics requirements\\nfor this course, there are no\\nreal math or stats prerequisites.\\nI'm going to be taking you through everything\\nyou need to know during the course.\\nSo, let's get started.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4589017\",\"duration\":185,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"How to use Codespaces with this course\",\"fileName\":\"3006708_en_US_00_03_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":332,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6859569,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's look at how to configure code spaces.\\nTo begin head over to the courses repository\\nand then select this green code button.\\nSelect the Code Spaces tab,\\nand then click this plus icon\\nto create a code space on main.\\nThis action launches a new tab to prepare your code space,\\nwhich might take some time to complete,\\nso we'll revisit once it's ready.\\n\\nOkay, so as you can see, the Code Spaces is now launched,\\nand you'll find yourself in a web-based working environment\\nthat looks like this.\\nHow Code Spaces works is\\nthat essentially it initiates a virtual machine providing\\nyou a web interface with which to interact.\\nWithin the terminal here\\nyou're free to execute common Linux commands\\nand Python scripts.\\nWe'll be using the terminal\\nto build Streamlit applications in\\nchapter eight of this course.\\n\\nFor our course,\\nsimply open this folder here called Notebooks.\\nThis is the primary folder for the course,\\nbut just so you know, chapter eight scripts are housed in\\nthis folder here called Streamlit.\\nNow let me show you how to open a Jupyter Notebook.\\nLet's click on the 0403 iPython notebook file.\\n\\nDo a double click, and it opens up a Jupyter Notebook.\\nOnce you're inside the notebook, well,\\nfirst thing I like to do is just close this terminal\\nbecause we don't need it when we're working\\ninside of a notebook.\\nSo let's look at how\\nto run a cell within a Jupyter Notebook.\\nSo just click on this first code cell here\\nand press control, enter to execute it.\\nAnd it's connecting here to, you could see it was connecting\\nto a Python kernel,\\nand it's already run, so that's great.\\n\\nWhen you see this check mark here,\\nyou know that the code is finished running,\\nand this timestamp indicates how long it took\\nfor the program to run.\\nAnd this demo has now guided you\\nthrough opening a project on GitHub, creating a code space,\\nopening a Jupyter Notebook,\\nand executing it via code spaces.\\nI find Code Spaces to be a handy GitHub product.\\nIt simplifies project initiation, removing a lot\\nof the hassle involved in environment configuration.\\n\\nIt's particularly useful for those people\\nwho might be a little less familiar\\nwith Python's virtual environments.\\n\"}],\"name\":\"Introduction\",\"size\":10331630,\"urn\":\"urn:li:learningContentChapter:2714164\"},{\"duration\":1598,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4583158\",\"duration\":823,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introduction to the data professions\",\"fileName\":\"3006708_en_US_01_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":1050,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to orient yourself in the data world. This video covers the definition of data science, data engineering, and data analytics.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":21506363,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Narrator] Welcome to Introduction\\nto the Data Professions.\\nThe conversation we're about to have is really important\\nto people who are aspiring to become data scientists\\nbecause it's vital that you understand\\nwhere you will fit in within the data profession spectrum,\\nwhen you actually do level up\\nand become a trained data scientist.\\nThe thing about the data space in general\\nis that there are many, many different types of roles,\\nand although there may be similarities\\nbetween the objectives that these roles fill,\\nthe roles are actually quite different.\\n\\nSo when you're taking a training course in data science\\nand you aspire to become a data scientist,\\nyou really do need to have a fundamental idea\\nabout where you fit in and what your responsibilities\\nand requirements should be\\nwith respect to other data professionals.\\nSo you want to have that in mind\\nwhen you get placed in the field\\nin order that in case your employer\\nis asking you to do something\\nthat is outside the scope of data science per se,\\nyou want to know that actually you transitioning your career\\ninto a similar, yet competing field.\\n\\nSo first, let's just talk about the functions\\nwithin the data space.\\nI've broken these down into data science,\\ndata engineering, data analytics, and business intelligence.\\nThese four functions work together\\nto generate business value from data.\\nWhat I really encourage you to do\\nif you're just getting started on your path\\ntowards becoming a data professional,\\nis to focus on mastering one area\\nand then leaning in on your peers\\nthat are data professionals in the other areas.\\nSo don't try and master all of these areas all at one time\\nbecause it will really impede your progress.\\n\\nIt will make it slow\\nfor you to progress down one career path,\\nand it's also going to make it confusing for you.\\nSo this section of the course\\nis really to help you compartmentalize\\nwhere you fit in\\nand what this course is actually preparing you to do,\\nwhich is data science.\\nIf you really want to learn how to become a professional\\nthat serves more than one of these functions,\\nthen I at least suggest\\nthat you master each function one at a time\\nand not try and master them all at the same time\\nbecause that's just going to make it harder for you\\nand result in you making slower progress\\nin your skill development.\\n\\nNow, let's talk about why we need clarity\\non this subject in and of itself.\\nSee, you're really the one who gets to decide\\nwhere you fit in best within a data profession.\\nSo I want to make sure that you are prepared\\nand that you can be deliberate in choosing\\nwhat you actually want to do\\nand not just what you happen to fall into\\nor what you happen to pick up\\nby basically being forced\\nto be the jack of all trades, right?\\nSo I'm trying to give a choice here\\nand a vision whereabouts\\nyou can be deliberate in the advancement of your career\\nas a data professional.\\n\\nI want you to be fully aware\\nof what you're actually signing up for\\nbefore you decide to move further\\ninto the data science space\\nor become a data scientist.\\nAnd there's lots of good news here.\\nThere's plenty of demands for skills in all of these areas,\\nand there are emerging roles\\nwithin the data space all of the time.\\nSo you're going to have opportunity as a data professional,\\nbut we do want to have clarity here\\nbecause basically less confusion in your mind\\nequals more results.\\n\\nAnd I want to make it easier for you to make this transition\\nto the data professions.\\nThat's why I'm clarifying this topic for you\\nso you can get results faster.\\nThis is a course on data science.\\nSo naturally we'll start with the data science rule.\\nAnd what data science really is\\nit's a systematic study of the structure\\nand behavior of data\\nin order to quantifiably understand past\\nand current occurrences\\nas well as to predict the future behavior of data.\\n\\nLet's look at some of the traits\\nof a typical data scientist.\\nData scientists generally should have a degree\\nin a quantitative field.\\nSo if you are already having a STEM degree,\\nit's going to be a lot easier for you\\nto transition into a data science role\\ncompared to if you are of a liberal arts background\\nor something like that.\\nAnother thing is that data scientists are programmers,\\nso they should really know how to program in R and Python,\\nand also how to write SQL queries, things like that.\\n\\nData scientists, by personality, well they're curious\\nand they're tenacious individuals,\\nso they're willing to really stick it out\\nuntil they get the findings and results that they need.\\nIf you have a background\\nin business intelligence reporting, data analysis,\\nor data-driven decision making,\\nthen you're a good candidate\\nfor looking into maybe stepping up\\ninto a data scientist role.\\nGenerally, data scientists are interested in why over how.\\n\\nSo they're looking for the patterns and correlations\\nand the trends and that basically indicate\\nwhy something is happening, not how it's happening.\\nThe reason for that\\nis that they're looking to make predictions.\\nSo once you understand why something is happening\\nthe way it's happening,\\nyou can then make predictions based on that why.\\nData scientists are known to spend hours and hours,\\nif not days, analyzing complex questions.\\nSo if you're a people person\\nand like to spend most of your time consulting\\nand working with people,\\nthen becoming a data scientist\\nmight not be the most fulfilling thing for you.\\n\\nNow, let's look at some of the typical tasks\\nthat data scientists are required to do.\\nData scientists in general derive insights from data,\\nincluding big data sets.\\nSo if you've been around in the field for a while,\\nthen you probably know what big data is.\\nIt's actually a term that's decreasing in popularity.\\nBut big data just means data of many different structures,\\nmany different velocities and many different varieties.\\nAnd a data scientist's job is to derive insights from data.\\n\\nThat data is probably going\\nto be sitting in a data engineered system.\\nAnd when I say data engineered system,\\nI mean if you're working on data\\nthat's being derived from big data,\\nthen it's probably going to be in a more innovative system\\nthan just the traditional\\nrelational database management system.\\nAlthough you may also be pulling data\\nfrom relational databases,\\njust like statisticians and business intelligence people\\nhave been doing for many, many decades.\\n\\nThe next type of task that data scientists do\\nis that they uncover correlations\\nand causations in business data\\nto support business decision making.\\nSo what this is actually called is decision support,\\nand the name makes sense in light of the task\\nassociated with that requirement.\\nAnother typical task that data scientists do\\nis they generate predictions from data\\nand communicate those predictions\\nthrough data visualization.\\nIn terms of competencies\\nthat are generally required of data scientists,\\nyou should have a STEM degree\\nwith some advanced mathematics.\\n\\nSo if you finish your calculus\\nand have gone into differential equations,\\nthat would be a good setup.\\nYou will also need to take fundamental statistics,\\nbut if you've gone into linear algebra\\nand these types of topics during your college education,\\nthen you're going to be in a good place\\nto try and move into the data science role.\\nData scientists also need to know how to code,\\nso they should know how to code\\nin languages like Python, R, and SQL.\\nThey should know how to use coding\\nto implement machine learning algorithms.\\n\\nSo these are basic typical competencies\\nthat are required of data scientists.\\nAnd just to put things into perspective,\\nthis course is Python for data science.\\nSo we'll be discussing some basic math,\\nand we'll be doing a high level overview\\nof machine learning algorithms\\nand the math that goes into them.\\nYou're going to be learning to use Python\\nto implement these.\\nThis is a coding course that's going to be teaching you\\nhow to implement data science methodologies.\\n\\nNow that you know what a data scientist is,\\nlet's compare that to a data engineer.\\nData engineering is the design,\\nconstruction and maintenance of data systems.\\nAs far as typical traits, data engineers are programmers,\\nthey're good at math, but honestly,\\nthey don't use advanced statistics\\nand really advanced mathematics very often.\\nTheir previous experience is likely to be something\\nlike working as a database architect\\nor an ETL developer,\\nwhich is extract, transform, and load.\\n\\nIt's just a process which is used to move data\\nfrom one storage system to another.\\nOther rules they may have had in the past\\nwould be database developer or security architect.\\nSo people that work as data engineers,\\nthey prefer to design\\nand build IT systems rather than to analyze data.\\nSo it's really a shift in scope.\\nData engineers are generally interested more\\nin how over why,\\nbecause they're actually building the systems\\nthat hold the data,\\nthat data scientists analyze,\\nand so they really need to build reliable systems.\\n\\nThey need to focus on how to actually get that done,\\nand they're not as much interested in uncovering the whys\\nthat are contained within that data.\\nObviously, you can see there's a bit of a scope shift there.\\nIn terms of typical tasks,\\ndata engineers design systems that collect, handle,\\nand store big data sets.\\nThey work to build modular,\\nscalable platforms for data processing,\\nand they also design,\\nbuild and maintain systems that store\\nand move big data and regular types of data as well.\\n\\nIn terms of typical competencies, people that work\\nas data engineers generally should have\\ngood computer science background,\\na background in software engineering.\\nAnd then in terms of coding requirements,\\nthey should generally know languages\\nlike Java, C++, and Python.\\nAnd moving into the data analytics professionals.\\nFirst, let's start off by what is data analytics?\\nData analytics are data products\\nthat describe data and how it behaves.\\n\\nSo these data products are generated\\nfrom data analysis and visualization processes,\\nand the people that are generally functioning\\nwithin a data analytics role would be titled something\\nlike analytics specialist or analytics expert.\\nThe role of analytics professionals\\nhas evolved significantly over the past few years.\\nIn 2023, analytics professionals are expected\\nto have a strong understanding of data science,\\nmachine learning, and artificial intelligence.\\n\\nThey're also expected to be proficient\\nin coding languages like Python and R\\nand be able to use advanced tools and platforms.\\nThey need to have a solid background in math and statistics,\\nand they also need to understand\\nthe business context in which they're working.\\nThey're often responsible\\nfor translating complex data insights\\ninto actionable business tactics and strategies.\\nIn terms of typical tasks,\\nanalytics specialists generate predictive,\\nprescriptive, and descriptive data insights.\\n\\nAnd of course, right now you are probably not exactly clear\\non what those terms actually mean,\\nbut don't worry because we're going to cover that later\\nin this course.\\nJust know that analytics specialists\\nuncover correlations and causations in business data\\nto support business decision making.\\nSo yes, they operate in a decision support function.\\nHowever, they're oftentimes achieving this outcome\\nby using applications.\\n\\nThey're often called to deploy analytics technologies\\nand software applications to generate their findings\\nand then return those insights\\nto business decision makers\\nin order to help them lead their organization\\nand become more data driven.\\nThe challenge for you in this section\\nis that I want you to sit back\\nand really think about yourself\\nand your background and your true passions,\\nand then decide which of these fields\\nreally sounds like the best fit\\nfor what you actually love doing.\\n\\nBecause you're going to thrive\\nif you are operating in a capacity\\nthat is the best fit for your personality,\\nfor your passion, and for your interests, right?\\nSo I want to encourage you to place yourself\\nin one of these three roles that we outlined here,\\nand then go ahead and just pursue that.\\nLet that be your focus for this journey in your career.\\nHopefully you will find\\nthat you are interested in pursuing the data science path\\nbecause this course is Python for data science,\\nand you're going to learn a lot about Python\\nand using it to implement data science\\nover the next several hours.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4588022\",\"duration\":297,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Data science careers: Identifying where and how you'll thrive\",\"fileName\":\"3006708_en_US_01_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":502,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about three main types of data science careers, as well as info on how data science is creating remarkable changes in all industries, especially the software industry.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13100151,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Narrator] For someone taking\\nan entry-level data science course,\\nit would be easy to assume\\nthat once you've developed data science skills,\\nyou will then become a data analyst or a data scientist.\\nAnd for some people that might be true,\\nbut oftentimes, people with data science skills\\ndon't actually work as data analysts or data scientists.\\nIn fact, by 2023, over 50 types of roles\\nhad begun to require applicants to have data science skills.\\n\\nThe fact of the matter is\\nthere are really three different types\\nof data science professional,\\nand where you fit in with these really depends a lot\\non your passion and unique talents.\\nAs I see it, all data science professionals\\ncan be classified as either a data implementer,\\na data leader or a data entrepreneur.\\nBut the long and the short of it is\\ndespite the surging demand,\\nnot everyone should seek to become a data scientist.\\n\\nIn fact, pursuing that goal\\nwhen the data scientist role is not aligned\\nwith your personality is\\nsetting yourself up for disappointment.\\nSpeaking of data scientists though,\\nlet's see where they fit in\\nwithin the three-tiered ecosystem\\nof data science professionals.\\nAs you can see here, the data scientist position\\nis classified with other implementation focused roles\\nlike machine learning engineer and enterprise architect.\\n\\nData implementers are the people we can thank\\nfor coding up all of these amazing applications\\nthat make our lives today so much easier\\nand so much more interesting.\\nImplementers love to code and focus on details.\\nThey're generally happy to code on their own all day long,\\nso long as that means that they don't have to talk to\\nor interact with other people all that much.\\nA recent market research shows that data implementers\\ngenerally earn anywhere between $60,000 to $120,000 per year\\nin salary in the United States.\\n\\nBut what if you love working with people\\nand you really need to see that big picture impact\\nof your work in order to be satisfied\\nin your professional life?\\nNot to worry.\\nPeople like this are just as needed and wanted\\nin the data science space as the data implementers.\\nThey're called data leaders.\\nData leaders are the people who have data science skills\\nand are responsible for leading teams\\nand project stakeholders through the process\\nof building successful data solutions.\\n\\nData science leaders often hold titles like\\nanalytics program manager, data product manager\\nand continuous improvements manager,\\nand their salaries average from $45,000 to $110,000 per year\\nacross the US in general.\\nAnd if neither of these sounds like you,\\nthen you may be a good, old-fashioned data entrepreneur.\\n\\nOf course, as an entrepreneur,\\nyou can be either a data implementer\\nor a data leader in your own business.\\nAnd how you function within the business depends on\\nwhat you want to do since it's your business.\\nData entrepreneurs are distinguished\\nfrom their counterparts\\nby their desire for creative autonomy,\\nfinancial autonomy and their higher risk tolerance.\\nOf course, entrepreneurs who run their own business\\ngenerally hold the title of founder or CEO,\\nbut for people who are just starting off as entrepreneurs,\\nthey generally need to do services work for a while\\nuntil they can build out a more scalable business model.\\n\\nNew data science freelancers would fall into this category\\nand are often offering machine learning services,\\ndata strategy services, or even just data analysis services.\\nFor new entrepreneurs who are just starting off\\nas data science freelancers,\\nthey're often earning between $45 and $115 per hour\\nin the United States.\\nBut one of the nice things about having a business\\nis that the growth potential is pretty open-ended.\\nI mean, look at Facebook and how much money and impact\\nthat Mark Zuckerberg had with that platform.\\n\\nAt its core, Facebook is a data company.\\nNow that you know about the different types of options\\navailable to you with data analysis\\nand data science skills,\\nlet's look at why Python is such a great\\nprogramming language for analyzing data.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2714159\",\"duration\":384,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Why to use Python for analytics\",\"fileName\":\"3006708_en_US_01_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":554,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to justify your choice in using Python for data science. This video covers an intro to Python, Python for analytics, and Python in big data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10046349,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's talk about why it's a good idea\\nto use Python for working with data\\ninstead of any of the other competing coding languages\\nor methodologies you could use to implement data science.\\nFirst of all, let's look at\\nwhat are your alternatives to Python?\\nFor data science, of course, you can use Python,\\nwhich is the focus of this course,\\nbut you could also use R, Julia,\\nand a program called Go, short for Golang.\\n\\nLooking into the definition of Python,\\nwhat is the Python programming language?\\nWell, Python is a high-level interpreted coding language\\nthat's useful for a wide variety of applications,\\nand it's the official programming language of Google.\\nThe benefits to using Python are\\nthat it's extremely easy to learn and it's human readable.\\nIt's got an extensive array\\nof well-supported data science libraries,\\nand it's got the biggest user base\\nof all data science languages.\\n\\nAlso, it's useful in data engineering\\nas well as data science.\\nYou can use it\\nfor building predictive web applications as well.\\nIt's basically quite extensible, and you can use it\\nfor a lot of different applications areas,\\nnot just data science.\\nAlso, another thing that's preferable about Python is\\nthat it's an extremely popular coding language.\\nAs of 2023, Python is the most popular programming language\\nof all programming languages around.\\n\\nIn the TIOBE Index, a measure\\nof the programming language's popularity,\\nPython ranks first,\\nwhereas the ubiquitous querying language SQL\\nis ranked at position nine in popularity.\\nR is another widely used language, especially in the fields\\nof data science and research and statistics.\\nBut as of October 2023, R actually ranks\\nas 17th in the TIOBE Index.\\n\\nAnd Go is ranked at the 11th place.\\nJulia, which is a relatively new language\\nthat came out in 2018, has really been growing\\nin popularity, and as of October, 2023,\\nJulia ranks 28th in the TIOBE Index.\\nSo if you're using Python for data science,\\nyou're just going to be a lot better off,\\nbecause when you get stuck, there's going to be a lot\\nof people out there that are getting stuck\\nin the exact same way as you are, and it'll be easy for you\\nto find solutions on Stack Overflow.\\n\\nAlso, the libraries in Python are super well supported\\nbecause it's a popular language.\\nIt's the most popular language\\nof all coding languages out there.\\nAnd so these are just some of the advantages\\nof learning Python for data science\\nrather than R or its alternatives.\\nHere's a Google Trends screenshot\\nthat shows you the difference between Python\\nfor data science in blue and R for data science in red.\\nAs you can see, R for data science has never been able\\nto keep up in popularity with Python for data science.\\n\\nThe nice thing about Python that makes it so desirable,\\nand this is probably why most people are pursuing Python\\nfor data science, is that it's also popular\\nfor data engineering.\\nAs you can see here in blue, that is the search trends\\nfor Python for data engineering.\\nHonestly, more people are searching Python data engineering\\nthan they are Java data engineering.\\nPython's very, very popular for data science,\\nbut people are also interested in working\\nto build out its functionality\\nwith respect to data engineering as well.\\n\\nSo if you know Python, it's good for you\\nbecause you can do data science\\nand then you can expand into data engineering,\\nand then you could get into machine learning engineering,\\nanother role we'll talk about later in this course,\\nbut basically learning Python first is\\njust a really well-rounded decision\\nand will be better for you and your long-term future.\\nNow, let's talk about why use Python for working with data.\\nPython is useful for data science, data analytics,\\nand data engineering, like we just discussed,\\nbut it's also useful in a professional academic environment.\\n\\nPython is an open-source programming language,\\nand so you can use it for web development,\\napplication development, and heck,\\nyou can even use it to build out your own games.\\nSo it's very open-ended.\\nIf you know how to use Python, then you can function\\nin a lot of different types of tech roles.\\nYou don't necessarily need to stay in the data professions\\nfor the rest of your life and you could branch off\\ninto many, many different types of roles.\\n\\nThe same is not true of other programming languages\\nlike R, Julia and Go.\\nThose languages are more likely to keep you locked down\\nand limited to working in a data science capacity,\\nnot extending much beyond that.\\nAnd that's just another reason\\nthat I love Python for data science.\\nNow, in terms of the main libraries that are used\\nwith Python for data science, for advanced data analysis,\\nyou'd use NumPy, SciPy, and pandas,\\nand then for data visualization,\\nthe most widely used libraries are Matplotlib and Seaborn.\\n\\nFor machine learning, in this course,\\nwe'll be using scikit-learn, but if you venture\\ninto deep learning later on\\nin your data science learning adventures,\\nthen you would probably be using TensorFlow,\\nKeras, or PyTorch.\\nIn this course, you're going to be learning how\\nto use NumPy, SciPy, pandas, Matplotlib,\\nSeaborn, and scikit-learn.\\nWe're not covering TensorFlow, Keras or PyTorch\\nbecause those are deep learning libraries\\nand we're only covering data analysis,\\ndata visualization, and machine learning in this course.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4583159\",\"duration\":94,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"High-level course road map\",\"fileName\":\"3006708_en_US_01_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":116,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Get oriented with this course. This video covers a roadmap for data preparation, data visualization, and the math requirements for analytics.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2545971,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now that you know where data science fits in\\nwithin the spectrum of data roles\\nand you understand the basics of the data professions,\\nlet me just give you a quick roadmap\\nfor what we're about to cover\\nin the remainder of this course.\\nSo we are here now.\\nWe've just finished up Introduction to Data Professions,\\nand then in the next section,\\nwe're going to talk about data preparation basics.\\nThen we're going to go into the fundamentals\\nof data visualization\\nin a section that's called Data Visualization 101.\\n\\nAfter that, we'll be moving\\ninto practical data visualization\\nwhere you'll be learning\\nto create data visualizations for yourself.\\nNext, we'll be covering exploratory data analysis,\\nalso called EDA.\\nAnd after you've grasped the EDA basics,\\nwe'll move into Getting Started with Machine Learning.\\nAnd then after that,\\nwe'll be moving into data sourcing via web scraping.\\nSo I'll be teaching you how to generate data sets\\nby basically going out onto the internet and scraping data\\nand generating data sets you can use for making predictions.\\n\\nAnd then lastly in this course,\\nwe'll be talking about\\nbuilding collaborative analytics with Streamlit.\\nSo I'm going to be teaching you\\nhow to create interactive data visualizations\\non the internet that you can share across your organization\\nand also use for business decision support.\\nNow that you understand where we're headed,\\nlet's get started with data preparation basics.\\n\"}],\"name\":\"1. Introduction to the Data Professions\",\"size\":47198834,\"urn\":\"urn:li:learningContentChapter:4590008\"},{\"duration\":3953,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4588023\",\"duration\":200,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Intro to data preparation\",\"fileName\":\"3006708_en_US_02_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":291,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about what role data preparation plays in the data science project lifecycle and its significance.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5190004,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Preparing your data for analysis is\\none of the most resource intensive requirements\\nin data science.\\nIn fact, the general consensus is that data scientists\\nspend 80% of their time on data preparation.\\nThat means the better and more efficient\\nyou become in data preparation,\\nthe more likely it is\\nyou'll be effective as a data scientist.\\nLet's look at where data preparation falls\\nwithin the typical data analytics project life cycle.\\n\\nThe data analytics project life cycle is pretty simple.\\nIt starts off with evaluation,\\nthen you move into data preparation,\\nthen analysis and model building.\\nNext implementation and then communication.\\nThere are six main steps involved in data preparation.\\nThose are importing data, cleaning data,\\ntransforming data, processing data, logging data,\\nand then backing up data.\\nThe first step is always\\nto import the data you want to work with\\ninto your programming environment or application.\\n\\nAnd step two is cleaning data,\\nwhich involves removing duplicates\\nand removing out of range records,\\nremoving stray characters, and standardizing casing.\\nThe third step is transforming data.\\nAnd that involves treating missing values\\nand scaling and normalizing variables.\\nIt's really important to scale your data\\nbecause you need to make sure that\\ndiffering magnitudes among the variables in your data sets\\ndo not produce erroneous or misleading statistics.\\n\\nAnd this is a basic step\\nin preparing your data for machine learning.\\nThere are two ways to scale your data.\\nOne is normalization and the other is standardization.\\nNormalization is where you put each observation\\non a relative scale between values of zero and one.\\nAnd this is where you would divide\\nthe value of the observation\\nby the sum of all observations in a variable.\\nThe other alternative is standardization,\\nwhich involves rescaling data\\nso that it has a zero mean and unit variance.\\n\\nStep four is processing data.\\nAnd that generally involves parsing data,\\nrecoding data, and formatting data.\\nAnd step five is logging your data.\\nAnd this is where you would generate descriptive statistics,\\nlog your variable information,\\nand store your variable information.\\nIn terms of what type of information you need\\nto log about your variables,\\nI would consider detailing your variable name\\nand statistical description,\\nthe format of the data,\\nthe method used to collect the data,\\nthe date of the data collection, the source of the data,\\nthe location, where the data is stored,\\nand any other notes that you feel are relevant\\nto your dataset.\\n\\nLastly, you'd be backing up your data,\\nwhich just involves creating a backup copy of your data\\nand then taking it over to begin data analysis.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4589018\",\"duration\":243,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Numpy and pandas basics\",\"fileName\":\"3006708_en_US_02_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":380,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Get introduced to the two most fundamental Python libraries for data science: numpy and pandas. Learn the numpy basics of arrays, vectorized operations, indexing, types and efficient ufuncs. Learn the pandas basics on series, dataframe, data reading, and saving.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6584805,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] For embarking on any sort of\\ndata analysis journey in Python,\\nyou need to be very clear on NumPy and Pandas basics.\\nThe NumPy library is a third party Python library\\nthat is used ubiquitously across data science\\nand data analysis.\\nNumPy is a numerical library that you can use\\nto create shape and reshape data structures.\\nEach of these data structures are what's called\\nan N-dimensional array or ndarray.\\n\\nNdarrays are faster than and more computationally efficient\\nthan Python's built-in containers,\\nand because of that, almost all data analytics\\nand data science Python libraries require NumPy\\nor are what we call built on top of NumPy.\\nAn ndarray is simply a container for data.\\nEach ndarray has a distinct number of dimensions\\nas well as a size and data type.\\n\\nConsider the two ndarrays you see on the screen here,\\nthey both contain identical data,\\nmeaning technically they are the same data set,\\nbut they're stored in completely different structures.\\nNumPy makes it very easy to shape\\nand reshape your data into the exact structure\\nyou need for analysis.\\nWith just one function call,\\nyou can easily use the NumPy library\\nto arrange and rearrange your data set\\nin order to store it in various structures.\\n\\nThe flat container you see on the screen here\\nis an array structure.\\nMore specifically, a one dimensional array.\\nThe table is an ndarray.\\nIt's a two dimensional array.\\nBoth are matrix structures.\\nTechnically, they're both ndarrays,\\nbut the flat array has only one dimension,\\nso it's more clear to call it an array.\\nThe Pandas library is also a third party Python library.\\nIt's built on top of NumPy\\nand it offers an easy way to work with arrays in matrices.\\n\\nPandas is useful for its fast data cleaning preparation.,\\nits powerful analysis capabilities,\\nIts ease of use for data visualization and machine learning,\\nand also for its deep compatibility with NumPy arrays\\nand matrices because of course, it's built on top of NumPy.\\nArrays and matrices are called series\\nand data frames in Pandas.\\nA series is an array similar\\nto a one dimensional array in NumPy.\\n\\nA data frame is a two dimensional table\\nthat can hold different types of data.\\nIt's similar to a matrix in NumPy.\\nWithin pandas, a series object is either a single row\\nor column, and it's always indexed.\\nA data frame object is pretty much like a spreadsheet\\nof rows and columns.\\nThe rows and columns individually are actually series\\nof objects in pandas library,\\nand data frames are always indexable.\\n\\nIn case you're curious about all this index talk,\\nAn index is a list of integers or labels\\nyou can use to uniquely identify rows or columns.\\nIn this course, we're going to be indexing\\nusing mostly square brackets.\\nIn the following coding demonstration\\nwe're going to be using comparison operators,\\nso I wanted to give you this table.\\nYou can review it on your own j\\njust to re-familiarize yourself\\nwith using comparison operators in Python.\\n\\nThe two I wanted to point out are simply\\nthe greater than symbol and the less than symbol,\\nbecause we'll be using those\\nin the following coding demonstration.\\nThroughout this course, we're going to be consistently\\nusing the NumPy and Pandas library.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2714160\",\"duration\":813,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Filtering and selecting\",\"fileName\":\"3006708_en_US_02_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"3 pickups recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1599,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to filter and select data by using indexes of series objects and DataFrame objects.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":31754399,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's talk about filtering\\nand selecting data with pandas.\\nNext up, I'm about to show you how to filter\\nand select data using plain indexing, data slicing,\\nand arithmetic comparisons using Python and Pandas.\\nIn this demonstration, we're going to work with two libraries,\\nNumPy and Pandas.\\nAnd the first thing you need to do is just to make sure\\nthat you have your library installed in your environment.\\nSo let's really quickly run pip install pandas.\\n\\nAnd in order to execute any code\\ninside of a Jupyter Notebook,\\nyou want to just hit Shift + Enter in order to run code.\\nSo that's what I did there.\\nAnd okay, so we have installed Pandas.\\nNow we want to import both NumPy and Pandas.\\nSo to do that we say import numpy as np,\\nimport pandas as pd,\\nand then from pandas\\nwe want to make sure to import DataFrame.\\n\\nAnd then we have all of that.\\nSo we will run this, Shift + Enter.\\nSo what you actually do is importing these libraries\\ninto the IPython environment.\\nThen the next step involves the creation\\nof a DataFrame object, and we'll fill it with some numbers\\nand then I'll show you how we can apply different functions\\non that DataFrame object.\\nSo let's call that object numbers_df\\nand we'll set it equal to,\\nand then we'll call the DataFrame constructor\\nand we'll pass in the np.arrange function.\\n\\nAnd then we'll say we want to generate a series of numbers\\nbetween zero and 90,\\nbut we want only every third number.\\nSo to do that, we say zero is the first number,\\n90 as the end.\\nAnd then define that we want only every third number here.\\nThe next thing we need to do\\nis to define a shape for this object.\\n\\nAnd so to do that, we will call the reshape method\\nand we'll pass in\\nthe values of 10 and three.\\n10 is going to be the number of rows,\\nand then three will be the number of columns.\\nThe next thing we need to do\\nis to set an index for this DataFrame.\\nSo to do that\\nwe'll just say index\\nand then we'll set it equal to a list.\\n\\nAnd then for each index, we just want to create a label.\\nSo we'll start with row one,\\nthen we'll go through\\nand just create a label for each index value.\\nAnd since we have 10 rows,\\nwe need 10 index values.\\nSo I'll go ahead and just copy in all of these labels.\\nAnd then also, let's name the columns here.\\n\\nSo we'll say columns equal to, and then create another list.\\nAnd then we'll just call the columns,\\ncolumns one through three.\\nSo column one, and then just finish this out.\\nI'll copy it in.\\nThis is red here.\\nSo it looks like we probably have a syntax error.\\nSo you can see here I have one too many parentheses.\\nSo let me just delete that\\nand then rerun it.\\n\\nOkay, that cleared out that error.\\nSo now let's print the numbers DataFrame object.\\nIn order to do that, we just need to say numbers_df\\nand then run this.\\nOkay, so great, now you can see\\nwe have a numbers DataFrame object,\\nand each of the index values are labeled\\naccording to the labels we passed\\ninto the DataFrame constructor.\\nSo now let's take a look at indexing and slicing.\\n\\nIndexing means accessing items from a data structure\\nlike arrays, series or DataFrames.\\nAnd there are three types of indexing.\\nSimple indexing, Boolean indexing, and fancy indexing.\\nLet's start first with simple indexing on a DataFrame.\\nTo do that, we can use the iloc indexer\\nto access items within a DataFrame.\\nOne thing you need to know\\nis that indexing starts from zero,\\nwhich means that the first element\\nis placed at the zero index.\\n\\nIn a DataFrame, we have multiple rows and columns.\\nSo in order to access an item in a DataFrame,\\nwe need to pass its row and column number\\nin square brackets separated by a comma.\\nThe value before the comma is the row number\\nand the value after the comma is the column number.\\nAnd if we want to access the second item\\nin the first row of the DataFrame,\\nthen we would just call the numbers_df object.\\n\\nAnd then we would use the iloc method,\\nand we will pass row number zero\\nin column number one into this iloc indexer.\\nAnd then when we run this,\\nwe get back the value of three.\\nSo if we look back up here,\\nwe can see that the value of three\\nis sitting in the row that is of index value zero\\nin the column which has an index value of one,\\n'cause again, indices start from zero,\\nwhich means that the first element\\nis placed at the zero index value.\\n\\nWe can use the equal notation\\nto replace a value in a DataFrame.\\nTo illustrate this better,\\nlet's replace the value at row one and column one\\nwith the number 20, and then run the code block.\\nSo we'll call the numbers,\\nnumbers underscore DataFrame object,\\nthe iloc method, and then we'll define the position here\\nas zero and one.\\nSo the value at row zero in column one,\\nand we'll set that equal to 20.\\n\\nAnd then we will, oops,\\nI missed an S here.\\nAnd then let's print out the object.\\nSo we'll say numbers underscore DataFrame\\nand then run this.\\nAnd you can see here now\\nthat what used to be value of three here\\nhas now been replaced with a value of 20.\\nNow let's look at how fancy indexing works.\\nFancy indexing is like the simple indexing\\nthat we just used, but there's a difference.\\n\\nInstead of passing single scalers,\\nwe're going to pass arrays of indices.\\nIn the DataFrame we can retrieve common items\\nbetween specific rows and columns using fancy indexing.\\nSo as an example, let's access the common items\\nbetween the second, third and fifth row\\nin the second and third column.\\nSo to do that, we call the numbers_df object\\nand we call the iloc method.\\n\\nAnd then let's define the rows\\nthat we want to return,\\nwhich would be one, two, and four,\\nand also the columns,\\nwhich is going to be columns at index position one\\nand index position two.\\nAnd then we run this.\\nAnd now as you can see, we have returned the common items\\nbetween the second, third and fifth row\\nand the second and third column.\\n\\nNow let's dig into Boolean indexing in a DataFrame.\\nBoolean indexing is done through comparison operators,\\nand in case you are not quite sure\\nwhat I mean by comparison operators,\\ncomparison operators are just like\\ngreater than, less than, equal than,\\nbasic arithmetic comparison operators.\\nSo we're going to look at comparison operators and masking.\\nComparison operators compare a single scaler value\\nwith all the values in the DataFrame,\\nand they always return a DataFrame with Boolean values.\\n\\nBoolean values are also called a Boolean mask.\\nLet's implement Boolean indexing on the DataFrame.\\nSo to do that, if we wanted to find values\\ngreater than 30 in the DataFrame,\\nwe could say mask equal to,\\nand then call the numbers_df object\\nand just say we want everything\\nthat's greater than the number 30.\\nAnd then print out the mask\\nand run that.\\n\\nThat was marked down so we need to make sure\\nthat it is actually the correct code formatting here.\\nSo I'll turn that to Python code and then run that.\\nAnd now you can see we have a DataFrame\\nfull of Boolean values\\nwhere if the value is greater than 30,\\nthen the result is returned as true.\\nAnd then if the value is less than 30,\\nthe result is returned as false.\\n\\nWe can use this Boolean mask to retrieve\\na subset of data from the DataFrame.\\nIf we pass the Boolean mask in the DataFrame,\\nit will return to us those values\\nwhich were greater than 30.\\nSo we'll say numbers_df\\nand then we just want to return the mask.\\nAnd if we run this code block,\\nwe can see that only the values\\nwhich are greater than 30 are returned.\\n\\nIf the value is less than 30,\\nthen we get NAN, not a number.\\nSo if the values are less than 30,\\nthen we just get this NaN.\\nSo as you can see,\\nthis just returns all the values from the DataFrame\\nthat are greater than 30 as defined by our mask object.\\nBoolean mask is also used to replace values in a DataFrame.\\nSo let's try that out really quickly.\\n\\nHere let's set all of the values in the DataFrame\\nthat are greater than 30\\nsuch that they are set equal to zero.\\nTo do that, we'll just call the numbers_df object,\\nand then we want to return\\nall of the numbers within the numbers DataFrame\\nthat are greater than 30,\\nand we want to set those values equal to zero.\\n\\nAnd then we'll call the numbers_df object again.\\nRun that, and you can see\\nthat all of the numbers from the DataFrame\\nthat were greater than 30 have now been set to zero.\\nThe last thing I wanted to show you here\\nis how to slice values from the DataFrame.\\nSlicing means pulling a section from the DataFrame.\\nSo we can slice values using the colon notation.\\nAny value that we put before the colon\\nis the starting index of the slice.\\n\\nAnd the value after the colon\\nis the ending index of the slice.\\nLet's slice the values from the third to the sixth row\\nand the second to third column.\\nTo do that, we'll say numbers_df\\nand then we'll call the iloc method.\\nAnd then we'll say we want the rows\\nin index position two through six\\nand column index position\\none through three,\\nand we can run this.\\n\\nAnd then as you can see, it has returned to us the values\\nthat are common between the third, fourth,\\nfifth, and sixth row and the second and third column.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4586141\",\"duration\":1084,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Treating missing values\",\"fileName\":\"3006708_en_US_02_04_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1945,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to treat missing values. This video covers dropping values, approximation, and filtering.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":35498780,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now we're going to talk\\nabout comparison operators and scalar values.\\nJust in case you don't know what a scalar value is,\\nit's basically just a single numerical value.\\nYou can use comparison operators like greater than\\nor less than to return true or false values for all records\\nto indicate how each element compares to a scalar value.\\nIn Python, by default,\\nmissing values are represented with a symbol, NaN,\\nwhich stands for not a number.\\n\\nBe warned, if your data set has zeros, 99s, or 999s,\\nbe sure to either drop or approximate them\\nas you would with missing values.\\nLet me give you an example\\nof where treating missing values is useful.\\nImagine you work in a marketing department\\nof a local car dealership.\\nYou've been tasked with summarizing recent results\\nfrom a customer satisfaction survey.\\nYou get this data set and you can see\\nthat most of the records have been completed,\\nbut Sally and Jim didn't respond with information\\nabout their opinion of quality of work.\\n\\nYou can see that here with the missing values.\\nNonetheless, Sally and Jim have responded\\nto 75% of their request for information,\\nso we wouldn't want to drop them from the survey altogether.\\nThat said, the other respondents,\\nRod, Sam, and Jane, did give information\\nabout what they thought of the quality of work.\\nSo we wouldn't want to drop this variable altogether either.\\nWhat could we do?\\nWell, we could take the average value\\nof the responses we do have, which would be an average\\nof eight, nine, and 10,\\nand then just fill in these missing values,\\nin order to generate an approximation\\nthat gives your boss a pretty good idea\\nof the customer's actual responses.\\n\\nYou'll see later in the coding demonstration,\\nwhy it's important to try and use approximation,\\nrather than just dropping missing values altogether.\\nIn the coding demonstration that's coming up,\\nI'm going to show you how to work\\nwith missing values in Python.\\nYou're going to learn how to discover what's missing,\\nfill in for those missing values, count up missing values,\\nand also filter them out.\\nLet's go ahead and look\\nat how to work with missing data and Pandas.\\nSo as you can see, this notebook is coming preloaded\\nwith our NumPy and Pandas libraries.\\n\\nWe just need to run that.\\nAnd when working in data science,\\nthere are going to be situations that arise all the time\\nwhere you encounter missing values.\\nSometimes it's caused by data entry errors,\\nother times by machine function,\\nbut really, we have a variety of ways\\nto handle missing values and data.\\nAnd I wanted to show you those.\\nBut before I do that, I need to create a dataset\\nthat we can work with.\\n\\nSo let's just call that dataset data,\\nand we'll create a dataset about a group of people\\nthat go by the names,\\nSteve, John, Richard,\\nSarah, Randy, Michael,\\nand lastly, Julie.\\n\\nAnd within this dataset,\\nwe're going to describe their age, gender, and rank.\\nSo we'll set the age equal to 20 for the first person,\\n22, 20, 21, 24, 23, and 22.\\nAs far as the gender,\\nI'm just going to copy and paste these over.\\n\\nOkay, and then let's assign them a rank\\nfor each of these people.\\nSo we'll just say two, one, four, five,\\nthree, seven and six.\\nOkay, so now we have a dataset\\nand everything seems to work fine here,\\nso what I want to do is I want\\nto use the data frame constructor\\nto create a ranking data frame object.\\n\\nSo I'll call it ranking_df.\\nAnd then we'll call the data frame constructor,\\nand we'll pass in our dataset here.\\nAnd then off of that, let's call the iloc method.\\nSo we'll say ranking_df.iloc,\\nand then let's pick the rows\\nin index position two through five.\\n\\nAnd then let's also select the column in index position one.\\nAnd we'll set all of these values equal to a missing value.\\nSo to do that, we will just call np.nan,\\nand that will set the values equal to missing value.\\nAnd then let's do the same thing for rows\\nat index position three through six\\nand column at index position three.\\n\\nAnd then lastly, let's do that\\nfor row at index position three and then all of the columns.\\nSo we can just use a colon operator here,\\nand then we can leave the start and end value undefined,\\nand that will set all\\nof the columns equal to missing values.\\nAnd then let's just print this out and see what we get.\\n\\nAnd as you can see,\\nnow, we have a data frame and it's got missing values\\nfor Richard, for all of row at index position three.\\nAnd at the other locations\\nthat we defined in our iloc indexer above.\\nThe Pandas has several different functions\\nthat are available to us for handling missing values.\\n\\nThe first step for handling missing values is\\nto detect if there are any missing values present\\nin the dataset.\\nIn order to do that, in the data frame,\\nPandas provides us two functions, isnull and notnull.\\nThe isnull function returns true for those values\\nwhich are missing values in a data frame,\\nand the notnull function returns true for the values\\nwhich are not missing.\\n\\nSo let's try both of those out.\\nLet's call our ranking data frame object,\\nand then we will call the isnull function.\\nAnd when we print it out,\\nwe see that we get returned a Boolean data frame\\nwhere true represents missing values\\nand false represent the values which are not missing.\\nNow, let's also try the notnull function.\\n\\nSo we'll say ranking_data frame,\\nand then call the notnull function.\\nAnd then when we print this,\\nyou see that it's absolutely the opposite\\nof the isnull method.\\nSo we are getting back true values\\nwhere the value is not missing,\\nand then where there are missing values,\\nwe should be getting back false,\\nwhich as you can see, we do here.\\n\\nNext, I want to show you how to apply Boolean masking\\nto show only the rows where there is a missing value\\nin a specific column.\\nTo do that, first, we need to find the Boolean mask\\nof the column age with the isnull function,\\nand then we'll pass this Boolean mask into the data frame.\\nSo let's call this whole thing bool_series,\\nand then we'll set it equal to pd.isnull,\\nand within this function,\\nwe will pass our ranking data frame,\\nand we are going to select the age column here.\\n\\nNext, let's pass the Boolean mask into the data frame.\\nSo to do that, we will just call our data frame object,\\nand then we'll pass in the Boolean series object,\\nand then print this out.\\nNow, you can see that we have only returned the rows\\nwhere the age is missing.\\nSo if the age value was not missing\\nin the original dataset,\\nthen those rows did not get returned.\\n\\nWe should also look at\\nhow to fill in missing values using the fill NA function,\\nthe replace function in the interpolate function.\\nHow these data frame functions work is\\nthat they replace missing values\\nwith some value of their own.\\nSo all of these functions help in filling missing values\\nwithin a dataset.\\nThe interpolate function is used to fill missing values\\nin the data frame,\\nbut it uses various interpolation techniques\\nto find the missing values,\\nrather than hard coding the value.\\n\\nLet's start out first by filling a missing value\\nwith a single value.\\nLet's call our ranking_df object,\\nand then we'll call the fillna function,\\nand we will pass zero.\\nAs you can see,\\nall of the missing values\\nin the data frame have been replaced with a zero.\\n\\nLooking at another example, let's fill missing values\\nwith the values that comes prior to the missing value\\nwithin the data frame.\\nIt's a little tricky for me to explain,\\nso I just need to show you.\\nWe'll use the fillna function with the data frame.\\nSo we'll say ranking_df.fillna,\\nand if we pass the method pad here,\\nand print this out.\\n\\nWhat you can see is that all the missing values\\nin the data frame have been replaced with the value\\nthat came prior to the missing value.\\nSo as you recall, all of the values in the row\\nat index position three were previously missing values.\\nAnd now, all of these values have been replaced\\nby the values that were in row at index position two.\\n\\nCan see what I mean here.\\nSo that's how the fillna function works.\\nAnother example is to fill missing values\\nwith the next value, which is not missing in the data frame.\\nSo it would be the opposite approach\\nto filling the missing value.\\nTo make this change, we can still use the fillna function,\\nbut we just need to change the method\\nto be fill for backfill.\\n\\nWe'll print this out.\\nAnd then as you can see here,\\nnow, the row at index position three has been filled\\nwith the values that came after it\\nin the original data frame, it came back up here.\\nYou can see that we had Randy, NaN, male.\\nNow interestingly, we had NaN here.\\nSo all of these NaN missing values were backfilled\\nwith the one prior, which would be six.\\n\\nSo that's why all of these values are now six.\\nThey have been backfilled from the value\\nthat was not missing that came after them.\\nLet's also look at filling missing values\\nin a data frame using the interpolate function\\nwith a linear method.\\nThe linear method ignores the index\\nand treats the values as equally spaced.\\nSo let's just call our ranking data frame object,\\ncalled the interpolate function.\\n\\nAnd here, we'll say that the method is equal to linear,\\nand run that.\\nSo now you can see that for each of the missing values\\nthat was numerical here,\\nit has been filled with a linear interpolation.\\nSo as you recall,\\nif we go back up to the original data frame,\\nwe had here in our rank column,\\nwe had numbers which were not a number,\\nand they ranged between four and six,\\nand there were three of them, right?\\nSo if we were to make a linear interpolation,\\nbetween the values of four and six,\\nthen we would come up with these values.\\n\\nAnd so that's exactly what this method did,\\nis it interpolated between the value\\nthat came before the missing numbers and the value\\nthat came after the missing numbers, column-wise.\\nWe can also drop all of the rows and columns\\nthat contain missing value use using the dropna function.\\nSo if we call the dropna function off\\nof the ranking data frame object,\\nand run this, see that all the rows and columns containing\\nat least one missing were all dropped from the data frame,\\nand all we're getting back are the rows and columns\\nwhich have no missing values whatsoever.\\n\\nNow, let's try to drop all the rows and columns,\\nwhich only contain missing and values,\\nusing the dropna function with the keyword,\\nhow equal to all,\\nand see how that changes things.\\nSo we'll say how, we'll set that equal to all,\\nand we'll run this.\\nAnd now you can see that all the rows and columns\\nthat contain only missing values have been dropped.\\nSo for example,\\nthe row at index position three was all missing values,\\nand now you can see it's been dropped.\\n\\nLet's also drop all the columns,\\nwhich contain at least one missing value.\\nWe can use the dropna function to do that as well.\\nAnd then what we would need to do here though is\\nthat instead of using how equals all,\\nwe would say access equal to one.\\nAnd you can see that each column had a missing value.\\nSo what we're doing here with the access equal to one is\\nwe are telling the dropna function\\nthat we wanted to look in all of the columns.\\n\\nAnd for all of the columns that contain a missing value,\\nthen that column needs to be dropped from the data frame.\\nAnd when we run this,\\nyou can see that we get returned no columns.\\nThat is because all of the columns had a missing value\\nin them in the original data frame.\\nSo we can go back here to the original data frame and look,\\nand there is not a column without a missing value,\\nspecifically because of this row at index position three.\\n\\nSo all of the columns have been dropped.\\nNow, if we wanted to drop all of the rows\\nthat contain a missing value,\\nwe can use the same function,\\nand then we would just change the access here to zero,\\nand run this.\\nAnd now you can see, okay, we back only the rows\\nthat have no missing values.\\nFor real, had a missing value,\\nthen it got dropped.\\n\\nAnd that is how to work with missing values using Pandas.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4586142\",\"duration\":470,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Removing duplicates\",\"fileName\":\"3006708_en_US_02_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":779,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to remove duplicates. This video covers dropping records with pandas.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":16426350,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] It's really important\\nto remove duplicates from your dataset\\nin order to preserve the dataset's accuracy\\nand avoid producing incorrect and misleading statistics.\\nFor example, imagine you're analyzing a retail sales table\\nand shopaholic Sally came in three times\\nand used three different credit cards\\nto make purchases but provided the cashier\\nthe same zip code, 3-2-8-0-3, for each sale.\\nJust based on the card number, Sally looks\\nlike three different customers all\\nfrom the 3-2-8-0-3 zip code.\\n\\nIf you fail to examine other attributes of the customer\\nso that you can identify and remove duplicates,\\nshopaholic Sally's results would skew the results\\nof any customer demographic analysis\\nbecause Sally would be counted\\nas three people rather than one.\\nTo market to the 3-2-8-0-3 customers effectively\\nyou need to understand their characteristics.\\nDon't let duplicate records skew your analysis.\\nOkay, now let's look at removing duplicates.\\n\\nThis notebook is coming preloaded with Numpy and Pandas.\\nAnd as you can see here, we're also going to be importing\\nthe series as well as data frame from Pandas library.\\nSo you can just run that.\\nAnd then we need to have data\\nfrom which to remove duplicates.\\nSo let's create a data frame.\\nWe'll call it DF_object.\\nAnd we'll set it equal to,\\ncall the data frame constructor here.\\n\\nAnd then we will create three columns.\\nSo column 1.\\nAnd in column 1 let's pass in a list of values.\\nWe'll say 1, 1, 2, 2, 3, 3, and 3.\\nGreat, so let's look at column 2.\\nFor column 2, we'll just create a list of letters.\\n\\nSay A, and I'm just going to copy these over.\\nSo we'll have it be a, a, b, b, c, c.\\nOkay, and then for column 3,\\njust name this column name\\nand then we will define the values\\nas same thing, A, A, B, B, C, C,\\nbut we'll just make those uppercase instead of lowercase.\\n\\nLet's just print this out.\\nOkay, so now we have a data frame to work from.\\nClearly has duplicate values in here\\nso I wanted to show you the .duplicated method.\\nThis method searches each row in the data frame\\nand returns a true or false value\\nto indicate whether it's a duplicate of another value found\\nin a different row earlier in the data frame.\\n\\nSo let me show you how that works.\\nWe'll say DF_obj and then we'll call the .duplicated method\\nand then we'll run this.\\nAnd looking at the original data frame, you can see\\nthat if there was a duplicate value within a row\\nso, looking at row at index position 0,\\nthere were no duplicate values here.\\n\\nSo we got returned a false.\\nBut then when you look at row at index position 1,\\nwe get returned a value of true and that is because\\nof values at row index position 1 are duplicates\\nof the row prior.\\nAnd then you'll see row index position 2 returns a false\\nbecause they are not duplicates.\\n\\nBut then row at index position 3 returns a true\\nbecause, again, we have another set of duplicates.\\nNow that we have found duplicate records, let's look\\nat how we can drop them.\\nTo do that, we'll use the drop duplicates method.\\nAnd if we wanted to just drop all the duplicate rows\\nwe would simply call the drop duplicates method\\noff of the data frame object.\\n\\nAnd we won't specify anything here\\nand you'll see that for row 2\\nor the row with the series index value at 1 was dropped.\\nAnd that makes sense here because it was a duplicate\\nof the row at index position 0.\\nSo it got dropped and then using similar logic\\nthe row at index position 3 also should have been dropped.\\n\\nAnd as you can see, it was.\\nSo, yes, it looks like absolutely all\\nof our duplicate rows have been dropped from our data frame.\\nBut I also want to show you how to drop records based\\non the column values.\\nIn order to do that, I need to make a small change\\nto our data frame.\\nSo let's just go back and copy this code we used originally\\nhere to create the data frame.\\nAnd what I'm going to do is I'm going to change\\nthis letter here at the end.\\n\\nThis C here, I'm going to change it to a D,\\nwhich is a pretty minor change.\\nBut let's just see how we can use this\\nto drop records based on the column values.\\nSo I'll print this out.\\nHere we see okay, we've got changed this C out to a D.\\nSo to drop the rows that have duplicates\\nin only one column series, you just call\\nthe drop duplicates method off of the data frame\\nand then pass in the label index\\nof the column you want to de-duplicate.\\n\\nSo let's say DF_obj and then say drop duplicates.\\nAnd I'll set column 3 here and then run this.\\nAnd just as we predicted, what this function has done is\\nit has dropped the series\\nthat have index values 1, 3, and 6.\\n\\nBecause we do not have a duplicate\\nin column 3 here, that row did not get dropped.\\nI want to highlight really quick\\nthat it's important, in fact it's really important\\nthat you check your data for duplicates\\nand remove them if you find them.\\nNow it's time to move on\\nto data concatenation and transformation.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4584140\",\"duration\":795,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Concatenating and transforming\",\"fileName\":\"3006708_en_US_02_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1375,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to concatenate and transform. This video covers combining data, converting data, and reformatting data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":26344657,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Voiceover] Knowing how to concatenate and transform data\\nis really important in data analysis.\\nConcatenation and data transformation are useful\\nfor getting your data into the structure\\nand order you need for analysis.\\nFor example, imagine you're mailing out a piece\\nof direct mail advertisement.\\nYou have one table with customer ID and name,\\nand you have another table with customer ID,\\nmailing address, and age.\\nYour mailing address application requires you\\nto supply it only one table that contained\\nonly customer name and address.\\n\\nYou generate this table by concatenating your two tables\\nby customer ID, row wise.\\nConcatenating is simply combining data\\nfrom separate sources.\\nTransformation, on the other hand,\\nis converting and reformatting data\\nto the format necessary for your purposes.\\nWhen you transform your data, you convert it\\ninto the format that's required to facilitate analysis.\\nThis could include dropping data, which is essentially just\\ndropping variables or observations,\\ncan include adding data, which is adding variables\\nand observations,\\nand it also includes sorting data.\\n\\nSo, going back to our example,\\ntransformation would be when you drop the age column\\nin order to get your data into the exact format\\nthat the application would need.\\nOkay, so in this demonstration we're going to look at\\nconcatenating and transforming data.\\nAnd as you can see, the notebook is coming loaded\\nwith both pandas and numpy.\\nSo, you've got everything set up here,\\nall you need to do is just make sure you run that.\\n\\nAnd then we'll start first with concatenating data.\\nBefore we do anything else, we'd need to create\\na data frame object, which I'll call \\\"DF_obj\\\"\\nand then call the data frame constructor.\\nAnd we'll pass the np,arange function,\\nand we will say\\nthat we want to create a series of values.\\n\\nRephrase, and we will say we want to create\\na series of 36 values,\\nand then we want them to come in the shape of a 6x6.\\nSo, we'll say reshape,\\nand then say six rows\\nand six columns.\\nAnd then if we print this out,\\nyou can see, great, we have a series of numbers\\nstarting at zero, going to 35, so it's 36 numbers\\nand it's in a 6x6 shape.\\n\\nNow let's create a second data frame.\\nAnd we'll call it \\\"DF_obj_2\\\"\\nand we'll do the same process here, except for let's just\\ncreate a series of 15 numbers\\nand then with five rows and three columns,\\nand print that out.\\n\\nGreat, perfect.\\nSo, now we have something to work with.\\nLet's look at how to concatenate data.\\nTo do that, we will use the concat method,\\nwhich joins data from separate sources\\nand combines them into one combined data table.\\nIf you want to join objects based on their row index value\\nyou just call the \\\"pd.concat\\\" method\\non the objects you want joined and then pass in\\nthe \\\"axis=1\\\" argument\\nand \\\"axis=1\\\" tells Python to concatenate the data frames\\nby adding columns.\\n\\nIn other words, joining on the row index values.\\nSo, let's just test that out here.\\nSay \\\"pd.concat\\\"\\nand we'll select our data frame object\\nand our data frame object two,\\nand we'll pass in \\\"axis=1\\\".\\nNow, as you can see, we have gone from having\\na data frame with... here has six columns,\\nthis one has three columns,\\nnow we have a data frame with nine columns\\nwhere data frame object two has been appended\\nor concatenated onto the original data frame object.\\n\\nAnd in the case where the shapes didn't match up,\\nlike at row index position five here,\\nwe don't have that row in the second data frame object,\\nand so as you can see, we got missing values returned\\nin that part of the results that were returned to us.\\nNow, if we wanted to do the opposite,\\nand we wanted to join on the column index values,\\nwhat we could do is we can just\\neither say \\\"axis=0\\\" or we can just completely omit\\nthis axis parameter and by default it will\\njoin on the column index values.\\n\\nSo, I run this, and now you see that\\nthe two objects have been concatenated column-wise.\\nLet's look at transforming data really quickly.\\nSo first, let's look at dropping data.\\nYou can easily drop rows from a data frame\\nby calling the drop method and passing in the index values\\nfor the rows you want dropped.\\nSo, say \\\"DF_obj\\\"\\nand then \\\".drop\\\",\\nlet's say we want to drop the rows\\nat index position zero and two.\\n\\nWe can execute the code here.\\nThere was a syntax error where I missed brackets.\\nThere we go.\\nNow you see that the rows at index position zero and two\\nhave been dropped from my results.\\nNow, if we wanted to go ahead and instead drop\\nthe columns at index position zero and two,\\nall you would need to do is pass a parameter\\nthat says \\\"axis=1\\\" and run that,\\nand you can see now it's dropped those columns.\\n\\nLet's look now at adding data.\\nSo, let's create a series object called \\\"series_obj\\\"\\nand then we'll call the series constructor,\\nwe'll pass in the \\\"np.arange\\\" function\\nand we will say we want a series of values\\nfrom zero to five, we'll want six values.\\nAnd then let's give this series the name \\\"added_variable\\\".\\n\\nSo, to do that we'll say \\\"series_obj.name\\\"\\nand we'll set that equal to \\\"added_variable\\\"\\nand then just print this out to see what it looks like.\\nOkay, great, so it's a series of six numbers\\nthat range between zero and five\\nand the series is named \\\"added_variable\\\".\\nNow, you can use the join method\\nto join two data sources into one.\\n\\nBy default, the join method works by joining the two sources\\non their row index values.\\nLet me show you real quick how this works.\\nWe will say \\\"variable_added\\\"\\nis equal to...\\nand we'll call \\\"DataFrame.join\\\" function\\nand we will pass in our data frame object\\nand our series object,\\nand then print it out.\\n\\nAh, look, and then as you can see, our series\\nthat we created here called \\\"added_variable\\\"\\nhas now been added to the data frame.\\nNow let's try adding data using the concat function.\\nWe will create a object called \\\"added_datatable\\\"\\nand we'll set that equal to \\\"pd.concat\\\".\\n\\nWithin the concat function, let's pass\\nthe \\\"variable_added\\\" object twice.\\nSo, we're asking it to concat the \\\"variable_added\\\"\\ndata frame to itself, essentially here.\\nAnd then, let's set the parameter \\\"ignore_index=False\\\"\\nand I'll print this out, and then I'll explain\\nhow this will work.\\n\\n\\\"Added_datatable\\\", okay, good.\\nSo, what you can see here is\\nit's taken our \\\"variable_added\\\" object that we had,\\nand it's concatenated it to itself.\\nBut, because we said this \\\"ignore_index=False\\\" here,\\nwhat it's done is the index now has duplicates\\nbecause the original index values from the original object\\nhave just been concatenated into the results\\nthat were returned.\\n\\nSo, if we wanted to reset the index so that it\\nworks properly, we would want to then just say\\n\\\"ignore_index=True\\\".\\nAnd basically what we have done here\\nis we have told pandas to ignore the index\\nof the input data frame and create a new integer index\\nfor this resulting data frame.\\n\\nAnd so, the new index starts at zero,\\nand it increments by one for each row,\\nregardless of what the original index was\\nin the input data frame.\\nThe last thing I want to look at here with you\\nis sorting data.\\nTo sort rows in the data frame, either in ascending\\nor descending order, you'd need to call\\nthe \\\".sort_values\\\" method off of the data frame\\nand then pass in the by argument to specify the column index\\nupon which the data frame should be sorted.\\n\\nSo, let's create a data frame called \\\"DF_sorted\\\"\\nand we'll set it equal to our data frame object,\\nand off of that object we'll call the sort values method,\\nand then we will say we want to sort by our column\\nthat is in index position five.\\nSo, in order to do that, we would just say \\\"by=[5]\\\"\\nand then if we want it to be in descending order\\nthen we just say \\\"ascending=False\\\".\\n\\nAnd then when we print this out,\\nyou can see that our column...\\nour data from our column index position five\\nhas been sorted in descending order.\\nSo, if we look back at our original data frame,\\nyou can see that it was in ascending order\\nand so now the order has been clipped\\nand with that, all of the other values\\nin all of the other columns have also been sorted\\nin that exact same way.\\n\\nAnd that's about all you need to know about\\nconcatenating and transforming data using pandas.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4586143\",\"duration\":348,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Grouping and aggregation\",\"fileName\":\"3006708_en_US_02_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":449,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to group and aggregate. This video covers subgrouping, describing subgroups, and data aggregation.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12311338,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now, we're going to talk about data grouping\\nand aggregation.\\nGrouping and aggregation are useful for exploring\\nand describing your dataset in its subgroups.\\nImagine you are a merchant that sells fruit\\nand you have a dataset that describes\\nthe different types of fruit you have\\nand where you purchase them from.\\nTo understand what a subgroup is,\\nlook at this example of the data set\\nthat has apple and orange records.\\nIf you were to reduce this data set down\\nto its fundamental subgroups by fruit category,\\nyou would get two records, apple and orange.\\n\\nGrouping is an excellent method to use\\nwhen you want to explore and understand your data\\nand its inherent subgroups.\\nIt's useful for many, many reasons.\\nYou can group data in order to compare subsets\\nand deduce reasons why subgroups differ the way they do.\\nOr you may only be interested\\nin specific subgroups for your analysis.\\nGrouping can help you identify\\nand subset out those subgroups.\\nOkay, let's go ahead\\nand look at how to group data by column index.\\n\\nThis Jupyter notebook is coming preloaded\\nwith numpy and pandas, so just make sure to run that.\\nAnd then for this demonstration,\\nwe're going to use a dataset called mtcars.\\nThe first thing we need to do\\nis define a location for that dataset.\\nSo it's going to be address, I say address is equal to,\\nand what you need to do is actually go\\ninto the data folder here and find mtcars dataset,\\nand then right click.\\nAnd you want to copy the full path here.\\nSo Copy path, and then back to the Jupyter notebook\\nand paste the path in here.\\n\\nAnd now you have defined the address\\nwhere the CSV file sits.\\nTo read it in, you want to use the read CSV function,\\nso we'll call this dataframe cars\\nand we'll say cars is equal to pd.read_csv,\\nand we'll pass in the address.\\nAnd then let's assign names\\nto each of the columns in this dataset.\\nSo to do that, we need to access the cars columns.\\nWe'll say cars.columns and let's have that equal to a list.\\n\\nWithin that list,\\nwe'll pass in the name of each of the columns.\\nSo the first column name is car_names.\\nAnd then I'm going to go ahead\\nand just paste in the rest of these.\\nIn a different coding demonstration,\\nwe will dig into further details\\nabout what these variables mean as needed.\\nBut for now, I'm just trying to show you\\nhow to group data by column index.\\nSo let's just take a look\\nat the first five records in this data set.\\nWe'll say cars.head, call the head method off of that,\\nand then print it out.\\n\\nAnd here we go, we have the first five records\\nfor the mtcars data set.\\nSo we're good to go there.\\nNow, what I want to do is show you how to group this dataframe\\nby a particular column.\\nAnd to do that we'll use the group by method.\\nSo let's create a new dataframe called cars_groups.\\nAnd we'll form that by calling the groupby method\\noff of the cars dataframe.\\nAnd then passing in, we'll be selecting the cylinder column.\\n\\nSo we want to say cars.groupby.\\nAnd then we pass in the cars dataframe,\\nand then we'll select the cylinder column here.\\nAnd so say we want to generate a mean of the values,\\nthe numeric values in the dataframe\\nas grouped by the cylinders column.\\nSo to do that, we would say cars_groups,\\nand then call the mean method off of that.\\n\\nWe need to pass in a parameter\\nthat says numeric only equal to true,\\nbecause we only want to be looking at the numeric values\\nsince we're calculating a mean.\\nAnd then here, we can see there's a little typo.\\nSo I'll fix that and then run this.\\nOkay, I need to add an S here.\\nOkay, great.\\nSo now, just to show you what this is doing,\\nthe groupby method has grouped the entire dataframe\\nby the different categories that were in the cylinder field,\\nwhich the cylinder field only had the values\\nof four, six, and eight.\\n\\nSo the dataframe is now grouped according to those values.\\nAnd then for each numeric field within this dataframe,\\nmean value was generated for each grouping.\\nSo to make this a little more relatable,\\nlet me just explain how.\\nHere, you can see, okay, for our four-cylinder group,\\nthe average, the mean miles per gallon is 26.6.\\nAnd then for our six-cylinder cars, our miles per gallon,\\nthe average miles per gallon for six cylinders cars is 19.7.\\n\\nAnd then for eight-cylinder cars,\\nthe average mile per gallon is 15.1.\\nAs you can see, as the cylinders increase\\nthe average mile per gallon that the cars gets decreases.\\nAnd that is how you group data by column index.\\n\"}],\"name\":\"2. Data Preparation Basics\",\"size\":134110333,\"urn\":\"urn:li:learningContentChapter:4583163\"},{\"duration\":1570,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4583160\",\"duration\":223,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Importance of visualization in data science\",\"fileName\":\"3006708_en_US_03_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":392,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn why visualization is a key aspect of data science. Learn how data scientists effectively communicate data, insights, and predictions.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5117666,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Presenter] And let's talk about the importance\\nof visualization in data science.\\nData visualization is important\\nbecause data insights cannot very easily be converted\\nto business value if they're not communicated effectively.\\nData visualization is the process of transforming data\\nin the form of graphs, charts,\\nor any other form of visual scheme.\\nThe purpose of data visualization is to explore data\\nand to extract useful information from it.\\n\\nVisualization helps in finding patterns,\\nleaning valuable insights,\\nand observing trends in the dataset.\\nDifferent types of graphs are used for data visualization.\\nLet's take a look at some examples of graphs\\nand their utilization for various problems.\\nA line graph graphically displays data\\nthat changes continuously over time,\\nor whatever variable you have mapped out over the X axis.\\nLine graphs have a horizontal axis and a vertical axis.\\n\\nWe use line graphs\\nwhen we want to demonstrate trends in data.\\nSimilarly, we can also use line graphs\\nwhen we want to compare different variables\\nin specific time periods.\\nBar graphs are one of the most popular types of graphs.\\nThey are popular for an obvious reason.\\nThey're easy to understand.\\nGenerally, how bar graphs are set up\\nis that on the horizontal axis,\\nwe represent the name of the categorical data,\\nand on the vertical axis,\\nwe demonstrate the measurable value.\\n\\nBar graphs can be used\\nto visualize the distribution of data,\\nand similarly, we can also use bar graphs\\nwhen we want to compare data of different categories.\\nThere are several other scenarios\\nwhere we would use bar graphs.\\nPie charts use pie slices\\nto display relative sizes of data categories.\\nYou can use pie charts\\nto show the relative sizes of many things.\\nFor example, what type of transport most people use,\\nor how many customers a shop has\\non different days of the week.\\n\\nWith technological advancements\\nand generative AI in the picture,\\nhuge volumes of data are generated on the internet\\non a daily basis.\\nWith machine learning,\\nyou make decisions based on actionable insights\\nthat you predict from datasets.\\nHuge datasets can be very tricky to interpret\\nand extracting information from them is a tedious job.\\nData visualization makes this task much easier.\\nData visualization makes decision-making\\nand problem-solving easier for data scientists.\\n\\nIt can help you understand the next steps\\nthat must be taken to complete a project.\\nSimilarly, it makes it easier for you\\nto document your findings\\nand communicate them visually with stakeholders.\\nLet's take a real-life example\\nwhere data visualization can help\\nin business decision-making.\\nHere's a graph showing the number of sales produced\\nfrom a sports store.\\nThe data is collected each hour\\nfrom the time when the store is open\\nto when the store is closed.\\n\\nThe store manager needs to decide\\nwhich time slot is the busiest at the store.\\nIn other words, which period requires\\nthat maximum staff be present at the store?\\nIf we take a look at the graph,\\nwe can see the spike in sales at the store after 4:00 PM,\\nand this goes on till 1:00 AM.\\nWith this, we can conclude\\nthat 4:00 PM to 1:00 AM is the busiest period at the store\\nand the maximum number of staff are needed\\nat this specific time interval.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4584141\",\"duration\":522,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"The three types of data visualization\",\"fileName\":\"3006708_en_US_03_02_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":767,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to identify three types of data visualization. This video covers data storytelling, data showcasing, and data art.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14070959,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this segment,\\nwe're going to be talking about the three\\ntypes of data visualization.\\nThose data visualization types are data storytelling,\\ndata showcasing, and data art.\\nData storytelling is the type\\nof data visualization you would create\\nfor generating presentations that you would use to present\\nto organizational decision makers.\\nData showcasing is the type\\nof data visualization you would be making\\nfor generating presentations for analysts, scientists,\\nmathematicians, and engineers.\\n\\nLastly, data art is the type\\nof data visualization you would create\\nfor presentations when you're presenting to activists\\nor the general public at large.\\nData art is pretty effective in these cases.\\nIn this section of the course, we're just going to go\\nthrough each of the different areas\\nand get them really well-defined for you so\\nthat you can use them effectively across your career.\\nDiving into data storytelling first, the purpose\\nof data storytelling is to make it easy for your audience\\nto really understand the point you're trying to make\\nwith the data visualization.\\n\\nYou need them to understand the point you're making\\nwith your data visualization,\\nand you want them to be able to understand that within 10\\nto 15 seconds of looking at the graphic.\\nIf they have to study it for longer than that, you're going\\nto lose their attention.\\nThat and your data visualization needs to be clutter-free\\nand highly focused.\\nAgain, you have to think about your intended audience when\\nyou're creating a data visualization.\\nIn these cases, when you are generating data\\nstorytelling pieces, your audience is going\\nto be comprised mostly of non analysts\\nand non-technical business managers,\\ndecision makers and leaders.\\n\\nThey do not want to wade through a bunch of clutter\\nand a bunch of details that they don't need\\nto make a conclusion.\\nThey want you to tell them your findings within the data\\nand basically show that in a visual manner.\\nWhen you're creating a data storytelling product\\nthat would be in the form of something like a static image\\nor a very simple interactive dashboard, maybe something\\nthat you would create in an application like\\nXcelsius by SAP.\\nIn this course, you're going to be learning how\\nto use a library called Streamlink\\nto create interactive data visualizations.\\n\\nThis slide demonstrates a great example\\nof data storytelling.\\nIf you look at this example here, you should be able\\nto understand what the graphic is telling you within about\\nfive seconds.\\nThe headline says, \\\"What's really warming the air?\\\"\\nObviously, they're talking about global warming,\\nand there is a simple line chart with a trend\\nthat's increasing.\\nBasically, without even thinking,\\nyou can tell that whoever produced this data storytelling\\npiece is trying to tell you\\nthat there's an increase in global warming.\\n\\nDo you see how simple that is to analyze and understand?\\nYou want to make sure that your data storytelling products\\nare also simple and easy to understand in this way.\\nNow, let's look at data showcasing.\\nData Showcasing is where you showcase a lot of data so\\nthat your audience members can think for themselves.\\nThis is an opposition to what you do with data storytelling.\\nData storytelling is\\nwhere you present your findings in a very, very clear\\nand easy to understand way.\\n\\nWhen you do data showcasing,\\nthe audience is actually looking\\nto draw conclusions for themselves.\\nTo that end, you need to make sure\\nthat your data visualization is highly contextual,\\nthat you've included all sorts of background information\\nto help people in your audience draw their own conclusions.\\nThe data visualization should be open-ended.\\nIn other words, you are not telling them your findings,\\nbut allowing them to think for themselves.\\nThe intended audience\\nfor data showcasing products would be analysts, quants,\\nengineers, mathematicians, scientists, basically people\\nwith an analytical mindset, background in that,\\nbecause this is the kind of thing that's going to be\\nof value to them.\\n\\nIn terms of product types,\\nwhen you are doing data showcasing,\\nyou'll be producing static images\\nand interactive dashboards,\\nand you're going to learn how to do both\\nof these things throughout the remainder of this course.\\nHere's an example of data showcasing.\\nI actually took this example from my book,\\n\\\"Data Science for Dummies.\\\"\\nYou can see that here,\\nthere's a lot of data that's being showcased on one screen.\\nYou could probably spend 15 minutes visually exploring this\\ndata visualization, looking for trends,\\npatterns, and correlations.\\n\\nYou could come away from this with many,\\nmany different types of hypotheses.\\nData showcasing is a good tool for exploratory data analysis\\nand for allowing other analysts\\nto do exploratory data analysis on a visual basis without\\nthem actually having to get in\\nand generate that data visualization for themselves.\\nIt's a great presentation tool.\\nLastly, let's look at data art.\\nYou can use data art to make a statement.\\nMost data art is used in something like data journalism,\\nor if you're trying to get your community\\nor a community of activists to be engaged in taking action,\\nthen you would benefit by generating a piece of art\\nthat's based on data.\\n\\nSomething that's really riveting\\nand inspiring or provoking to get them to have an opinion\\nand take action based on the data you're presenting them\\nthrough the artwork.\\nYour data visualization should be attention-getting\\nand creative, if not controversial,\\nbecause the more controversial it is,\\nthe more action you're likely to inspire.\\nIntended audiences\\nfor data art tend to be people like idealists, dreamers,\\nartists, and social activists.\\n\\nData art is a tool to educate, inform, and motivate\\nthe public in most cases.\\nIn terms of product types,\\ndata art is almost always produced as static images.\\nIn some cases, it's also produced\\nas an interactive data piece on the internet.\\nStatic data visualizations are generally\\nsufficient here though.\\nHere's an example of data art\\nthat I took from periscopic.com.\\nIf you're like most people, then you look at this example\\nfor about 10 to 15 seconds\\nand you understand exactly what it's trying to say.\\n\\nIt's saying that US gun laws need to change.\\nThis data art is attempting to get people\\nto rally up against the Second Amendment\\nand get gun laws tightened up in America.\\nIt's using data art to achieve that goal.\\nAs you can see here on the left, you've got an account\\nfor the number of people killed,\\nand that's 9,595 people\\nas highlighted in orange.\\nThen on the right side of this example, you see\\nthat they have actually counted up an approximation\\nof the number of years that were lost from those lives,\\nfrom those people that were killed.\\n\\nWhat they're really trying to do in this piece is\\nthat they're trying to numerically exaggerate the\\ndetrimental impact\\nthat US gun laws have upon the population as a whole.\\nIn order to do that, they're breaking down the number\\nof people killed to the number of years of life\\nthat's been lost.\\nIt's clear that they really want to rally up people in a\\npolitical way and get them\\nto take action against a Second Amendment.\\nBy exaggerating impact, they're looking for ways to amplify\\nthat impact as much as possible by using a metric\\nthat's got the highest number possible.\\n\\nIn this case, it's number of years of life lost\\nbecause of the gun laws.\\nThen if you look here under the title, it's not the number\\nof years of life lost.\\nThey call it stolen.\\nThey're really trying to impassion people\\nto take action against gun laws.\\nThey're saying, okay, people's lives are being stolen here.\\nIn fact, that may or may not be true\\nbecause a lot of times death from guns happen as a result\\nof deliberate criminal activity that a person chooses\\nto participate in despite its high risk nature.\\n\\nIf a person dies via a gunshot wound in this case,\\nis that truly a life stolen?\\nIt's up for you to decide.\\nWhat's presented in this visualization is a\\nmatter of perspective.\\nWhoever created this is really trying\\nto push people in their audience to get out\\nand change gun laws.\\nThey're infusing their opinions in order to get that done,\\nbut they're using data art as a way to drive impact.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4590003\",\"duration\":417,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Selecting optimal data graphics\",\"fileName\":\"3006708_en_US_03_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":575,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to select optimal data graphics. This video covers graphics for storytelling, graphics for showcasing, and graphics for data art.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15285627,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] It is time to talk about selecting\\noptimal data graphics.\\nYou're going to use different types of data graphics\\ndepending on the type of data visualization you're creating.\\nAgain, referring back to the last section,\\nyou learned that you can create data storytelling pieces,\\ndata showcasing visualizations, or data art.\\nIf you were to choose graphics for data storytelling,\\nappropriate graphics for this type of data piece\\nwould be things like pie charts, line charts,\\nbar charts, even area charts, point maps,\\nor chloropleth maps.\\n\\nA chloropleth map is a map that shows area boundaries.\\nIf you looked at a map of the United States,\\nyou would see 48 different areas outlined,\\nthat's a chloropleth map.\\nNow, if you were to take a look\\nat a map of the United States\\nto examine major cities within the United States,\\nyou would see points to indicate\\nwhere the major cities are located,\\nthat's a point map.\\nThe reason why these types of graphics are appropriate\\nfor data storytelling\\nis that people understand what they mean.\\n\\nAgain, going back to the purpose of data storytelling,\\nthat purpose is to share your findings\\nwith an audience of people\\nwho are generally not very quantitative\\nor analytical by nature.\\nBecause this type of audience tends not\\nto be very analytical,\\nthey generally want to be given the findings\\nin a way that is comfortable\\nand easier for them to understand.\\nThat's why these are the best type of graphics\\nwhen we are creating data storytelling pieces.\\nNow, in terms of creating data showcasing pieces,\\nyou have a little bit or a lot more options\\nwith respect to the type of graphics you can use here.\\n\\nOf course, you can use the same type of graphics\\nthat you would in data storytelling,\\nbut you also want to add more context\\nto data showcasing pieces.\\nIn addition to pie charts, line charts,\\nand bar charts, you would also want to include\\nor could include things like histograms,\\nscatter plots, scatter plot matrices, and even raster maps.\\nTo add a little bit of clarity here\\non what exactly a raster map is,\\nI brought in a picture of a Doppler weather radar\\non top of a geographic map.\\n\\nBasically, what a raster map is,\\nis it's a raster file that's actually made\\nof an X and Y grid, which is filled\\nwith a variety of numerical values.\\nThe grid is X and Y.\\nThe numerical values are colored according to count.\\nIn this case, the raster is showing you\\nthat where it's red, the value is higher,\\nand then as the value decreases,\\nit goes from yellow to green to blue,\\nand then turquoise.\\n\\nRaster coverage is just a coverage\\non top of a geographic map that provides\\na layer of information about one metric.\\nSince data showcasing is intended\\nfor analytical audiences, STEM grads,\\nanalysts, engineers, quants, these type of people,\\naudience members usually want\\nthat extra bit of information,\\nand these would be an appropriate graphic type\\nto use in the data showcasing piece.\\nOkay, now let's look at data art.\\n\\nIn terms of data art, like we saw in the last section,\\ndata art is intended for the general public\\nor for activists, dreamers, and doers.\\nThey're generally not super analytical people,\\nso we would want to keep it simple for them.\\nYou could use a line chart, a graph network,\\nor a chloropleth map to communicate\\nas the data graphic within data art.\\nBut honestly, most of the time when you see data art,\\nthe data is shown in some sort of weird\\nor artistic representation\\nthat actually isn't a data graphic at all.\\n\\nThe creator often finds some sort of artistic way\\nto represent their data\\nand what they're trying to present about that data.\\nTo try and make it just a little more clear\\nabout what I mean when I say weird\\nor creative data visualization,\\nI brought in this data visualization\\nthat was presented by Mona Challabi and TEDx at NYC.\\nShe was basically educating her audience\\non three ways to spot a bad statistic.\\n\\nBut as you can see here,\\nthe state of the visualization is definitely provocative\\nbecause it's kind of gross.\\nWell, it's really gross,\\nand it's also really, really creative\\nas data visualizations go.\\nIn this case, the graphic represents\\nthe peak month of flu virus and the times per month\\nthat this season's peak since 1982.\\nThe creator is trying to make a point\\nabout when flu viruses peak,\\nand they've created this provocative data visualization\\nto get the point across.\\n\\nThis is a fine example of data art.\\nLet's now move on to the four steps\\nto choosing the right type of data graphic\\nwhen you're creating a data visualization.\\nStep one is that you make a list of questions\\nthat your data visualization is meant to answer.\\nStep two is to consider\\nwhether your data visualization type\\nshould be data storytelling,\\ndata showcasing, or data art.\\nYou would make that determination\\nby thinking about your intended audience,\\nwho is meant to consume the data visualization.\\n\\nStep three is just to answer the question,\\n\\\"What data graphic types are preferable\\nfor this type of data visualization?\\\"\\nWe covered this topic in this lecture here.\\nOnce you've answered that third question,\\nmoving into step four,\\nyou go ahead and you test out\\nthe different types of data graphics with your data\\nand decide which graphic type displays\\nthe most clear and obvious answer to your question.\\nLet me illustrate what I mean\\nbecause it could sound kind of vague\\nif you haven't seen an example.\\n\\nIn step four, we're actually testing out\\nyour data graphic to see\\nwhich is the most effective visual communication tool.\\nJust look at this example.\\nYou see that you have two data graphics.\\nThey both represent the same statistic,\\nbut you notice that the data graphic on the right\\ndoes a much better job of visually emphasizing\\nthe differences in values.\\nOn the left, you basically can't see\\nany difference whatsoever,\\nso it makes it very hard to communicate the data.\\n\\nYou should always test your different data graphics\\nto make sure that you use the one\\nthat is most clear and effective\\nin displaying your data\\nand the trends and patterns within the data.\\nThe graphic on the left below\\nis definitely not the most effective choice.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4588024\",\"duration\":408,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Communicating with color and context\",\"fileName\":\"3006708_en_US_03_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":590,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to communicate with color and context. This video covers color schemes, annotation, and visual context.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11462929,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now let's talk about communicating\\nwith color and context.\\nThe correct choice of color and context can really,\\nreally add a lot of value to your data visualization,\\nso it's an important topic for us to cover here.\\nStarting first with creating context with color.\\nColor should be used strategically,\\nsparingly, and consistently.\\nWe want to use color to draw attention\\nto the most important parts of your data visualization\\nand away from the parts that are not that important.\\n\\nThe first and foremost rule you need\\nto follow when you're choosing a color scheme\\nfor your data visualization is that you need to make sure\\nthat the color stage you choose\\nto use in your data visualization are all from the\\nsame color formula.\\nThere are many different options out there\\nfor color formulas.\\nPopular ones include monochromatic, analogous,\\nand complementary,\\nbut then there's also split complimentary, triadic\\nand tetradic and many other options out there as well.\\n\\nColor formulas are based on extensive color theory\\nthat's been adapted over hundreds of years,\\nand so you want to use them in order to ensure\\nthat you're using a color scheme\\nor a color formula that's going to be harmonious to the eye\\nof the end consumer.\\nYou also want to make sure that you have some shades\\nthat are useful for drawing the attention of the eye\\nand other shades that are more muted so\\nthat you can use your color strategically\\nto attract attention where it's needed.\\n\\nIn case you're looking for a good tool\\nto find good color schemes\\nto use in your data visualizations,\\nthere is an AI driven color matching tool\\ncalled Colormind IO.\\nI'll put a link for you here.\\nI took this color scheme\\nthat you're seeing on this side from Colormind,\\nand it's actually a very good example of something\\nthat could be used in a data visualization quite well.\\nAs you can see, there are two shades that really pop,\\npurple and red, and the other three shades are muted\\nor they don't draw a lot of attention,\\nso you can use the muted shades to basically plot out data\\nthat is of lesser interest.\\n\\nAnd then the purple\\nand red shades would be good\\nfor really showcasing whatever trend\\nor finding that you're demonstrating.\\nIn other words, the part of the data visualization\\nthat you want to stand out the most to the consumer.\\nAll of the color schemes that are generated\\nwith a Colormind tool include shades\\nthat are within the same color formula.\\nThis tool takes care of that for you, which is very nice.\\nNow, moving into annotation,\\nyou can see here an example I got from USA ID\\nwhere they're basically using annotation to add context\\nand tell the story around how the data is performing.\\n\\nUsing annotations as context is useful\\nfor providing your audience some information about why the\\ndata behaves as it does.\\nAnnotation is especially useful in data storytelling\\nand data showcasing because they add a layer of context\\nand meaning to the data visualization\\nthat helps consumers understand\\nwhat it is that you're showing them.\\nAnother way that you can add context\\nto a data visualization is to introduce graphic elements.\\n\\nIn this case, we're looking at a trend line,\\nbut also single value alerts\\nand benchmarks are very useful in helping your audience\\nunderstand the relative significance\\nof the data that you're showing them.\\nLet's talk a bit about the mechanics\\nbehind creating context in a data visualization.\\nYou can create context in your data visualization just\\nby adding data on additional and relevant metrics.\\nFor example, if you were\\nto be featuring some statistics on dropout rates in a\\nparticular region of a city, some data\\nthat might be a good contextual layer\\nto add would possibly be data on the household\\nand the parents' level of education,\\nand if the parents were also dropped out of high school,\\nbecause it would seem that if a child comes from a household\\nwhere both of the parents\\nor one of the parents dropped out of high school\\nand did relatively okay with their life,\\nthat the child would also feel like dropping out of\\nhigh school would be a viable alternative for them.\\n\\nSo that would be an example of adding relevant data sets\\nand additional metrics around\\nwhat you're featuring in your data visualization.\\nAbout why you'd want to create context.\\nBy adding context,\\nyou're giving your audience a deeper perspective\\nand insight into what's happening with the situation\\nthat's represented by your data visualization.\\nOther ways that you can add context are, again,\\nlike I stated, just adding trendlines, single value alerts,\\na strategic use of color and annotations.\\n\\nAdded context is especially useful in data showcasing\\nbecause again, data showcasing pieces are meant\\nfor analytical audiences that really want to put a lot\\nof thought into drawing conclusions from the data\\nthat they're looking at.\\nBut that said, context is also useful in data showcasing,\\nbecause usually in data showcasing,\\nyou would just add a little bit of textual context\\nor maybe a trend line to clarify exactly what you're trying\\nto tell your audience.\\n\\nYou wouldn't expect them to go in\\nand read multiple layers of annotation\\nor to go deep into evaluating the visualization,\\nbut you may want to give them some text\\nor markers in order to make it abundantly clear\\nwhat you're trying to tell them\\nwith your data visualization.\\nHere's an example of a good solid data visualization.\\nWhat I really love about this data visualization is its\\nstrategic use of color.\\nThe color is done right.\\nThis data visualization is taken from Data Lab's agency,\\nand they have really done a great job in using color\\nto draw attention to what matters most in this piece.\\n\\nThey are using more muted shades in order\\nto underlie and support their data visualization\\nfor completeness, but they use the red\\nand orange to really draw attention\\nto the points they're trying\\nto make within the data visualization.\\nNow that you know the basics of data visualization,\\nthe types of data visualizations you can create,\\nhow to select optimal graphics\\nand how to communicate well with color\\nand context, it's time to move into the next section\\nof the course where we'll talk about practical data\\nvisualization and how\\nto build data visualizations using Python.\\n\\n\"}],\"name\":\"3. Data Visualization 101\",\"size\":45937181,\"urn\":\"urn:li:learningContentChapter:4586146\"},{\"duration\":5866,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4590004\",\"duration\":1069,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introduction to the matplotlib and Seaborn libraries\",\"fileName\":\"3006708_en_US_04_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1651,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Get a practical introduction to Python's matplotlib and Seaborn libraries. Learn how to set up an environment for creating data visualizations.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":36335610,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this lecture, we'll explore two\\nof Python's most powerful visualization libraries,\\nMatplotlib and Seaborn.\\nWhile both are highly capable,\\nthey have distinct features and use cases.\\nSeaborn stands out for its simplicity and its elegance.\\nIt requires less complex syntax\\nand offers beautifully designed default themes,\\nmaking it ideal for creating sophisticated visualizations\\nwith very little effort.\\n\\nOn the other hand, Matplotlib excels at customizability.\\nIt allows for extensive visual modifications\\nthrough direct axis to its classes.\\nIt suffers greater control if you need\\nto fine tune every aspect of your charts and plots.\\nIn this module, I'm going to give you an overview\\nof how to effectively utilize both libraries\\nfor your data visualization needs.\\nMatplotlib and Seaborn are versatile libraries\\nfor creating a wide range of standard chart graphics.\\n\\nThey're useful for visualizing all types and kinds of data.\\nThis, of course, includes but isn't limited\\nto financial data, operational data,\\nemployee data, engagement and behavioral data.\\nOne thing to note, however, is\\nthat you can display network data\\nusing standard chart graphics,\\nbut you cannot create network graphs when you're working\\nwith only the Matplotlib or Seaborn library.\\n\\nMatplotlib is typically employed for generating basic plots.\\nIts visualization capabilities commonly include\\ncreating bar charts, line charts, scatter plots,\\nand similar straightforward graphical representations.\\nConversely, Seaborn offers an extensive array\\nof visualization options.\\nIt's known for its beautiful themes,\\nand Seaborn allows for the plotting of complex graphics\\nwith relatively minimal coding effort.\\n\\nIt specializes in statistical visualizations,\\nmaking it ideal for summarizing\\nand displaying distributions within datasets.\\nNow let's look at how we can utilize both Matplotlib\\nand Seaborn to create various types of graphics.\\nOkay, in this coding demonstration,\\nI just want to show you how to get started really quickly\\nwith Matplotlib and the Seaborn library.\\nSo we're first going to start\\nby installing the required libraries.\\n\\nSo for that, we'll need to do a pip install.\\nAnd we will pip install Matplotlib,\\nand run this.\\nAnd we'll also need to do a pip install\\nof the Seaborn library.\\nOkay, so we've got all of those installed.\\nNow we need to import them into our IPython environment.\\nSo we'll do that now\\nby just saying import matplotlib.pyplot as plt,\\nand import seaborn as sns.\\n\\nAnd then for this demonstration,\\nwe also need to work with DataFrames.\\nSo we'll say from pandas import DataFrame.\\nOkay, so then we'll run this.\\nOkay, now we have imported both,\\nall of the libraries that we need here.\\nSo let's get started looking\\nat how we can use them to generate plots.\\nI will go ahead\\nand just create a dataset that we can use\\nfor practicing our data visualization.\\n\\nSo we'll just call it data.\\nAnd then let's create a dataset\\nwith the names of individuals\\nalong with their age, gender, and rank.\\nThis is a dataset that we actually\\ndefined in an earlier lecture,\\nso I won't spend too much time typing everything out.\\nThe names here should be steve, john, richard,\\nsarah, and julie.\\n\\nSo we'll pass these strings\\ninto the list.\\nAnd then,\\nwe will define the age of each of these people.\\nSo create a column named age,\\nand we'll set the ages equal to,\\n20, 22, 20, 21, 24, 23, and 22.\\n\\nAnd it looks like we forgot a bracket.\\nSo let's close that.\\nAnd then create the next column, which is going to be gender.\\nWe'll say gender,\\nand set that equal to Male, Male, Male,\\nFemale, Male, Male, and Female.\\nAnd then the next column we need is rank.\\nWe'll create a rank column\\nand then we will assign a rank to each of these people.\\n\\nAnd that is 2, 1, 4, 5, 3, 7, 6.\\nSo now we have a dataset that we can work with.\\nLet's create a DataFrame from this dataset.\\nTo do that, we just use the DataFrame constructor\\nand pass in data,\\nand we'll set this equal to df,\\nand then let's just print it out.\\n\\nOkay, it looks like we have a syntax error,\\nwhich should be pretty easy to fix.\\nThe string just was not closed here for gender.\\nRun this again. Okay, so here is now our DataFrame.\\nAnd let's use this to create a Matplotlib bar chart.\\nAnd we'll just start by creating a simple bar chart.\\nTo do that, we will use the bar function.\\n\\nAnd in the function, we'll pass the DataFrame's names column\\nas the first parameter,\\nand then we'll pass in the DataFrame's age column\\nas a second parameter.\\nSo let's get started with that.\\nWe'll say plt.bar,\\nand then of the df DataFrame,\\nwe want to select names.\\nAnd then we also want\\nto select the age column from the DataFrame.\\n\\nSo we'll say df, and then we'll select here, age.\\nThe values of the first parameter will be shown\\nacross the X-axis, and then the values\\nof the second parameter will be shown across the Y-axis.\\nSo let's go ahead\\nand set some labels for both of these axes.\\nIn order to do that, you would use the xlabel function,\\nand so that looks like plt.xlabel.\\nAnd yeah, so along the X-axis we're going to have Names.\\n\\nAnd so we'll just create a label here for that axis.\\nAnd then for the Y-axis ylabel,\\nwe're going to have Age.\\nSo this is going to be shown along the Y-axis.\\nAnd let's also create a title for this plot.\\nTo do that, we would say plt.title,\\ncalled the title function,\\nand then pass in a string for the title of the plot.\\n\\nLet's call it Comparing Ages.\\nAnd then let's print the plot out.\\nTo do that, you use the show function.\\nSo plt.show,\\nand run this.\\nAnd okay, great.\\nSo now we actually have a bar chart\\nthat shows the ages of each\\nof the people in our DataFrame.\\n\\nNow let's create a similar bar graph using Seaborn.\\nAnd to do that, we'll call Seaborn's bar plot function.\\nIn the bar plot function,\\nwe'll just pass the DataFrame\\nand specify the columns we want to show on the X and Y-axis.\\nSo let's test that out real quick.\\nWe'll say plot, we'll say plot is equal to sns.barplot.\\n\\nAnd then we will say\\nthat our data is equal to the DataFrame function here.\\nOur x value is equal\\nto the names column.\\nSo we're selecting the names column for the X-axis.\\nAnd the Y-axis, we will select the age column.\\nAnd then let's create a title for this plot.\\nTo do that, we use the set_title function.\\n\\nSo this is plot.set_title.\\nAnd again, we'll just call it Comparing Ages.\\nNow to plot out this graph,\\nwe would just say plt.show,\\nand run that.\\nAnd here you can see we have similar object,\\nbut now it's a Seaborn bar graph.\\n\\nAnd as you can see, there are small differences\\nbetween the Matplotlib one and the Seaborn one,\\nbecause this name's variable\\nand this age variable have different casing than above,\\nbut in general, they look pretty darn similar.\\nThe key difference here though, is you can see\\nwith the method we're creating the bar chart\\nusing Matplotlib, it just requires more code\\nto generate the same graph.\\nSo it's pretty much always the case\\nthat Seaborn is more efficient\\nat data visualization than Matplotlib.\\n\\nAnd in general, it's more beautiful.\\nIt comes up with graphs that are more beautiful,\\nalthough in this example we aren't seeing a huge difference.\\nLater on in the course, you will start\\nto see the differences between Seaborn and Matplotlib\\nin terms of aesthetic.\\nDrawing all sorts of other types of graphs\\nwith Matplotlib and Seaborn are also pretty straightforward.\\nSo let's go ahead and create a line plot with Matplotlib.\\n\\nTo create a line plot with Matplotlib,\\nyou just use the plot function, so that's plt.plot.\\nAnd then let's pass in the DataFrame,\\nand let's put out again the names and age\\nof the people in this DataFrame.\\nSo put name.\\nAnd then for our Y-axis,\\nlet's select the age column here.\\n\\nAnd we can name the label names the same\\nas the bar chart here.\\nSo we can actually just copy these labels,\\npaste them down here.\\nAnd then to take a look at it,\\nwe would just say, plt.show,\\nand run this.\\nAnd okay, so here we have the ages\\nof our people plotted out in line chart format.\\n\\nLet's also look\\nat how we can create a line plot using Seaborn.\\nSo again, this is going to be a lot more simple.\\nWe'll create an object called plot,\\nand we'll say that it's equal to the DataFrame.\\nWe need to call the line plot function.\\nSo that's sns.lineplot,\\nand we'll pass in our df DataFrame\\nas our data here.\\n\\nAnd then again, we will just take X and Y-axis.\\nWe can just copy and paste these from above.\\nSo that's all the same as the bar chart.\\nAnd then to plot this out, we'll say plt.show.\\nOkay. And we'll run this.\\nAnd as you can see, it looks pretty much exactly\\nlike the line plot we created with Matplotlib,\\nwhich makes sense since it's exactly the same dataset,\\nbut it's just again, a lot more efficient\\nto use this method to create a line chart\\nthan it is to create a line chart in Matplotlib.\\n\\nLet's really quickly look at how\\nto create a pie chart in Matplotlib and in Seaborn.\\nSo for Matplotlib, we need to use the pie function.\\nIt will say plt.pie.\\nAnd then let's plot out the age column.\\nSo we'll select that from our DataFrame here,\\nand we'll add some labels.\\n\\nTo do that, we create a labels parameter\\nand we set it equal to the names column.\\nSo we'll select df, names column here.\\nAnd then let's create a title for the chart.\\nSo to do that, we use the title function,\\nwe pass in a title that says Age Comparison,\\nand then we can print this out.\\n\\nI have a typo here\\nwhere it should be title, so I'll fix that.\\nAnd then, okay,\\nso here's our Matplotlib pie chart.\\nNow, Seaborn doesn't have a default function\\nto create pie charts,\\nbut we can use the syntax in Matplotlib\\nto create a pie chart\\nand then add the Seaborn color palette.\\nLet me show you how to do that real quick.\\nSo we'll create a variable say, colors,\\nand then we we'll call the color_palette function.\\n\\nSo that's sns.color_palette.\\nLet's take a look and get the pastel color palette.\\nAnd then say if we only want five colors,\\nwe can just slice the color palette from zero to five.\\nAnd then what that will do is\\nit will just extract the color palette list,\\nand the colors that are associated\\nwith position 0, 1, 2, 3, and 4.\\n\\nAnd then now we have a color palette set up.\\nSo let's call the pie function, which is plt.pie,\\nand we'll pass in our DataFrame and select the age column.\\nAnd then let's set our labels equal to the names column.\\nSo to do that, we say labels is equal to df,\\nand select names here.\\n\\nAnd then lastly, we'll define the colors\\nequal to the colors palette\\nthat we just created in the line of code above.\\nAnd then we can run that.\\nSo I just ran this without using the show function.\\nAnd then that's actually what you get,\\nis you get all of this extra information about the plot\\nthat you don't necessarily want or need to see.\\nSo just to clean this up, I'm going to say plt.show\\nand run it again.\\nAnd here now we have a Seaborn chart.\\n\\nSo we have a pie chart that we actually used Matplotlib\\nto help build.\\nAnd we've then applied the color palette\\nfrom Seaborn to this pie chart.\\nAnd those are the basics of just how to get started\\nusing Matplotlib and Seaborn libraries.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4584142\",\"duration\":625,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating standard data graphics\",\"fileName\":\"3006708_en_US_04_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1141,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to create standard data graphics. This video covers line charts, pie charts, and bar charts.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":20015180,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Standard chart graphics are excellent tools\\nfor conveying simple data insights in a way\\nthat anyone can understand.\\nFor example, imagine you are an E-commerce business analyst\\nfor a company that just made some major changes\\nto its website layout.\\nYou are visualizing the site usability insights\\nthat you've discovered to convey findings to your manager.\\nTo do this, you use a line chart,\\na bar chart, and a pie chart.\\nYou use a line chart to show how the total number\\nof items purchased per day has increased since the change.\\n\\nYou'll use the bar chart to show how the number\\nof purchases increased for customers\\nin the 18 to 25-year-old category,\\nbut that it decreased\\nfor customers in the greater than 44-year-old category.\\nLastly, you use a pie chart to show what categories\\nof products generate the greatest proportion of sales\\nand how the site changes affected that proportion,\\nboth before and after.\\nTo summarize, you'd use a line chart\\nto show the changes over time,\\nyou'd use a bar chart to show changes in categorical data,\\nand you'd use a pie chart to visualize categorical data\\nas a proportion of a whole.\\n\\nNow, we have two methods for plot building,\\na functional method and an object-oriented method.\\nIn the demonstration we're about to do,\\nI'm going to show you\\nhow to build plots using the functional method.\\nWith the functional method, you build plots\\nby calling the plotting function on a variable\\nor set of variables.\\nWith object-oriented plotting,\\nyou build a plot by first generating a blank figure object\\nand then populating that object with plot and plot elements.\\nWe'll be doing that in later demonstrations.\\n\\nIn the coding demonstration to come, I'm going\\nto show you the most popular data visualization libraries\\nin Python, which are, of course, Matplotlib and Seaborn.\\nIn this demonstration,\\nI'm going to show you how to use Matplotlib.\\nLet's get started.\\nSo here we have our Jupyter Notebook,\\nand I'm giving you the mtcars dataset preloaded here\\nwith the column names and everything prepared for you,\\njust so we don't have to type that out again.\\n\\nBut first things first, let's go ahead\\nand import our libraries, and in this demonstration,\\nwe're going to use NumPy.\\nSo we'll import numpy as np.\\nWe're also going to be generating random numbers\\nin this demonstration, so within NumPy,\\nwe need to import randn.\\nSo let's say from numpy.random,\\nimport randn,\\nand then we always of course need to import pandas,\\nand we'll import that as pd.\\n\\nAnd then let's bring in our series and data frame.\\nSo we'll import Series and DataFrame,\\nand we're going to be using Matplotlib in this demonstration\\nso we'll import matplotlib.pyplot\\nas plt,\\nand from matplotlib,\\nlet's also import the rcParams.\\n\\nGreat, and so now let's just run this\\nand we'll have our libraries ready for us.\\nNow, the first thing we're going\\nto work on here is that we're going\\nto create some line charts, so the first line chart,\\nlet's just generate two variables, an x and a y variable,\\nand x will be equal to a range\\nof values between one and nine.\\nSo we'll say x equal to range one through 10,\\nand y is equal to a list,\\nand it's just going to be a list of numbers,\\none, two, three, four,\\nzero, four, three, two, one.\\n\\nTo generate the line plot,\\nwhat we need to do is call the plot function,\\nso that's plt.plot,\\nand then let's just pass in the x and y,\\nand run this.\\nOkay, so we have our line chart,\\nand that's pretty straightforward.\\nNow let's work on creating a line chart\\nfrom a Pandas object.\\nWe've already got your data loaded\\nand ready to go inside of the Jupyter environment,\\nbut we need to go ahead and isolate a variable.\\n\\nSo let's isolate the mpg variable,\\nwhich we're doing here, mpg,\\nand we're setting it equal to the cars data frame,\\nand we've selected the mpg column out of that data frame.\\nSo the first thing I want to do is just run this,\\nand then we have our mtcars dataset available to us in here.\\nAnd now, let's just plot out the mpg variable,\\nso to do that, we'll do mpg.plot, and we'll run this.\\n\\nHere we go, we have a line chart\\nthat shows the values within the mpg variable.\\nNow, I want to show you\\nhow to actually plot out three variables in a chart.\\nSo to do that, let's create a data frame called df,\\nsay df is equal to cars,\\nand then let's select the variables we want to plot\\nand let's make those, let's make those cyl,\\nweight, and mpg.\\n\\nAnd then to plot it out,\\nwe just call the plot method off of the DataFrame object\\nand run this.\\nOh, it looks like I need to actually add another set\\nof brackets here.\\nAnd I'll run it again.\\nOkay, cool, so now we have a line chart\\nwith three variables plotted out instead of just one\\nand we've got a nice little legend here\\nso that we can make sense of which variable is which.\\n\\nNow, let's look at how to create bar charts.\\nTo create a bar chart, you would just call the bar function,\\nso that's plt.bar.\\nAnd then let's pass in our x and y variable\\nand run this, and it's easy as that.\\nWe've got a bar chart.\\nLet me show you how to create a bar chart\\nwith a Pandas object.\\nTo do that, you would just call the plot function,\\nso let's plot out our mpg variable as a bar chart.\\n\\nTo do that, we will say mpg.plot,\\nand then we'll pass a parameter\\nthat says kind is equal to bar,\\nand run this.\\nAnd now we have our mpg variable plotted out as a bar chart.\\nIf you wanted to plot this horizontally instead\\nof vertically, all you would need\\nto do is use the same command,\\nbut the parameter would instead be kind equal to barh,\\nand then you would get a horizontal bar chart,\\nso let me show you that real quick.\\n\\nI'll just copy this code,\\nand add a H here for this.\\nNow we have a horizontal bar chart, and that is easy-peasy.\\nThe last thing I want to show you\\nin this demonstration is how to create pie charts\\nand actually print them out.\\nSo for this example, let's create a new variable x,\\nand let's say that x is just a list\\nwith the values one, two, three, four, and 0.5.\\n\\n'Kay.\\nAnd then if we wanted to create a pie chart from this,\\nwe would just call the py function, plt.py,\\nand pass in our variable x,\\nand then to print it out, we would call the show function,\\nwhich is plt.show,\\nrun this.\\nAnd okay, now we have a very, very simple pie chart.\\nBut what I really wanted to do\\nin this demonstration was actually to show you\\nhow to save this pie chart as an image file.\\n\\nSo to do that, first, let's generate the pie chart again.\\nSo we'll say plt.py, and then we'll pass in our x variable.\\nAnd then to save this as an image file, we actually need\\nto use the savefig function,\\nso that's plt.savefig,\\nand we'll pass in a string with the file name,\\nand we'll call that pie_chart.png,\\nand then we can just plot this out\\nusing the show function here,\\nand great, okay, so that's still the same pie chart,\\nbut what this has actually done\\nwith the savefig function is it's saved this pie chart\\nas an image.\\n\\nSo you can see that here in our Notebooks folder,\\ndown at the bottom, you'll see pie_chart.png,\\nand here we go, we have the same image\\nthat is from our notebook\\nthat has been now printed within a separate image file.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2715031\",\"duration\":748,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Defining elements of a plot\",\"fileName\":\"3006708_en_US_04_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1306,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to define elements of a plot. This video covers object-oriented plotting, sub-plots, and axis labels.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":23712323,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now you're going to learn\\nabout defining plot elements using Matplotlib.\\nPlot elements add context to your plot\\nso that the plot effectively conveys meaning to its viewers.\\nYou set axes limits to make sure\\nthat your chart is well fit to your data graphic,\\nyou set axes tick marks and plot grids\\nto make it easier and faster for viewers\\nto interpret your chart at a glance.\\nYou can use subplots to visually compare changes\\nin data values under different conditions,\\nlike in different seasons,\\ndifferent locations, or in different years.\\n\\nThere are two methods for building plots.\\nYou can use the Functional method\\nor the Object-oriented method.\\nAdding plot elements is an essential part\\nof Object-oriented plotting.\\nWe discussed the functional plotting\\nin the last demonstration.\\nSo now it's time to go over Object-oriented plotting.\\nAgain with Object-oriented plotting,\\nyou build a plot by first generating a blank figure object\\nand then populating that object\\nwith plots and plot elements.\\nThere are four steps in Object-oriented plotting.\\n\\nFirst one is to create a blank figure object.\\nThen second, you will add axes to the figure.\\nThird, you'll generate a plot\\nor plots within the figure object.\\nAnd then fourth, you will specify plotting\\nand layout parameters for the plots within your figure.\\nOne last thing I want to touch on here\\nbefore we go into the demonstration is subplots.\\nA Subplot is a plotting figure\\nthat contains more than one plot or subplots.\\n\\nIt's easy to generate a subplot in Matplotlib\\nwith the subplots function.\\nLet's look at how to define elements\\nof a plot using Matplotlib.\\nSo this Jupyter notebook is coming preloaded\\nwith the libraries you'll need,\\nwhich is NumPy, Pandas and Matplotlib.\\nAnd I wanted just to start off this demonstration\\nby showing you how to set the figure size\\nand inline settings for your Matplotlib plots.\\n\\nSo to make sure that your plots print out properly\\nwithin Jupyter notebooks,\\nyou want to use the command matplotlib inline.\\nWhat this does is it tells Python\\nto print your Matplotlib plot in line\\nwith your Jupyter notebook.\\nAnd then you'll also probably want to go ahead\\nand do define a figure size.\\nSo to do that, you're going to use the rcParams function.\\n\\nThen create a list,\\nand you want to define the figure and figure size.\\nSo you'll say fig.figsize.\\nIn manual, just set this equal to the dimensions\\nyou want for your plots.\\nSo in this demonstration,\\nlet's just print plots\\nthat are 5 inches wide and 4 inches tall.\\nSo we'll put 5, 4 here.\\nAnd then we basically have set the parameters\\nfor generating plots within our Jupyter notebook.\\n\\nSo we run this.\\nNow let's go ahead and define axes, ticks and grids.\\nTo do that, we first need to generate some variables.\\nSo let's create an x variable here,\\nand then we'll say that the x variable\\nis a series of numbers between 1 and 9.\\nSo we'll use the range function\\nand we'll pass in the starting value of 1,\\nand then the end value is 10,\\nwhere 10 is actually excluded\\nfrom the numbers that are generated.\\n\\nAnd then another variable will be y\\nand we'll set y equal to a list.\\nThe values in this list will be\\n1, 2, 3, 4, 0, 4, 3, 2, 1.\\nNow let's generate a figure.\\nSo to do that, we will say fig is equal to plt.figure,\\nwe'll call the figure function here.\\n\\nAnd this is basically going to create a blank figure.\\nThe next thing we're going to do\\nis that we're going to add axes to this figure.\\nTo do that, you call the add axes method\\noff of this fig object.\\nSo we say fig.add_axes, axis.\\nAnd this will be our ax object.\\nSo I'll set this whole thing equal to ax.\\n\\nAnd for our axes parameters,\\nlet's just pass a list\\nthat will contain the numbers we want.\\nSo that'll be 0.1, 0.1, 1, 1.\\nAnd now let's plot this out.\\nTo plot it out, all we need to do is call the plot method\\noff of the ax object\\nand pass in our x and y variables.\\n\\nThere is a extra equal sign.\\nSo then we'll run this.\\nAnd there we have it.\\nWe have a simple line chart,\\nbut now we have created tick marks along the axes,\\nand we've created a figure\\nin which the line chart can be displayed.\\nThese are the basics of Object-oriented plotting.\\nNow let's go into some more details.\\n\\nWe're going to add limits\\nand tick marks on the x and y axes.\\nSo we'll start again with our figure and our axes.\\nAnd I'll just go ahead and copy this code\\nso we don't have to type it all out again.\\nNow let's add some limits to our x and y axes.\\nTo do that, you're going to call the xlim function\\nand set the 8 ylim function.\\nAnd basically what we want to do\\nis we want to set the limits of our axes,\\nand then we need to pass a list\\nthat tells Python what we want those limits to be.\\n\\nSo in this case, that will be 1 and 9.\\nSo we'll say ax.set_xlim\\nand then the limit is going to be 1 and 9.\\nAnd then moving into the ylim,\\nwe say set_ylim,\\nand we define those limits.\\n\\nSo those are going to be 0 and 5.\\nAnd then we also want to just practice\\nwith setting some tick marks.\\nSo to do that, we're going to use the xticks function\\nand set yticks function.\\nSo to do that, we're going to use the set xticks function\\nand the set yticks function.\\nSo let me just write those out here\\nand we'll say set_xticks.\\n\\nLet's also just set it up for our y tick marks.\\nI'll copy this over.\\nOkay, great. So now let's pass in a list here\\nand say what we want our tick marks to be.\\nAnd so for the x axes, let's just make that,\\nmake those 0, 1, 2, 3, 4, 5, 6, 8, 9 and 10.\\n\\nAnd then for our y ticks,\\nlet's make those 0, 1, 2, 3, 4, and 5.\\nAll right. And then we'll plot this out\\nusing the plot function.\\nSo now we have another line chart,\\nbut you can see now that we've added tick marks\\nand we've also added limits to our axes.\\nSo if we look back and forth between the prior graph\\nand this one, you can see the changes.\\nSpecifically, you can see that\\nthe 7 tick mark is missing here on the x axis,\\nand the maximum limit of the y axis is now 5 here\\ninstead of 4,\\njust above 4 in the prior graph.\\n\\nAnother thing you can do is that you can add a grid\\nto the background of your chart.\\nSo let me just show you how that works real quick.\\nWhat I'll do is I'll copy in this code from above,\\nand then we'll just add a grid to the plot.\\nAnd to do that, we just call the grid function.\\nSo we'll say here,\\nI'm going to take these tick marks out,\\nand then we'll say ax.grid\\nand then we'll plot it out, ax.plot\\nand then we'll pass in our x and y variables.\\n\\nAnd then we'll run this\\nand we'll get a nice little grid background\\non our line chart so that it makes it more easy to read.\\nYou can also generate multiple plots in one figure\\nwith a subplots function like I mentioned in the lecture.\\nSo let's go ahead and do that.\\nFirst things first, of course, we need to generate a figure.\\nSo in this case, it's going to be fig is equal to plt.figure.\\n\\nThis is the same as last time,\\nbut we're going to adjust the figure\\nto actually include two subplots.\\nSo to do that we need to say fig,\\nand then we need to create two axes here.\\nTo do that, we're going to create a tuple,\\nand we're going to say ax1 and ax2.\\nThen we're going to set these\\nequal to the subplots function.\\nThat's plt.subplots,\\nthen we want to define\\nwhat we want our subplots to look like.\\n\\nSo let's say we have\\n1 row and 2 columns.\\nSo we would just say 1 and 2 here.\\nOkay. And then to plot it out,\\nwhat you need to do\\nis you need to actually plot both axis,\\naxis 1 and axis 2.\\nSo we'll say ax1.plot.\\n\\nThen we'll pass in the x variable.\\nAnd ax2.plot\\nand we'll pass in the x and y variable here\\nso that we have a little bit\\nof a difference in our plots.\\nAnd then when I run this,\\noh before I do that, it's in markdown,\\nso I need to just make sure it's in Python.\\n\\nAnd then when I run this,\\nI'm missing a comma here.\\nSo I'll run this\\nand I need to add the figure subplots, okay.\\nHere we go.\\nSo now we have two subplots.\\nWhat we've actually done though\\nis we have created one figure with two axes,\\nand then we have generated plots within that one figure.\\nPretty cool, huh?\\n\"},{\"urn\":\"urn:li:learningContentVideo:4588025\",\"duration\":901,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Plot formatting\",\"fileName\":\"3006708_en_US_04_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"2 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1357,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to format plots. This video covers custom plot colors, line styles, and marker styles.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":31101182,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this section, I'm going to show you\\nhow to define custom plot colors,\\nline styles, and marker styles.\\nThe reason I think this is important to show you\\nis that enhancing chart colors and markers\\ncan help you convey a point\\nin a way that is visually easier to understand.\\nJust think, if all the slices of a pie chart\\nhad the same color, the chart would be meaningless.\\nAnd if all the lines in a line chart were the same color\\nand use the same markers,\\nyou wouldn't be able to decipher one from the other,\\nand the chart would have very little or no meaning.\\n\\nMatplotlib offers colors, line styles,\\nand marker style options\\nto help you build clarity into your data visualization.\\nMatplotlib has unlimited color options,\\nso you can set the color parameter\\nequal to the name of your color you want,\\nor you can set the color parameter equal to RGB\\nor RGBA color codes for even more customization.\\nMatplotlib also has a variety of line styles.\\n\\nIf you look at this chart here,\\nyou can see some of the codes\\nthat you would use to set those styles.\\nAlso, I'm going to show you how to do this\\nin the coding demonstration.\\nMatplotlib also has a lot of different options\\nin terms of marker styles.\\nI created this handy-dandy little chart\\nfor you to review\\nin case you want to get fancy with your charts,\\nyou can look it over,\\nbut I'm also going to show you\\nhow to use these markers in Python in just a second.\\nYou've already learned to generate your plots,\\nso let's go ahead and start working on customizing them.\\n\\nSo for this coding demonstration,\\nthe Jupyter Notebook is already coming preloaded\\nwith the libraries you need, which are NumPy,\\nPandas, and Matplotlib.\\nAnd also, I've gone ahead and set the parameters\\nin terms of the plot size.\\nWe discussed that in an earlier lecture.\\nSo all of this information is preloaded for you,\\nand we're going to start off by working to define plot color.\\nWe'll just run these.\\n\\nSo the first thing we need to do\\nis just to create some variables,\\nand we'll create an X variable and a Y variable.\\nWe'll set X equal to\\na series of numbers between one and nine.\\nTo do that, we'll call the range function\\nand we'll pass in, starting number one and number 10,\\nwhere the last number is excluded in the output.\\nSo that will generate a series of numbers from one to nine,\\nand then we'll define Y as\\nthe numbers one, two, three, four,\\nzero point five, four, three, two, one.\\n\\nOkay, so we have two variables,\\nand now let's just plot this out as a bar chart.\\nSo to do that, we'll say plt.bar,\\nand we'll pass in our two variables, X and Y,\\nand run this.\\nOkay, so now we have a nice little bar chart.\\nNow let's define some colors here.\\nLet's change things up a bit,\\nbecause the default colors are kind of boring,\\nand we could create something a lot more fun.\\n\\nSo we'll change the width of the bars\\nand we'll also change the color.\\nHow about that?\\nWe can also go ahead and change the alignment a bit.\\nSo first things first,\\nlet's create a list to define the width of our bars.\\nTo do that, we'll just create a variable name,\\nwe'll call it wide,\\nand we'll say wide is equal to a\\nkind of just a mix and match of some sizes here.\\nSo zero point five, zero point five, zero point five,\\nand then maybe we'll make\\na bar that's zero point nine wide,\\nanother one that's zero point nine wide,\\nanother one that's zero point nine wide,\\nand then we'll go back to bars of width zero point five.\\n\\nSo I'll just kind of paste these in here.\\nOkay, cool.\\nSo there we have some widths\\nthat Python is going to use to adjust our default bar chart.\\nSo we'll use the color label here\\nand we'll say, \\\"Color is equal to,\\\"\\nand then we'll create a list,\\nand we'll pass on a string that reads, \\\"Salmon.\\\"\\nNow, to plot this bar chart out,\\nwe again need to use our bar function,\\nso that's plt.bar.\\n\\nAnd then we want to pass in the X variable,\\nthe Y variable,\\nand then we wanted to find the width, color,\\nand alignment parameters.\\nSo to do that, we'll say width,\\nand we'll set width equal to wide,\\nwhich is the variable we just created.\\nWe'll set color equal to color,\\nand then we'll say that we want to align\\nthe bar in the center,\\nso we'll say align equal to center here.\\n\\nNow the color is going to be equal to color,\\nwhich we just defined a sum in,\\nand then we set the alignment to center.\\nAnd alignment refers to\\nwhere the bars actually plot out within the chart,\\nso we want them to be centered when they plot.\\nSo we have set the line parameter equal to center,\\nand so now all we need to do here is print this out.\\nOkay, so, cool.\\nYou can see we have made these outer bars,\\nand they're a lot more narrow than the ones in the center,\\nand the color has been changed to salmon,\\nand you can also see that the alignment of the bars\\nis centered between each of the intervals\\ninstead of kind of being pushed inwards.\\n\\nLooking at the difference between the two,\\nyou can see that, yeah, it's definitely made a difference.\\nSo we've got that there.\\nAnd now let's go ahead and start working with\\nsome data frame objects.\\nThe Jupyter Notebook is coming preloaded\\nwith the NumPy cars data set.\\nOf course, you are going to need to customize your address\\nand specify where that data set is stored on your computer,\\nbut I've named all the columns and everything,\\nso the next thing you'd want to do\\nis just to create a data frame.\\n\\nI'm going to run this.\\nAnd we will call the data frame DF,\\nand we'll set it equal to cars,\\nand then let's just go ahead and\\nselect the cylinder column,\\nthe MPG column,\\nand the weight column,\\nand then plot it out with the plot method,\\nso df.plot, and we'll run this.\\n\\nOkay, so now we have here a little line chart,\\nand that looks decent,\\nbut these are just the standard settings\\nthat you would get with Matplotlib.\\nAnd I want to play with these a little bit,\\nso we'll change the color scheme,\\nwe'll create a new variable called color theme.\\nSo let's do that now, color underscore theme,\\nlet's add an \\\"equal to,\\\"\\nand then we'll create a list of colors\\nwe want to use in the chart.\\nSo we'll say dark gray,\\nlight salmon,\\nneed to change this to a single quote,\\nand then the last color will be powder blue.\\n\\nOkay, and then we'll go ahead and call\\nthe plot method off of the data frame,\\nand we'll pass in the color theme.\\nTo do that, we say df.plot,\\nand then we set the color parameter equal to\\nour color underscore theme, and run this.\\nAnd now you can see we have shifted the colors\\ninto these sort of more fancy-looking colors.\\n\\nI also want to show you how to change colors\\nbased on our GP codes.\\nSo we will create a Z variable\\nand we'll set Z equal to a list,\\nand it'll have the numbers one, two, three, four,\\nand zero point five.\\nAnd now we want to call the pie function\\nand we'll pass in our Z object.\\nSo that's plt.pie.\\n\\nAnd then, we'll go ahead and say plt.show,\\nprint it out,\\nand there we have a pie chart,\\nbut it's coming with our default colors for Matplotlib.\\nSo let's go ahead and adjust these colors using our GP codes\\ninstead of the color labels we have been using.\\nSo we'll create a new color theme.\\nAnd we can just overwrite the previous one,\\nso we'll say \\\"color theme equal to,\\\"\\nand then we'll create a list.\\n\\nAnd this time, instead of passing in the color labels,\\nwe're going to pass in our RGB codes.\\nSo our first RGB code will be #A9A9A9.\\nAnd then I'm going to paste in the next four.\\nThese are just RGB codes to represent\\nwhat the color should look like\\nwhen it gets plotted into the pie chart.\\nNow we've got those.\\nAnd now all we need to do is call the pie function.\\n\\nSo that's plt.pie, and we'll pass in Z.\\nAnd then our color, we'll set our color parameter\\nequal to color theme,\\nand then plot it out for the plt.show.\\nThere is a typo here, so this should be colors.\\nAnd as you can see,\\nwe have now adjusted the colors out a little bit\\nso that there's just a bit more creativity\\nthan the standard Matplotlib printout.\\n\\nNow let's look at how to customize line styles.\\nSo the first thing we would do\\nis just to create two new variables.\\nWe'll call them X1 and Y1.\\nAnd let's say X1 is basically the same variable\\nthat we created here, series of numbers between\\none and nine.\\nSo I'm just going to copy this.\\nAnd then our Y1 variable will be a series of numbers\\nbetween one and 10 in descending order,\\nso I'll just create a list and write them in.\\n\\nOkay, so now let's just plot this thing out.\\nThe plt.plot.\\nWe'll pass in our X and Y.\\nLet's also create a second plot.\\nAnd we'll just plot the X1 and Y1 variables\\nin that second plot.\\nSo just copy this,\\nand add the numbers for the variable names\\nthat we just created, and run this.\\n\\nOkay, so it looks like we have a little bit of a typo\\nbecause X and Y must have the same first dimension,\\nso I messed up in my creating\\none of my variables here, and that would be this variable\\nbecause we need to have nine numbers in X1\\nand nine numbers in Y1,\\nso I wrote in one too many.\\nSo let's run this, and yeah, okay, great.\\nSo here we have now two lines in the plot,\\nand that's exactly what we wanted,\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4586144\",\"duration\":1129,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating labels and annotations\",\"fileName\":\"3006708_en_US_04_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1973,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to create labels and annotations. This video covers labeling pot features, legends, and annotating plot features.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":38717589,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Labels and annotation\\nadd a deeper layer of context to a plot,\\nenabling the plot to convey extra meaning to its viewers.\\nIn this section, I'm going to show you\\nhow to label plot features,\\nadd a legend to your plot,\\nand annotate features on your plot.\\nBut before that,\\nlet me give you an example of where this comes in handy.\\nData journalists often add a lot of context\\nand annotation to their visualizations\\nin order to add and augment the story they're telling.\\nFor example, imagine you're a data journalist\\ncovering a story of tourism in central Florida.\\n\\nYou'd use a simple line chart\\nto show the number of travelers over time.\\nBut if you're telling a story about the success\\nof a grand opening of a new theme park,\\nyou may want to add some text\\nabout how many visitors came to the grand opening.\\nYou then may also want to add a pointer\\nthat ties that text into the date\\nof the park's grand opening.\\nThere are two methods for labeling and annotating.\\nAgain, the functional and object-oriented method.\\n\\nI'm going to show you both of those\\nin the demonstration to come.\\nBut before that, let me give you some extra information\\nabout the methods we'll use.\\nIn order to add a legend,\\nyou'll need to call the legend method\\nand you'll pass in a label and a location.\\nThis will then place a legend on your plot axes.\\nOver here on the right of the page,\\nyou can see all of these different options\\nin terms of where you want to position your legend\\non your plot.\\n\\nAnd then with respect to annotating your plot,\\nyou would call the annotate method,\\nand then you would just pass in a parameter\\nfor the location that's being annotated,\\nas well as the location of the text\\nand any information that you want to add\\nabout how the arrow should be drawn.\\nSo let's get to work on doing this in Python.\\nThe Jupyter Notebook here is already coming preloaded\\nwith the libraries you'll need.\\nSo those are numpy, pandas, matplotlib, and seaborn.\\n\\nI've also added in the parameters\\nfor plotting by matplotlib, which you can see right here.\\nSo everything is all typed out for you.\\nLet's just start by labeling plot features.\\nAnd I'm going to start by showing you\\nhow to do that using the functional method.\\nSo the first thing we need to do\\nis to create some variables.\\nLet's create an x variable using the range function,\\nand we'll just create a series of numbers\\nbetween 1 and 9.\\nAnd then let's create a Y variable.\\n\\nWe'll set that equal to a list,\\nwhich will contain the values, 1,2,3,4,0.5,4,3,2,1.\\nAnd then say we want to generate a bar chart.\\nSo to do that, we'll use the bar function.\\nThat's plt.bar, and we'll pass in our variables x,y.\\nAnd then let's go ahead and create a label.\\nTo do that, we will call the xlabel function.\\n\\nSo that's plt.xlabel.\\nAnd then we'll pass in a string\\nwith the label that we want to be showed on the x-axis.\\nSo we'll say your x-axis label\\njust to make this super clear.\\nAnd then let's also create a label for the y-axis.\\nSo to do that, it's actually very simple\\nbecause the function for that is just ylabel.\\n\\nSo I'm going to copy this code\\nand then just change out the function name here for x for y,\\nand then update the string\\nand then run this.\\nOkay, cool.\\nSo now we have a bar chart\\nand it looks like we actually have two labels,\\none on the x-axis and one on the y-axis.\\nAnd we did that using the functional method.\\nSo now I want to show you how to do the same thing\\nwith something like a pie chart.\\nSo let's create a new variable here and we'll call it z.\\n\\nAnd we'll say z is equal to a list\\nthat contains the values, 1,2,3,4,0.5.\\nAnd then let's also create some labels\\nand we'll label it according to vehicle type.\\nSo we'll call this veh_type,\\nand we'll set it equal to a list\\nthat contains a series of strings\\nwhere the first string is 'bicycle',\\nthe second one is 'motorbike',\\nthird one is 'car',\\nthe fourth one is 'van',\\nand then the fifth one, let it be 'stroller'.\\n\\nNow we want to generate a pie chart.\\nSo we'll call the pie function,\\nit's plt.pie, and we'll pass in our variable Z.\\nAnd then we needed to find the labels parameter.\\nWe'll just say labels=veh_type.\\nAnd then plot it out using the show function, plt.show.\\nYou can see what this has done is it has gone along\\nand nicely created some labels\\non the outside of the pie chart.\\n\\nThis is still the functional method of creating labels.\\nNow I want to show you the object-oriented method.\\nSo for this part of the demonstration,\\nI want to use the mtcar dataset\\nthat we've been using in earlier demonstrations.\\nSo I'm going to set the address for that first.\\nI'm going to go up to the data folder\\nand right click to get the path\\nand then paste that into the string here, right?\\nSo we have the address for our CSV file.\\n\\nAnd then let's create a data frame called cars.\\nAnd then we'll read in the file using the read_csv function.\\nSo that's pd.read_csv,\\npass in address,\\nand then let's assign names to the columns.\\nTo do that, let's first select the columns\\nby saying cars.columns,\\nand then we'll assign values to these column names\\nby just setting this equal to a list of strings\\nwith names for each of the columns.\\n\\nSo the first column is 'car_names',\\nand then I'm just going to copy and paste\\nover the rest of the column names,\\nwhich are 'mpg','cyl','disp',\\n'hp','drat','wt','qsec',\\n'vs','am','gear','carb'.\\nAnd now let's isolate the mpg variable.\\nSo to do that, we're going to say mpg = cars.mpg.\\n\\nAnd since this is object oriented,\\nof course, we'll start first by creating a figure.\\nSo we're going to do that by calling the figure function,\\nwhich is plt.figure,\\nand we'll set this equal to fig.\\nThen we're going to add some axis to the figure.\\nTo do this, we need to call the add_axes method\\noff of the fig object.\\nSo we'll say fig.add_axes,\\nand then let's define what we want for our axes.\\n\\nSo create a list here.\\nI'll say .1,.1,1,1,\\nand we'll set this whole thing equal to ax.\\nNow let's call the plot method.\\nSo to do that we say mpg.plot.\\nBut before actually running the cell,\\nlet's go ahead and add some tick marks and labels\\nalong the axis.\\n\\nSo to do that, we will use the set_xticks method.\\nSo we'll say ax.set_xticks,\\nand then we'll pass in range function\\nand we'll pass in the value 32.\\nOkay, and then let's also set some xticks labels.\\nSo we'll say ax.set_ticklabels.\\n\\nAnd within that function,\\nwe're going to pass the car names,\\nthat's cars.car_names.\\nAnd let's add a little rotation to the label\\nso that they're easier to read.\\nTo do that, we'll pass a perimeter\\nthat says rotation=60.\\nLooks like I need to move this bracket.\\nAnd then we'll also set a fontsize='medium'.\\nSo it's fontsize=, and then create a string\\nthat reads 'medium'.\\n\\nOkay, so we need to make sure\\nthat this says set_xticklabels.\\nSo I need to add an x here.\\nAnd then let's also add a title.\\nTo do that, we'll call the set_title method\\noff of the ax object, set_title,\\nand then we'll pass in a string for the title of the graph.\\nAnd let's call it 'Miles Per Gallon of Cars in mtcars'.\\n\\nLastly, I would like to create some labels for our chart.\\nSo we'll call the set_xlabel method\\nand the set_ylabel method.\\nSo ax.set_xlabel,\\nand then pass in a string that reads 'car names'.\\nAnd then for the ylabel,\\nwe can actually just copy this.\\n\\nThe method is ylabel.\\nSo I changed this here,\\nand then I update this string to read 'miles/gallon'.\\nOkay, I am just looking it over real quick to make sure\\nthat I don't see any syntax errors.\\nAnd then let's run this.\\nAnd here we have our printout.\\nSo look how nice this is.\\nWe've got a little rotation\\nin terms of the labels on the x-axis,\\nand both the x-axis and the y-axis are labeled.\\n\\nThe only thing that's not so nice about this graph\\nis of course the text,\\nwhich is kind of piling on top of itself.\\nAnd that's actually because we set the parameters\\nwith the rcParams at the top.\\nAnd so it's eight inches wide.\\nIf we wanted to fix that,\\nwe could just change that to 12 here and then go back down\\nand run this code block and you'll see that.\\nOkay, now it's much more nicely spaced.\\n\\nAnd we also have a title to the chart.\\nI also wanted to show you how to add a legend to your plot.\\nSo we'll first do that with a functional method\\nand let's call the pie function\\nand we'll pass it in our z variable.\\nSo that's plt.pie, and then pass in our z variable.\\nAnd then let's also create a legend.\\nTo do that, you need the legend function.\\nSo we'll say plt.legend,\\nand then we need to pass in our vehicle type\\nbecause we want to use those as our labels in our legend.\\n\\nSo that's the veh_type.\\nAnd then we want this legend location\\nto be set at the best possible location, right?\\nSo to make that happen,\\nwe will set the loc parameter equal to 'best',\\nand then you just plot it out.\\nTo do that, you call the show function.\\n\\nAnd then, cool.\\nNow we actually have created a nice little legend\\nfor our pie chart,\\nand that's awesome.\\nLet's also look at how we can create a legend\\nwith the object-oriented method.\\nSo what I'm actually going to do\\nis I'm going to go back up here to the top\\nwhere we've already written all of this code\\nand I just want to reuse it and adjust it.\\n\\nSo I'm going to take this chunk here\\nand I'm going to paste it down here.\\nAnd we're still using the mpg variable.\\nThe xticks are the same.\\nAnd the only thing we're actually going to change here\\nis that we're going to add a legend.\\nTo add a legend,\\nwe'll call the legend method off of the ax object.\\nSo we'll say ax.legend.\\n\\nAnd then we also need to set the perimeter for the location.\\nAgain, let's just set that equal to best.\\nSo we'll say loc='best'.\\nRun this, and nice.\\nYou can see up here in the upper right corner,\\nwe now have a legend.\\nAnd the final thing we're going to cover in this demonstration\\nis how to annotate your plot.\\nSo first things first,\\nlet's just look and find out\\nwhat is the max value for our mpg variable.\\n\\nTo do that, we're going to call the max method\\noff of the mpg variable here,\\nso mpg.max.\\nAnd we can see that the max value\\nin the mpg variable is 33.9.\\nSo let's go ahead and label that point on our chart.\\nI'm going to go back up\\nand get the code for this chart\\nagain so we can reuse it.\\n\\nAnd what I want to do\\nis set a limit to this chart for the y-axis.\\nSo to do that, I'll use the set lim method.\\nSo we'll say ax.set_ylim,\\nand then we'll pass in\\na list that contains two values.\\nSo our minimum y limit will be 0,\\nand our maximum y limit will be 45.\\n\\nAnd let's also create some annotation.\\nTo do that, we'll call the annotate method\\noff of the ax object, so ax.annotate.\\nAnd the first thing we need to do is pass in a string,\\nwhich is going to be the text\\nthat should be added as annotation to the graph.\\nWe'll move this up\\nand then create the string that says 'Toyota Corolla'.\\n\\nAnd then we need to give some details\\nabout where this annotation should be placed.\\nSo we're going to say xy=,\\nand then we're going to create a two pull\\nthat describes the position.\\nSo we'll put 19 and 33.9.\\nOkay, and then we also need to go ahead\\nand pass a parameter for the xy text.\\nBasically, where the text needs to be placed.\\nSo we'll say xytext=,\\nand then it's created two pull.\\n\\nAnd we'll say we want the text\\nto be positioned at 21 on the x-axis\\nand 35 on the y-axis.\\nAnd let's also set an arrow, okay?\\nSo to do that, we're going to pass a perimeter\\nthat reads arrowprops,\\nand then we're going to set it equal to a dictionary.\\nAnd then we're going to set a color for our arrow,\\nand that color is going to be black.\\nSo we need to call the dictionary constructor.\\nAnd then we need to say that the facecolor parameter\\nis equal to a string called 'black'.\\n\\nAnd then let's just shrink it up a little bit.\\nSo we'll pass a perimeter that says shrink=0.05.\\nSo I'm going to print this out.\\nAnd look at that,\\nwe have a nice annotation with an arrow pointing\\nto the maximum value for mpg.\\nAnd because we created this nice label,\\nit's really obvious and easy to see\\nthat the Toyota Corolla is the vehicle\\nthat is getting the best mile per gallon\\nout of all of the cars in the mtcar dataset.\\n\\nNow that you know how to add labels and annotation,\\nlet's look at how to visualize time series in Python.\\nThat will be our next lecture.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4579301\",\"duration\":503,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Visualizing time series\",\"fileName\":\"3006708_en_US_04_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":810,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to visualize time series. This video covers handling time series and plotting time series.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":17052908,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Time series plots convey\\nhow an attribute changes over time.\\nUsing statistical methods like\\nautoaggressive integrated moving average,\\nyou can reliably predict or forecast\\nthe demand of a particular retail product\\nbased on historical time series\\non previous sales of that product.\\nBefore forecasting your time series,\\nyou need to know how to handle and plot time series\\nin Python.\\nWorking with time series in Python can get really tricky,\\nbut Pandas makes it simple.\\n\\nBefore showing you how to use time series in Pandas,\\nlet me just show you what a time series looks like.\\nThese four plots all show time series,\\nthe first one is a constant time series.\\nBasically, you're not seeing any trends or changes\\nin the variable over time.\\nTrended time series is like this chart over here\\nin the upper right,\\nthat's where you see a net increase or decrease\\nin the time series variable over time.\\nIn the lower left you'll see an untrended time series.\\n\\nThis is an untrended seasonal time series,\\nso the variable is increasing and decreasing\\naccording to the seasons of the year,\\nbut you're not seeing a net change\\nin the average value of the time series.\\nSo we call that untrended.\\nIn contrast, over in the lower right corner,\\nyou'll see a trended seasonal time series.\\nThis is where the variable increases and decreases\\nwith the season,\\nbut there's a net gain in the variable over time.\\nSo like I said,\\nand your Jupyter Notebook is coming\\nall loaded with the libraries you need,\\nwhich are NumPy, Pandas, and Matplotlib.\\n\\nWe're also going to be using Seaborn here\\nto just set the style.\\nSo I actually prefer to use the alias, sns here,\\nlet me update this.\\nAnd we can just run this\\nand we'll be ready to start creating time series.\\nSo we'll get right to work with just reading in our data\\nand then printing it out as a time series.\\nThe first thing we need to do is set an address.\\nSo let's say address =,\\nand then for this demonstration,\\nI want to use the Superstore Sales data.\\n\\nSo the location of that is over here in the data folder.\\nJust Right Click, copy path,\\nand then paste it into this string here.\\nAnd then I'll close this.\\nThe next thing that we'll do\\nis we'll create a data frame from the CSV file.\\nAnd we need to read the CSV file in\\nusing the read CSV function.\\nSo let's just call this data frame df\\nand we'll set it =pd.read_csv,\\nand then we'll pass in the address.\\n\\nAnd then we also want to set our index.\\nThe index for the data frame\\nshould be set to the order date.\\nSo to do that we'll just say index_col,\\nwhich is the parameter which is used to set the index.\\nAnd let's set that = 'Order Date.'\\nAnd we have to be sure also to encode the data properly.\\nFor this particular demonstration,\\nwe need to set our encoding here = cp1252.\\n\\nAll we need to do is define the encoding parameter\\nequal to a string, which reads cp1252.\\nAnd let's also parse the dates.\\nSo we'll want to pass a parameter that says\\nparse_dates=True.\\nAnd then let's just look at the first five records\\nby calling the head method off of the DF data frame.\\n\\nSo we'll say df.head() and run this.\\nAnd here we've got now a little preview of the data\\nthat is sitting inside of this Superstore Sales data set.\\nAnd as you can see,\\nPerfect, so now let's go ahead\\nand create a time series plot from this.\\n\\nTo do that, we're going to call the plot method\\nand we'll just select the order quantity variable\\nfrom the data frame.\\nSo we'll say df and then select order quantity here,\\nand then call the plot method off of this and run it.\\nAnd it printed out very quickly,\\nbut it's way too much data.\\n\\nWe can't make any sense of this, right?\\nSo what I'm going to do is I'm just going to create\\na small sample of this data\\nso that we can plot something out\\nthat we're able to actually decipher.\\nSo let's create a second data frame\\nand we'll call it df2.\\nAnd then we'll call the sample method\\nin order to take a random sample of data points\\nfrom the dataset.\\nSo we'll say df.sample(),\\nand then let's take 100 data points.\\n\\nTo do that, we need to define the n perimeter = 100.\\nNow this function is going to be doing random sampling\\nand I want you to get the same results on your machine\\nthat I get in here in this demonstration.\\nSo what we need to do is actually set the seed for that.\\nSo to do that,\\nwe'll say random_state and just set it equal to 25.\\nAnd that will ensure that the points that are pulled\\nin this demonstration are the same ones that are pulled\\non your machine when you're doing this\\nat home or in the office.\\n\\nAnd then we also have to pass in a perimeter for the axis\\nand we'll say axis=0.\\nAnd let's also create some labels real quick.\\nSo plt.xlabel on the x-axis.\\nLet's label that order date.\\nAnd then let's create a y-label.\\n\\nChange out the string here, call that order quantity.\\nAnd let's also call the title function,\\nwhich is pt.title.\\nAnd we'll just pass in a string for the title of the chart.\\nSo that's going to be Superstore Sales.\\nAnd then let's just plot out this smaller data frame.\\nSo we're going to call the plot method.\\n\\nLet's select only the order quantity field.\\nSo we'll say df2.\\nAnd then from that data frame,\\nwe want to select only the order quantity.\\nAnd then off of this entire thing, we call the plot method.\\nAnd I'm going to run this.\\nLooks like there was a syntax error.\\nI need to add a letter T here, run this again.\\n\\nOkay, cool.\\nSo this is a lot easier to read than the graph prior.\\nAnd what we're seeing along the x-axis\\nis really the dates that the orders were made\\nand the quantity of orders that happened on those dates.\\nSo this was a really quick and easy way\\nto use Python to generate a time series.\\nIn the next section,\\nI'm going to show you how to create statistical data graphs.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2714161\",\"duration\":891,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating statistical data graphics in Seaborn\",\"fileName\":\"3006708_en_US_04_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1398,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to create statistical data graphics. This video covers histograms, box plots, and scatter plots.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":29803818,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Statistical plots allow viewers\\nto identify outliers, visualize distributions,\\ndeduce variable types, and discover relationships\\nand core relations between variables in a dataset.\\nIn this course, I'm going to show you\\nhow to use statistical plots to visually detect outliers,\\ndeduce variable distribution and type, and uncover\\nrelationships and core relations between variables.\\nNow, histograms are very simple plots\\nthat are used to show variable distribution.\\nScatter plots, on the other hand,\\nare used to show relationships between variables.\\n\\nScatter plot matrices show core relations between variables\\nand box plots show variable spread\\nand are useful for outlier detection.\\nLet me show you how to create these in Python.\\nSo your Jupyter Notebook is coming loaded with the libraries\\nthat you will need, or at least most of them.\\nSo we have our standard libraries, which are num, pi,\\npandas, matplotlib and seaborn.\\n\\nI've also gone ahead here\\nand set the plotting parameters for matplotlib.\\nIn this demonstration, we're actually going to be working\\nwith seaborn, and I showed you\\nhow to import that in earlier demonstrations.\\nA nice thing about seaborn is that it provides you options\\nfor style sets and there are a lot of different options.\\nSo what I did is I set a seaborn style equal to whitegrid.\\nThere are many different options,\\nbut I like the whitegrid option, so we'll use that.\\n\\nAnd yeah, I'm just going to run all of this\\nto have it imported into our IPython environment.\\nAnd then let's get started\\nwith creating statistical data graphics.\\nThe first thing we're going to do in this demonstration is just\\nto start eyeballing dataset distributions with histograms.\\nAnd this Jupyter Notebook is coming loaded with the data\\nthat you should need, although you'll want to change\\nthis file extension for the file extension at your setup.\\n\\nSo you would just go over here to the Explorer\\nand then go to the data folder and right-click.\\nIn this demonstration,\\nwe're going to be using the empty cars dataset.\\nSo then you will just right-click here,\\nget the empty car dataset,\\nand then change out the string for the address.\\nAnd for this demonstration, let's just set the index\\nfor the data frame equal to the car names column\\nin the cars data frame.\\n\\nAnd I've already preloaded this into your notebook\\nso you don't have to type all of this out again,\\nbecause this is very similar to prior demonstrations.\\nSo what I want to do here first\\nis just isolate the mpg variable.\\nSo we'll say mpg variable is equal to cars.\\nAnd then we'll use the indexer\\nto select the mpg column here.\\nAnd to plot it out, we'll call the plot method\\noff of the mpg variable,\\nso mpg.plot, and since we are creating a histogram,\\nwe need to set that perimeter inside of the plot method.\\n\\nTo do that, we're going to say kind=,\\nand then create a string that reads hist\\nand that will instruct Python to create a histogram.\\nAnd then we run this,\\nand as you can see, we have a nice little histogram\\nthat shows the distribution of data in our mpg variable.\\nAnother way to create a histogram\\nwould be to just call the hist function\\nand then pass in the mpg variable.\\n\\nLet me show you how to do that,\\nplt.hist and then pass in mpg\\nand then call the plot function,\\nplt.plot, and run this.\\nAnd now we have just a different way\\nof creating a histogram inside of Python.\\nNow let me show you how to create a histogram using seaborn.\\nTo do that, you would use the disc plot function,\\nso that's going to be snsdisplot.\\n\\nAnd then we need to pass in the mpg variable.\\nAnd this is one of the two ways\\nI want to show you how to do it.\\nThis is the simplest way\\nof creating a histogram using seaborn.\\nSo I'm going to run this.\\nAnd there you can see we have a nice,\\nkind of more beautiful, more styled plot\\nthat was actually created more simply\\nusing the seaborn method.\\nNow let me show you another way to use seaborn\\nto create scatter plots.\\n\\nBasically, I messed up with our script.\\nSo I said something about\\nhow I'm going to show two different ways to do this,\\nbut that wasn't proper.\\nThat wasn't correct.\\nSo let's just actually start over again from the part\\nwhere we're going to create this from scratch\\njust 'cause it's super simple.\\nSo now let me show you how to use seaborn\\nto create a histogram.\\nTo do that, you would use the displot rephrase.\\n\\nTo do that, you would use the displot function,\\nso that would be sns.dissplot,\\nand then pass in the mpg variable and run this.\\nAnd as you can see, you know,\\nwe've got a nice styled histogram here\\nthat was extremely simple to create,\\nand that's the basics of how\\nto create a histogram using seaborn.\\nI'm going to show you two different ways of doing this,\\nand the first one is just via the plot method.\\n\\nSo we'll call plot off of our cars data frame,\\nand then we'll pass in a parameter\\nto say what kind of plot we want.\\nSo we'll say kind is equal to scatter.\\nAnd then we want to set our variables.\\nSo X should be equal to hp\\nand then let's set our Y variable equal to mpg.\\n\\nAnd let's also go ahead and set a color for this plot.\\nTo do that, we can just say C is equal to,\\nand then we'll select the color dark gray\\nby writing a string that reads dark gray.\\nAnd lastly, let's set a size\\nfor each of our dots in our scatterplot.\\nSo to do that, we would say S is equal to 150.\\n\\nAnd I'm just checking this over\\nfor any obvious syntax errors.\\nOkay, and then run this.\\nAnd there we go, we have a nice scatter plot.\\nBut I also want to show you\\nhow to create a scatter plot using seaborn.\\nSo with seaborne, you just use the reg plot function,\\nwhich it's going to be sns.regplot,\\nand you pass in the variables that you want plotted out\\nfor the X and Y axis.\\n\\nSo for X, we will say x should be equal to the hp variable,\\nand then y should be equal to the mpg variable.\\nAnd then we also need to define\\nwhere we're actually pulling this data from.\\nSo for that, we would need to create a perimeter\\nthat says data and then set the equal to cars,\\nour cars data frame.\\nAnd then to make sure that this comes out as a scatterplot,\\nwe need to say scatter equal to true.\\n\\nAnd this just tells seaborne yes, create a scatterplot.\\nAnd then when we run this, we see seaborn's version\\nof the very same scatterplot.\\nAs you can see, it's a lot more detailed\\nand helpful than the generic version\\nwe created with the simple plot method above.\\nMoving on, I want to show you how to use seaborn\\nto generate a scatter plot matrix.\\nAnd this is actually really, really easy.\\nSo you would just call the pair plot function.\\n\\nSo this is going to be sns.pairplot\\nand we pass in our data frame cars and then run this.\\nAnd of course this is awesome,\\nbut it's also a lot of information to take in at a glance.\\nSo what I think we should do is let's just create a subset\\nand then we'll plot that out instead.\\nWe'll call it car subset.\\nSo we'll say cars_subset,\\nand we'll set that equal to cars.\\n\\nAnd then we'll use the indexer\\nto select the variables we want here.\\nLet's make those mpg,\\ndisp, hp and wt.\\nAnd then again, we will use our pairplot function.\\nSo sns.pairplot,\\nand we'll pass in this time our car subset.\\n\\nAnd then to generate the plot, we just need\\nto say plt.show, run this.\\nAh, okay, so this is a bit easier to read,\\nbut the nice thing about having a scatterplot is\\nat a glance, you can really get an idea about the type\\nof relationship that is occurring between the variables.\\nAnd also you can see things like possible outliers.\\nLike this point right here is possibly an outlier\\nor over here, this could be an outlier.\\n\\nAnd then you can also tell at a glance\\nwhat type of variables you have.\\nFor example, these variables plotted here are\\nall continuous variables, which you can tell by looking at\\nthe distribution of points that are plotted.\\nSo scatterplot matrices are just really nice to have\\nto get a fast understanding of your variables\\nand your variable pairs.\\nNow let's look at how to build a box plot.\\nWe're going to use the box plot method to do this.\\n\\nSo we're going to say cars.boxplot,\\nand then we'll pass in the perimeter to say\\nwhat we want plotted in our box plot.\\nSo our first box plot, let's just plot out a column mpg.\\nSo we'll say column equal to mpg.\\nAnd then we want to plot the mpg variable\\nagainst the automatic manual transmission variable.\\nSo to do that, we'll say by is equal to am\\nfor automatic and manual transmission variables.\\n\\nAnd then let's also just create a second block box plot.\\nAnd this time we'll plot weight against transmission type.\\nSo we'll say cars.boxplot,\\nand we'll say our first column should be weight.\\nAnd then we want to plot this\\nagainst or by the automatic manual transmission variable.\\n\\nSo am here and then we run this.\\nI would say it's a little cramped.\\nI'm going to go up and just change this variable\\nto make it a little taller.\\nI'm going to say six inches, six inches tall for our chart\\nhere, just so we don't have any labels scrunched up.\\nOkay, so this is better.\\nAnd this is showing us how our data is distributed\\nacross mpg and automatic and manual transmission.\\n\\nSo what it's saying here is that cars\\nthat get less miles per gallon\\ntend to not have an automatic transmission.\\nIf they do have an automatic transmission,\\nwhich is am equal to one, then they tend\\nto get more miles per gallon.\\n\\n\"}],\"name\":\"4. Practical Data Visualization\",\"size\":196738610,\"urn\":\"urn:li:learningContentChapter:4583164\"},{\"duration\":4779,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4583161\",\"duration\":494,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Simple arithmetic\",\"fileName\":\"3006708_en_US_05_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":749,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to perform simple arithmetic. This video covers standard arithmetic, arithmetic multiplication, and matrix multiplication.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15743431,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this section,\\nwe're going to be talking about how\\nto do simple arithmetic in Python.\\nThe benefit of the NumPy library\\nis that it makes it really easy to do math on data\\nthat's stored in either arrays or matrices.\\nNow, an array is just a one-dimensional container\\nfor elements that are all of the same data type.\\nAnd, matrix is just a two-dimensional container\\nfor elements that are stored in an array.\\nThe basic arithmetic operators in Python\\nare addition, subtraction,\\nmultiplication, and division.\\n\\nLet me give you an example of where NumPy can come in handy.\\nHave you ever tried to use a spreadsheet application\\nto perform mathematical operations on a data set\\nthat has more than 300,000 rows?\\nWhat happened?\\nIf the application didn't crash,\\nthen it took a lot of time\\nand effort to get the program to make the computation.\\nBut with NumPy, on the other hand,\\nyou can quickly and easily do mathematical\\nand statistical operations on data sets\\nwith even millions of records.\\n\\nSimply put, NumPy makes it easy\\nto do math on large data sets.\\nWith this slide here, I just wanted to familiarize you\\nwith the operators that you will use in Python\\nto achieve these arithmetic operations.\\nSo we're starting out the coding demonstration\\nwith having imported NumPy\\nand also imported the random number generator from NumPy.\\nJust run this.\\nAnd,\\nit never looks good to see more than two digits\\nafter a decimal point,\\nso let's go ahead and limit the number\\nof decimal places returned in this demonstration.\\n\\nSo to do that, you would just say,\\nnp.set_printoptions,\\nand then let's set a precision of 2.\\nSo precision equal to 2,\\nand run this.\\nNow the first thing we're going to do\\nis look at math with arrays.\\nSo we of course need to create some arrays to do that math.\\nAnd our first array will be array a,\\nand we'll set that equal to an array of six values.\\n\\nSo what we need to do is call the array function,\\nnp.array,\\nand then we'll pass in a list\\nof values from one to six.\\nAnd then print that out.\\nWe'll just say a and run this,\\nand there you go,\\nwe have an array of six values.\\nAnd then let's create a second object,\\nwhich will be a matrix and we'll call it b.\\n\\nAnd to create this matrix,\\nwe'll use the array function,\\nso we'll say np.array,\\nand this time we need to pass in two lists.\\nSo the first one is going to contain the values,\\n10, 20, and 30.\\nAnd then the second list will contain the values,\\n40, 50, and 60.\\n\\nAnd then we'll print this out\\nand very good, we have a matrix.\\nNow, before we're actually going in and doing math,\\nlet's create an array via assignment.\\nI want to show you how to do that.\\nSo let's this time use a random number generator\\nin NumPy, and what we're going to do\\nis we're going to create six random values.\\nSo to set the seed for a random generator,\\nwe need to say np.random.seed.\\n\\nAnd this just makes so\\nthat the results you get on your computer\\nwill be the same as we,\\nas I show you here in the demonstration.\\nAnd then we need to assign a value to the variable c.\\nSo we'll say c is equal to 36 times,\\nthe random numbers that are generated\\nby the random number generator in NumPy.\\nSo we'll say np.random.randn,\\nand let's pass in the value of six.\\n\\nThis is just saying that we want to have six values.\\nAnd we'll print this out.\\nAnd the one thing that I would like to point out here is\\nthat when we use the randn function,\\nwhich is the random number generator in Python,\\nwhat that is actually doing\\nis it's generating both positive\\nand negative random numbers.\\nNow let's create a fourth array,\\nwhich is going to be called d,\\nand we'll set d equal to a series of sequential numbers\\nbetween 1 and 34.\\n\\nSo we'll use the arange function,\\nnp.arange,\\nand then we will make the starting point 34\\nand the ending point 35,\\nwhere 35 is excluded from the series of numbers\\nthat's output from this function.\\nAnd we can just go ahead and print this out.\\nOkay, great.\\nSo we have our array.\\nNow let's just start performing some math.\\n\\nBefore doing so, I just want to point out one thing here\\nbecause I'm calling all of these objects arrays,\\nbut some of them are actually matrices,\\nand what I want you to keep in mind\\nis that a matrix is actually just a two-dimensional array.\\nThat's why you're hearing me say array\\nwhen I'm actually creating matrices\\nand kind of using them interchangeably in certain points.\\nFirst things first,\\nlet's just multiply the array a by the number 10.\\nSo we'll say a times 10, run this.\\n\\nAnd if we look back here at the a variable,\\nif we multiplied each of these numbers by 10,\\nwe would get this output here.\\nMakes sense, and that was very easy to do,\\nvery straightforward.\\nNow let's try an addition operation.\\nSo we'll do c plus a,\\nand run this.\\nSo this output array is a result of adding the c array\\nto the a array.\\n\\nJust to do a little back checking here,\\nlet's look at the value of c and a,\\nto make sure it matches up.\\nWe've got a 9.22 and up here, c,\\nc here is 8.22,\\ngo up a,\\nvalue here is 1,\\nso when you add those together, you get 9.22.\\nSo yeah, that makes sense.\\nAs you can see, it's really simple\\nto do straight-out arithmetic\\nwith arrays and matrices in Python.\\n\\nLet's do c minus a.\\nAnd again, makes sense.\\nThis time we're getting 7.22,\\nwhich is 8.22 minus 1.\\nSo yep.\\nNow let's try c times a,\\nwhich of course should be 8.22\\nas the first value in the output array.\\nSo we'll say c times a,\\nand run this.\\nAnd yeah, it's 8.22.\\n\\nMakes sense.\\nAnd if you wanted to do division here,\\nall you'd have to do is say c divided by a.\\nAnd of course, 8.22 divided by 1 is going to be 8.22.\\nSo we're good to go there.\\nAnd now you know how to do simple arithmetic using arrays\\nand matrices in Python.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4589019\",\"duration\":579,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Generating summary statistics\",\"fileName\":\"3006708_en_US_05_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":868,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to create standard data graphics. This video covers line charts, pie charts, and bar charts.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":18762967,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Descriptive statistics describe a variable's\\nvalues and their spread.\\nFor example, imagine you work for a company\\nthat monitors patients' health in real time\\nand you need to build a script\\nthat detects dangerous anomalies.\\nYou could generate summary statistics\\nin micro batch and then calculate the mean, max,\\nand standard deviation of incoming health data\\nthat's generated from the monitoring device.\\nWith these descriptive statistics\\nyou could generate automatic alerts\\nwhen unusually high or low data points are generated\\nby the patient's monitoring dived\\nindicating potentially dangerous health status\\nof the monitored patient.\\n\\nDescriptive statistics provide a quantitative summary\\nof a variable and the data points that comprise it.\\nYou can use them to get an understanding of a variable\\nand the attributes that it represents.\\nThere are two categories of descriptive statistics.\\nOne is descriptive statistics that describe the values\\nof an observation in a variable.\\nAnd the second is the descriptive statistics\\nthat describe a variable's spread.\\nSo to get more specific,\\nif you wanted to generate descriptive statistics\\nthat describe the observations in a variable\\nthen what you would do is you'd go ahead\\nand calculate the sum, median, mean, and max\\nof those observations in the variable.\\n\\nBut again, the second way\\nto generate descriptive statistics is\\nto describe the variable's spread.\\nAnd if you wanted to do that\\nwhat you would to is calculate out standard deviation,\\nvariance, counts or quartiles.\\nIf this doesn't make a lot of sense to you now\\njust hold on, because I'm going to show you an example\\non the coding demonstration and then\\nyou'll really understand what I'm talking about.\\nBut first let's look at some of the uses\\nfor descriptive statistics.\\nYou can use descriptive statistics\\nto easily detect outliers.\\n\\nYou can use it for planning data preparation requirements\\nfor machine learning, and you could use it\\nfor selecting features for use in machine learning.\\nOkay, so now let me show you how\\nto generate summary statistics using pandas and scipy.\\nWe're going to start off, of course,\\nby bringing in our libraries.\\nSo we need to to import numpy, pandas, series and data frame\\nand then also scipy.\\nI want to point out that we are importing stats\\nfrom scipy.\\n\\nAnd I've got these already loaded for you\\nin the Jupiter notebook\\nand your notebook's also coming preloaded\\nwith the MT car's data set\\njust to save you time\\nso you can just go ahead and run these.\\nWe've already covered these in previous lectures.\\nAnd what I'd like to do first is just to print out\\nthe first five records of the cars data frame here.\\nSo we'll say cars.head.\\n\\nRun this.\\nAnd now we have a little preview of the data\\nwe've got inside of the cars data set.\\nNow I want to show you how to use the sum method.\\nSo we'll drop down here\\nand the sum method adds up the total of numbers\\nin a column, or in rows of a data frame.\\nBy default, sum will count up values and provide a total\\nfor each column.\\nBut if you pass an axis equal to one argument\\nthen it will add up the values\\nalong the data row-wise instead.\\n\\nSo let's practice with this one.\\nSo we'll just say cars.sum and run this.\\nSo how this works is, the sum method has gone along\\nand summed up the values in the columns\\nof the cars data frame.\\nBut if you wanted to sum the values along the rows instead\\nthat's easy enough to do.\\nYou would just take the same command\\nand then you would pass in a perimeter\\nthat says axis is equal to one.\\n\\nSo let's try that out here.\\nWe'll say cars.sum\\nand we'll say axis equal to one.\\nAnd of course, we only wanted to sum up numeric values\\nso we'll pass in an argument that says numeric_only\\nequal to true.\\nLooks like I need to change this m to an n.\\n\\nAnd run this.\\nAnd there we have it.\\nSo this is basically the output of the summation,\\nthe values of the dataset, row-wise.\\nThe median method finds and returns a median value\\nwith the middle value from the columns\\nor rows of the data frame.\\nSo let's calculate a median value by saying\\ncars.median and then again we need to pass a perimeter\\nthat says numeric only equal to true\\nand run this.\\n\\nAnd then what Python has done here is it's gone\\ninto each of the variables in the cars data set\\nand it's found the median value for each of those variables.\\nIt's returned those as an output here.\\nAnd to calculate the mean it's very, very simple.\\nYou can just say cars.mean.\\nAnd again pass in the numeric only equal to true.\\n\\nAnd run this.\\nAnd this is the average value for each variable\\nin the cars data frame.\\nIf you wanted to generate some statistics\\nabout the maximum value for each variable\\nwe would just say cars.max\\nand then what this is doing is\\nit's outputting the greatest value in each of the variables.\\nNow if you wanted to be able to identify the row\\nwhere the maximum value came from\\nyou'd just call the id maximum method.\\n\\nTo see the index value of the row\\nthat contains the maximum value.\\nSo let's try that out here.\\nWe'll look at the mpg variable.\\nTo isolate that, we'll say mpg is equal to cars.mpg,\\nselect that variable,\\nand then we'll call the id xmax method off of this\\nso that's mpg.idxmax\\nand then what we're seeing here is that\\n19 is the index number of the row\\nwhere the maximum value was found in the mpg variable.\\n\\nNow let's look at som summary statistics\\nthat describe variable distribution.\\nThe most fundamental summary statistic\\nthat describes distribution would be standard deviation.\\nAnd in order to generate that in Python\\nyou can just use the standard method.\\nSo for our example we would take cars.std and then\\npass in perimeter numeric only equal to true\\nand run this.\\n\\nAnd what this has done is it's gone along\\nfor each of the variables\\nand calculated the standard deviation\\nof the values in that variable.\\nTo calculate the variance you would say cars.var,\\nuse the var method here.\\nAnd set numeric only equal to true.\\nAnd now you're getting the variance\\nfor each of the variables.\\nThere's also the value counts method\\nand this method counts up the unique values\\nin and array or a series object.\\n\\nIt shows you how many unique values are present\\nin a data set.\\nSo let's look at the gears variable.\\nWe'll isolate that.\\nWe'll create and object called gear\\nso we'll say gear is equal to cars.gear\\nand then off of that object\\nwe will call the value underscore counts method\\nand run this.\\nAnd what you're seeing here is that\\nthe gear variable has three unique values.\\n\\nThose are three, four, and five.\\nOn the right side you can see the unique counts\\nfor each of these variables.\\nIf you wanted to take a broader perspective,\\nwhat this is really saying is\\nthat the cars data set has 15 cars with three gears,\\n12 cars with four gears\\nand five cars with five gears.\\nAnd I want to show you really quickly the easy peasy way\\nto get an entire statistical description of a data set.\\nThis is the describe method.\\nSo all you would need to do is say cars.describe\\nand with this you're basically getting\\nall of the descriptive statistics that can be generated\\nfrom each of the variables in the entire data set\\nall at one time.\\n\\nSo it's super efficient and helpful to have this on hand.\\nNow that you know how to summarize numerical variables\\nlet's move on to summarizing categorical ones.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2715032\",\"duration\":619,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Summarizing categorical data\",\"fileName\":\"3006708_en_US_05_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"2 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1018,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to define elements of a plot. This video covers object-oriented plotting, sub-plots, and axis labels.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":20130845,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's talk about how to summarize\\ncategorical data.\\nCategorical data is described by how observations\\nare distributed across a variable's categories.\\nA very simplistic approach to sentiment analysis\\ncould involve web scraping public product reviews,\\nthen classifying certain words found in the scraped data\\nas positive or others as negative.\\nYou could then do a categorical word count\\non the product review data\\nto score product reviews or feedback as either good or bad.\\n\\nCategorical variables only assume a fixed number of values.\\nSo, as you can see here, we have a very simple dataset\\nthat contains apples and oranges.\\nSo this variable here, the fruits variable,\\nis actually a categorical variable.\\nAnd then what you can do\\nis you can group your categorical variable into subgroups\\nbased on the fruit category.\\nSo in this case we could break down the dataset\\ninto apples and oranges\\nbased on this categorical fruits variable.\\n\\nThere are three main ways to describe categorical variables.\\nThose are counts, variable description, and grouping.\\nSo I'm going to show you\\nhow to create each of these three types of descriptions\\nover in the Jupyter Notebook.\\nRight, so we're bringing in numpy and pandas.\\nI've already got those imported for you\\ninto the IPython environment.\\nAnd I've also got the empty cars dataset\\nready for you to go here.\\nSo let's just take a look at the first 15 records.\\n\\nTo do that, we will use the head method.\\nSo we'll say cars.head() and then pass in the number 15.\\nNow, you get a basic idea\\nof the data that's inside this dataset.\\nAnd, of course, we've also now set an index.\\nThat index here is equal to the car_names variable.\\nSo you can see that here.\\nWe set it here.\\n\\nOkay, so the first thing that I want to show you\\nis the value_counts method.\\nAnd this method makes a count of all the unique values\\nin an array or a series object.\\nSo, first, let's just isolate our car variable.\\nThis is for carburetors.\\nIt's a number of count of carburetors that each car has.\\ncarb is equal to cars.carb.\\nAnd then we'll generate the value counts\\nby calling the value_counts method.\\n\\nWe'll say carb.value_counts() and run this.\\nAnd what you're seeing here is that there are 10 cars\\nthat have four carburetors.\\nThere are 10 cars that have two carburetors.\\nSeven cars have one carburetor.\\nThree cars have three carburetors.\\nAnd so that's basically how you need\\nto interpret this result.\\nNow, let's look at the groupby function.\\nAnd to group a dataframe by its values\\nin a particular column,\\nyou just call the groupby method off of a dataframe\\nand then pass in the index value of the column series\\nyou want the dataframe to be group by.\\n\\nSo let's just create a subset of our dataframe,\\nand we'll call that cars_cat.\\nAnd within this subset, we'll include five variables,\\nthe cyl variable,\\nthe vs variable,\\nthe am variable,\\nthe gear variable,\\nand, last one, carb.\\n\\nAdd the single quote here.\\nThen let's just print this out.\\nSo we'll say cars_cat.head().\\nRun this.\\nOkay, so here we have a small subset\\nof our original cars dataframe.\\nSo now let's group by the gear variable\\nand then describe the dataset by that unique grouping.\\nIn order to do this,\\nlet's just create a new variable called gears_group.\\n\\nAnd then we'll set it equal to our cars_cat dataframe.\\nAnd what we'll do for the grouping part of this\\nis that we will call the groupby method\\noff of that dataframe\\nand pass in the label index of the column\\nby which we want the dataframe to be grouped which is gear.\\nThen we want to generate a statistical description\\nof the dataset based on that grouping.\\nSo let's just use the describe method.\\n\\nWe'll say gears_group.describe()\\nand run this.\\nAs you can see, we have three rows by 32 columns.\\nThe reason we have three rows\\nis because there are only three different options\\nor your count for the cars dataset.\\nAnd then we have 32 columns\\nbecause for each of the variables in the small subset,\\nthe cars_cat subset,\\nwe have generated statistical descriptions for each of them.\\n\\nSo, as you can see, we're getting a long output table.\\nThe next thing we need to look at\\nis how to transform variables to categorical data type.\\nNow, to create a series of categorical data type,\\nyou would just call the pd.Series function\\non an array or a series that holds\\nthe data you want the new series object to contain.\\nNow, when you pass in the dtype\\nequal to category argument,\\nthis tells Python to assign\\na new series data type of category.\\n\\nHere, we'll create a new categorical series\\nfrom the gear variable.\\nAnd then we'll assign it to a new column\\nin the cars dataframe called group.\\nSo we'll say cars[],\\nand then we will create a new column here called group.\\nAnd then we're going to say that this is equal\\nto a new series object that we're going to create here\\nwith the series constructor.\\nSo we're going to say pd.Series().\\n\\nAnd we'll pass in cars.gear because we're interested\\nin converting the cars.gear variable to a series.\\nAnd then we want to, of course, assign the dtype\\nequal to category.\\nTake that and then print this out.\\nNow, let's just print out this new variable\\nand look at its data type.\\nSo to do that, we will just copy our variable here,\\ncars['group'],\\nand then we call the dtypes method off of that, .dtypes,\\nand run it.\\n\\nOkay, great, so now you see\\nthat we actually do have a categorical data type,\\nand it's part of our cars dataframe which is cool.\\nWe just created this new variable\\nand added it to our cars dataframe.\\nNow, let's look at the distribution of gear types\\nin this variable.\\nTo do that, what we would need to do\\nis say cars[] and then select our group variable here\\nand then call the value_counts method off of that,\\nvalue_counts().\\n\\nAnd so here we have got cars\\nwith three different counts of gears.\\nSo pretty much makes sense.\\nLast thing I wanted to show you in this demonstration\\nis basically how to create a crosstab\\nor cross-tabulation table.\\nThese are very important.\\nYou need to know how to use these\\nin order to make sense of categorical data in data science.\\nLet's just start by creating our crosstab.\\nTo create a crosstab,\\nyou would just call the pd.crosstab function\\non the variable you want included in the output table.\\n\\nSo let's do that here by saying pd.crosstab().\\nAnd we're going to select our first variable\\nwhich would be the am variable.\\nSo we'll say cars[]\\nand then select the am variable.\\nThen for our second variable,\\nthat's going to be the gears variable.\\nSo we'll say cars[], and we'll select gears.\\n\\nAnd we run this. Looks like there is a typo.\\nYeah, so I need to take this s out,\\nthen run it again.\\nAnd, as you can see, we have a really concise summary\\nof each of these two variables.\\nA crosstab is a cross tabulation of two or more features.\\nBy default, a crosstab shows frequency counts for features.\\nSo the example we just did in Python, we basically selected\\nthis am variable and the gear variable,\\nand Python went ahead and created\\na small cross tabulation of these two variables\\nand described to us basically about how gears are broken up\\nwith respect to automatic and manual transmission.\\n\\nSo the am stands for manual transmission.\\nAnd what this table is telling us\\nis that for cars with a manual transmission,\\nthey mostly have three gears,\\nbut cars that have an automatic transmission\\nare more likely to have five gears.\\nIt's basically showing a distribution of gear counts\\nbased on the type of car transmission.\\nThis is just a very simple example\\nof how you can use crosstabs to generate\\nsummary statistics and descriptions of categorical data.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2714162\",\"duration\":893,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Pearson correlation analysis\",\"fileName\":\"3006708_en_US_05_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"3 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1573,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to conduct parametic correlation analysis via Pearson correlation. This video covers linear correlation and causation.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":32785448,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's talk about\\nparametric correlation analysis.\\nParametric correlation analysis is\\na method you can use\\nto find correlation between\\nlinearly related continuous numeric variables.\\nDon't worry if you don't\\nexactly understand what that means\\nbecause I'm going to show you how\\nto figure this out in just a minute.\\nFirst, I want to explain one\\nimportant point about correlation.\\nCorrelation does not imply causation.\\nLet me explain.\\nImagine you're a doctor studying\\nregional obesity trends.\\n\\nYou have two data sets:\\nOne on store size reported\\nby zip code,\\nand two on national obesity prevalence broken down\\nby zip codes.\\nIn the course of your investigation,\\nyou apply the Pearson correlation method,\\nthat's the method I'm about to show you,\\nand you find that there's\\na very strong positive correlation\\nbetween grocery store size and obesity.\\nThe bigger the grocery stores,\\nthe more obesity there tends to be.\\nOf course, the size of the store doesn't cause obesity,\\nbut they're correlated,\\nand that correlation is quantifiable\\nthrough the Pearson Method.\\n\\nPearson correlation is measured\\nby the correlation coefficient, R.\\nIf you have a Pearson R\\nthat's close to one,\\nthen that's a strong positive relationship.\\nAnd if you had an R value\\nthat is close to negative one,\\nthen you've got a strong negative relationship.\\nIf you have an R value equal to zero,\\nor close to it,\\nthen you're basically seeing\\nthat your variables are not linearly correlated.\\nNow, the Pearson correlation assumes\\nthat your data is normally distributed,\\nthat you have continuous numeric variables,\\nand that your variables are linearly related.\\n\\nA really important note\\nthat I wanted to add here is\\nhow do you use the Pearson correlation?\\nSo it's safe to use Pearson correlation\\nto uncover linear relationships between variables,\\nbut you can not use it\\nto rule out the possibility\\nof non-linear relationships between variables.\\nFor this demonstration,\\nwe're going to be bringing\\nin our standard libraries,\\npandas and numpy,\\nbut also please note that\\nI've imported matplotlib and seaborn,\\nas well as the rcParams.\\n\\nWe're also going to be using scipy\\nin this demonstration.\\nSo all of these are already loaded\\nin our notebook,\\nand I've also preloaded\\nthe empty cars data set\\nthat we've been working with,\\nand set the plotting parameters for matplotlib.\\nWe covered all of these things in previous lectures,\\nbut the one thing I want to point out here is\\nthat we are importing Pearson R\\nfrom the scipy stats package.\\nSo you have to first start off\\njust by running these.\\n\\nAnd like I said,\\nempty cars is ready to go.\\nSo all we need to do is run this\\nto load it into our environment.\\nCool.\\nand let's just go ahead and start\\nby generating a pairplot using\\nthe seaborn library.\\nTo do that, we'll use the pairplot function,\\nso that's sns.pairplot,\\nand we'll pass in cars data frame\\nand run this.\\n\\nMove it up a bit.\\nIt's just thinking for a little while.\\nYou can tell what Python's doing\\njust by seeing this moving blue scroll\\nat the top,\\nand also this icon here saying\\nhow long it's taken for Python\\nto process the request.\\nOkay, wow.\\nSo we have a lot of data here\\nthat's been plotted out for us.\\nAnd as you can see,\\nif you were to count them up,\\nwe actually have 11 numeric variables\\nin the cars data set.\\nThis basically takes up a lot of space.\\n\\nI went ahead and selected\\nsome variables for our analysis,\\nand I'll go ahead and generate\\na scatter plot matrix of those\\nin order to show you\\nwhat about them is desirable\\nfor the Pearson correlation.\\nAnd I'm going to take you over\\ninto another screen\\nto explain really quickly.\\nBut before I do that,\\nlet's just make this second scatter plot.\\nSo we'll call it X,\\nand we'll set X equal to our cars data frame,\\nbut we only want to select four columns,\\nwhich are mpg, hp, qsec, and wt.\\n\\nAdd the single quotes here.\\nAnd then, again, we use the pairplot function,\\nso that's sns.pairplot,\\npass in X, and run this.\\nOkay, so that was a lot faster,\\nand here we have a smaller pairplot.\\nNow let me take you over to\\nthe other screen to explain\\nwhat all this means.\\nSo let's consider the model assumptions\\nfor the Pearson correlation analysis.\\n\\nPearson correlation assumes\\nthat your data is normally distributed,\\nthat variables are linearly related,\\nand that the variables are\\ncontinuous numeric variables.\\nLet's look here at\\nthe normally distributed requirement.\\nA normally distributed requirement is going\\nto give a shape like a bell curve\\nin a histogram.\\nI wouldn't say that all these variables are\\nexactly normally distributed,\\nbut they could possibly be close enough\\nin order to generate\\nsome sort of correlation using\\nthe Pearson correlation method,\\nso I'm going to go with these.\\n\\nNow, let's look at the requirement\\nfor a linear relationship.\\nDo these variables have\\na linear relationship between them?\\nIn other words, does one increase\\nwhile the other decreases?\\nBased on the shape of the distribution\\nof points between the variables,\\nit looks like most of these have\\na distribution that could be\\nat least close to linear,\\nso I'm going to test them out\\nwith the Pearson correlation method.\\nThe last requirement is that\\nthe variables be continuous numeric variables.\\nThe best way for me to show you\\nwhy I think that these are\\ncontinuous numeric variables is\\nto show you what a variable looks like\\nwhen it's not a continuous numeric variable.\\n\\nIf you look over at the scatterplot on the right,\\nthese variables over here are not\\ncontinuous numeric variables.\\nThese are categorical variables\\nbecause they can only assume\\na fixed number of positions,\\nlike we just discussed in the last section.\\nSo this variable can assume one of two values.\\nThat makes it a binomial variable.\\nIn the gear variable,\\nit can assume three values;\\nthree, four, or five.\\nThat makes it a multinomial variable.\\nThese are not continuous numerical variables.\\n\\nWhen you see continuous numerical variables,\\nthe scatterplot of the variables is much\\nmore randomly and evenly distributed.\\nThe end conclusion here is that\\nthe variables that are shown\\non the right would not qualify\\nfor the Pearson R correlation analysis.\\nOkay, great.\\nSo let's get back to our coding demonstration\\nand use scipy to calculate Pearson correlation coefficients.\\nNow, let's look at how to use scipy\\nto calculate the Pearson correlation coefficient.\\n\\nOkay, so let's start by creating\\nsome variables we can use here.\\nSo we'll create an mpg variable,\\nand we'll set that equal to cars.mpg.\\nAnd then let's create an hp variable\\nthat's equal to cars hp.\\nWe'll create a qsec variable\\nand we'll set that equal to cars qsec.\\n\\nAnd then, a wt variable,\\nwhich will be directly\\nfrom our cars data frame,\\nthe weight variable here.\\nOkay.\\nSo let's start first by taking\\nthe Pearson R coefficient\\nof the mpg and hp variable pair.\\nSo to do that,\\nwe're going to say pearsonr_coefficient\\nand P value.\\n\\nWe're going to set these equal to\\nthe Pearson R function,\\nand we'll pass in our mpg\\nand our hp.\\nAnd then, let's print out the label.\\nSo we'll say print,\\nand let our label be Pearson R correlation coefficient.\\n\\nOkay.\\nAnd then, say %0.3f.\\nAnd then, give another percentage sign,\\ncreate a tuple here,\\nand pass in our Pearson R coefficient object.\\nOkay, I'm going to look this over\\nreally quick for any typos.\\nOkay, yeah, so one issue is that\\nI needed to close out\\nthe string here after the F,\\nso I'm going to add a single quote\\nand remove the single quote from there,\\nand then we should be good to go.\\n\\nSo let's run this.\\nAnd then, what I'm going to do\\nto calculate the Pearson R\\nfor the other variable pairs is\\njust to copy this little chunk of code\\nand paste it down here,\\nand just change the variables out.\\nOnce we have the Pearson R coefficients,\\nthen we will discuss.\\nSo the second variable pair is going to be mpg and qsec.\\nAnd the third variable pair will be mpg and weight.\\n\\nSo let me run this.\\nAnd this.\\nOkay, great.\\nSo now we have our Pearson R values.\\nLet's just look at what this means.\\nBased on the Pearson correlation coefficient\\nof these three variable pairs,\\nthe mpg weight variable pair appears\\nto have the strongest linear correlation.\\nThe mpg qsec variable pair has\\na moderate degree of linear correlation.\\n\\nAnd you may be wondering,\\n\\\"Well, what do I do\\n\\\"with this information once I have it?\\\"\\nWhen you're doing machine learning,\\nor other forms of advanced statistical analysis,\\nthese models often have assumptions\\nthat either the features are independent\\nof one another\\nor that they exhibit\\na degree of correlation,\\nand you're going to see\\nthat later in this course.\\nSo you can use the Pearson R correlation coefficient\\nto establish whether or not\\nyour variable pairs meet the requirements\\nof more advanced models.\\n\\nNow, that you've seen the long form way\\nof calculating the Pearson R value,\\nlet me show you some shortcuts.\\nWe will start by using pandas\\nto calculate Pearson R correlation coefficient.\\nSo let me just notate that here.\\nYou can also generate\\nsome Pearson R statistics\\nby using this corr method,\\nso let's do it really quick.\\nUsing pandas, you can also generate Pearson R statistics\\nby using the corr method.\\n\\nSo let's do that real quick.\\nWe'll say that corr here is equal to x.corr,\\ncalled the corr method.\\nAnd then we'll print this out.\\nAnd as you can see,\\nit's really quickly generated\\nall of the Pearson R values\\nfor each of the variable pairs\\nin our smaller subset.\\nThe last way you can do this is using seaborn,\\nand that would be with its heat map function.\\n\\nSo we'll just say sns.heat map,\\nand then we'll pass in our corr variable,\\nand then we'll create some tick labels.\\nSo our xticklabels will be equal to\\nthe column values in our corr data frame.\\nSo we're going to say corr.column.values,\\nand then our ytick labels will be equal to\\nthe columns in the corr data frame.\\n\\nSo corr.columns.values,\\nand then we run this.\\nThat looks nice, but what does it mean?\\nWell, the darker shades of red indicate\\na strong degree of positive correlation,\\nas you can see from the legend.\\nBased on what we see,\\nthe hp weight variable pair has\\nthe highest degree\\nof positive linear correlation.\\nJudging by the darker hues in the grid,\\nthe mpg weight variable pair appears\\nto have the strongest degree\\nof negative linear correlation.\\n\\nYou'll of course see here\\nthat when mpg is plotted against itself,\\nthen it has an absolute value of one.\\nIt correlates 100% with itself,\\nthat's why these are solid cream colors here.\\nAnd then the sort of fuchsia color here,\\nthe weight qsec variable pair is not linearly correlated.\\nKeep in mind, that doesn't mean\\nthere's no correlation between\\nthese variables whatsoever.\\nIn the next video, I'm going to show you\\nsome methods you can use\\nto establish correlation\\nbetween non-linearly related variables.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4588026\",\"duration\":888,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Spearman rank correlation and Chi-square\",\"fileName\":\"3006708_en_US_05_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1465,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to conduct non-parametric correlation analysis. This video covers categorical variables, Spearman rank correlation, and Chi-square tables.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":29738610,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's talk about nonparametric\\ncorrelation analysis.\\nYou can use nonparametric correlation analysis\\nto find correlation between categorical nonlinearly,\\nnon-normally distributed variables.\\nFor an example of where nonparametric correlation analysis\\ncould be useful, imagine that you're a social scientist\\nthat studies smoking habits.\\nYou'll use a non-parametric correlation analysis\\nlike Spearman's rank to test the population\\nfor a correlation between income as a bracket\\nand cigarette consumption of smokers.\\n\\nYou find that higher income individuals are much more likely\\nto smoke cigarettes than lower income people.\\nI'm about to show you how to use Spearman's rank correlation\\nand chi-square tables to establish correlation\\nbetween categorical variables.\\nThe Spearman's rank correlation method\\nworks on ordinal variables.\\nIn case you don't know what that is, an ordinal variable\\nis a numeric variable that is able to be categorized.\\nThe Spearman's rank method converts ordinal variables\\ninto variable pairs and then calculates\\nan R correlation coefficient\\nby which to rank their variable pairs\\naccording to the extent of their correlation.\\n\\nIf that doesn't make much sense for you now,\\ndon't worry at all because I'm going to show you\\nwhat this means in the coding demonstration to come.\\nBut first, let's talk about the R values\\nfor Spearman's rank test.\\nSimilar to Pearson correlation,\\nif you get an R value that is close to one,\\nthen you are seeing a strong positive relationship.\\nWhereas if you get an R value that's close to negative one,\\nthen you're seeing a strong negative relationship.\\nIf your R value is close to zero,\\nyou are seeing that there is either a weak relationship\\nor no relationship whatsoever.\\n\\nIn terms of assumptions, the Spearman's correlation\\nassumes that your variables are ordinal.\\nIn other words, they are numeric\\nbut able to be ranked like categorical variable.\\nIt also assumes that your variables\\nare related nonlinearly.\\nLastly, it assumes that your data\\nis non-normally distributed.\\nDon't worry about these too much right now though,\\nbecause in the coding demonstration,\\nI'm going to show you how to examine your variables\\nand find out whether they meet these assumptions.\\n\\nYou can also use the chi-square test to see\\nif non-linear variables are independent of one another.\\nThe null hypothesis of this test\\nis that the variables are independent of one another.\\nSo if you have a p value of less than 0.05,\\nyou would reject the null hypothesis\\nand conclude that the variables are correlated.\\nIf you had a p value greater than 0.05,\\nyou'd accept the null hypothesis and conclude\\nthat the variables are independent of one another.\\n\\nIn terms of the assumptions of the chi-square test,\\nyou just want to make sure\\nyour variables are categoric or numeric.\\nIf you have numeric variables, then you're going to need\\nto make sure that you have binned them.\\nAnd in case you don't know what binning is,\\nnow is a great time to get familiar with that term.\\nAs an example, imagine you had a variable\\nthat had values between zero and 100.\\nThat's a numeric variable.\\nAs an example of binning, you could break up that variable\\ninto 10 separate groups, 10 groups of 10.\\n\\nAnd then within these 10 groups of 10,\\nyou would just put your data into different categories\\naccording to its numeric values, like this.\\nNow that you know what binning is, let's move on\\nto the coding demonstration portion of this section.\\nOkay, so for this demonstration, we're bringing in\\nour standard libraries, pandas and numpy,\\nbut please note that I also imported matplotlib\\nand seaborn as well as rcParams.\\nAnd we're going to be using scipy in this demonstration.\\n\\nSo all of these are already loaded into our notebook.\\nI've also preloaded the mtcars data set\\nthat we've been working with\\nand set the plotting parameters for matplotlib.\\nWe covered all of these in previous lectures.\\nThe one thing I wanted to point out specifically here\\nis that we are importing spearmanr from scipy.stats package.\\nYou can see that here.\\nSo you just start by running this.\\n(keyboard taps)\\nAnd as I said, the data sets are already loaded for you,\\nbut what you need to do is just run this\\nso we can take a look at the head of the dataframe,\\nthe cars dataframe.\\n\\nOkay, great.\\nSo we have a little preview\\nof what's inside the cars data set.\\nNow, let's just generate a quick pairplot from seaborn.\\nTo do that, we'll say sns.pairplot,\\nand we'll pass in cars, and run this.\\nLet it think for a little while.\\nOkay, great.\\nSo we've got our scatter plot matrix, but as you can see,\\nsince there's so many variables in the dataset,\\nit's pretty hard to visually see what's going on.\\n\\nSo I went ahead and selected some variables\\nfor our demonstration here.\\nSo let's just make a scatter plot matrix of these\\nso I can show you why I chose them.\\nWe'll call the subset x\\nand then we'll just select some variables\\nfrom our cars dataframe.\\nWe'll take the cyl variable, the vs variable,\\nthe am variable and the gear variable.\\n\\nAnd then we will call the pairplot function.\\nSet sns.pairplot, and then we'll pass in our x object,\\nand run this.\\nAll right, so there you have it.\\nNow let me explain why I chose these variables.\\nThe first thing I looked at is are these ordinal variables?\\nWell, if they're numeric\\nbut able to be ranked into categories,\\nthen yes, all of these variables are numeric.\\n\\nAnd they each assume only a set number of possible values.\\nSo yes, these variables are ordinal.\\nAre these variables related nonlinearly?\\nWell, based on this quick glimpse,\\nI don't see any linear relationships between the variables.\\nSo hopefully, yes.\\nLastly, is the data distribution\\nof each variable non-normal.\\nJudging from the histogram here, I'd say yes.\\nBased on this reasoning,\\nI decided to test the variables cylinder, vs, am and gear.\\n\\nSo next steps.\\nLet's just go ahead and isolate each of these variables.\\nSo we'll have cyl, and that's going to be equal\\nto the cyl column of our car dataframe.\\nAnd vs is equal to our vs column.\\nAm is equal to our am column.\\n\\nAnd gear is equal to our gear column.\\nOkay, so we isolated our variables here.\\nSo now, let's go ahead and let's just create some outputs\\nfor our Spearman rank correlation.\\nLet's do that by saying spearmanr_coefficient,\\n(keyboard taps)\\nunderscore coefficient.\\nP_value, and most of these equal to the spearmanr function,\\nand we'll pass in our cyl and vs variable pair here.\\n\\nNeed to change this order here to YL.\\nAnd then we'll also print out a label\\nso that we can really understand\\nwhat our test is telling us.\\nTo do that, we'll just write print,\\nand then we'll create a string which reads,\\nSpearman Rank Correlation Coefficient %0.3f.\\nSpearman Rank Correlation Coefficient %0.3f.\\n\\nAnd then close out the string.\\nAnd then we will write out our results,\\nwhich is going to be this placeholder here,\\nwhich is spearmanr_coefficient.\\nAnd check the syntax real quick.\\nLooks okay.\\nI'm going to run this.\\nOkay, so now what I'm going to do is I'm going to copy\\nthis code here so that we can use it\\nto calculate spearmanr for the other variable pairs.\\n\\n(mouse clicks)\\nSo the second variable pair here should be cyl versus am.\\nAnd then the third variable pair will be cyl versus gear.\\nOkay, so we have these, let's just run them real quick.\\nSo based on the Spearman's rank correlation\\ncoefficient of these three variable pairs,\\nthe cylinder vs variable pair\\nappears to have the strongest correlation.\\n\\nThe other variable pairs do show some correlation,\\nbut only a moderate amount.\\nThat was pretty easy.\\nNow let's look at the chi-square test for independence.\\nTo implement the chi-square test,\\nwe first need to start off by creating a cross tab.\\nWe'll call it table,\\nand we'll say cross tab equal to pd.crosstab.\\nWe'll pass the cylinder and am variables into this function.\\n\\nAnd then what we need to do\\nis we need to import our chi-square function.\\nSo that comes from the scipy library stats module.\\nSo we'll do an import by saying,\\nfrom scipy.stats import chi2_contingency.\\nOkay, now we have what we need\\nto actually implement a chi-square test.\\n\\nLet's write some placeholders for our output.\\nWe'll put chi2 as the first placeholder,\\nand then p and then dof, and lastly, expected.\\nAnd we'll set these equal to the output\\nof our chi contingency function.\\nSo chi2_contingency.\\n\\nAnd then we need to pass in our table.\\nThis is the table we just created,\\nbut we want to pass in only the values.\\nSo we'll say table.values.\\nAnd then let's print this out.\\nWe need a label so that it all makes more sense.\\nSo let that label be Chi-square Statistic %0.3f,\\nclose the string.\\n\\nAnd then we want to print out the p value, p_value label.\\nOkay, so we should actually close the string here.\\nThis is all one label.\\nAnd add a percentage sign.\\nAnd then our actual values that are generated by the task.\\nSo that's going to be chi2 and p placeholders here.\\n\\nOkay, and then we'll run this.\\nOkay, so it looks like there's a typo here.\\nLet's go ahead and take out this comma\\nand then also just fix the spelling here, table.\\nAnd then run this.\\nSo what we're getting here is a chi-square statistic\\nfor cylinder am variable pair.\\nLet's just generate a few more chi-square statistics\\non other variable pairs.\\nTo do that, I'm just going to copy this code over.\\n\\n(keyboard taps)\\nAnd then I'll change out the variable pairs.\\nSo let's change this for vs.\\nAnd then we'll switch out the am\\non the third variable pair for gear.\\nRun this.\\n\\nSo I'll move this up so we can see the results.\\nRemember, with the chi-square test,\\nwe need a p value greater than 0.05\\nin order to conclude that the variables\\nare independent of one another.\\nBased on what I see here, none of the p values\\nare greater than 0.05, so we must reject the null hypothesis\\nand conclude that the variable pairs are correlated.\\nThat's exactly how you would use the chi-square test.\\n\\nNow, let's look at how to do extreme value analysis\\nfor outliers.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4588027\",\"duration\":839,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Extreme value analysis for outliers\",\"fileName\":\"3006708_en_US_05_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"5 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1581,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to conduct extreme value analysis for outliers. This video covers point outliers, contextual outliers, and collective outliers.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":27084894,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now, let's look at extreme value analysis\\nfor outliers.\\nMost machine learning methods assume your data\\nhas been treated for outliers.\\nDetecting outliers can be a data preprocessing task\\nor an analytical method of its own merit.\\nBasically, use outlier detection\\nto uncover anomalies in data.\\nIn this section, we're going\\nto talk about univariate methods.\\nSome use cases for outlier analysis include detecting fraud,\\ndetecting equipment failure,\\nand also cybersecurity event detection.\\n\\nTukey methods are useful\\nfor identifying a variable's outliers.\\nYou can detect unusually high\\nor low data points in a variable\\nby applying the Tukey method for outlier detection.\\nData points identified using the Tukey method\\nshould be treated as potential outliers\\nto be investigated further.\\nThis is a Tukey Boxplot\\nand I wanted to point it out to you to show you\\nhow you can use a Boxplot to detect outliers.\\nBoxplot whiskers are set\\nat 1.5 times the interquartile range.\\n\\nThe interquartile range is really just a distance\\nbetween the lower quartile and the upper quartile.\\nThe upper quartile is where 25%\\nof data points are greater than the value.\\nAnd the lower quartile is where 25%\\nof data points are less than the particular value.\\nAny points beyond 1.5 times the interquartile range\\nare considered outliers.\\nThey'll show up in a boxplot visually as the dots\\nthat extend past the whiskers of the boxplot.\\n\\nThe other way to use Tukey methods to find outliers\\nis to use the Tukey outlier labeling method.\\nAnd this is essentially calculating the Tukey outlier\\nmathematically instead of using the boxplot.\\nSo let's look at how to do all of this in Python.\\nFor this demonstration,\\nwe're bringing in the standard libraries,\\npandas and numpy.\\nPlease note that I also imported matplotlib\\nand cbon as well as rcParams.\\nWe're going to be using scipy in this demonstration.\\n\\nSo all of these are already loaded into your notebook.\\nAnd I've also set the plotting parameters for matplotlib.\\nWe covered all of these in previous lectures.\\nWe're going to be using the iris data set\\nin this demonstration.\\nSo let's go ahead and get that imported.\\nWe'll start by setting the address variable\\nand we'll say that address is equal to\\nand then create a string.\\nAnd I'm going to go over to the notebook.\\n\\nYou'll need to do this for your setup.\\nSo we want iris.data.csv.\\nRight click \\\"Copy Path\\\"\\nand then copy that into the string.\\nOkay, great.\\nSo next, let's create a data frame called \\\"df\\\"\\nand we'll use the read CSV file.\\nSo we'll say, \\\"pd.read_csv\\\"\\nand then say \\\"filepath_or_buffer\\\" here.\\n\\nAnd we'll set that perimeter equal to \\\"address\\\".\\nAnd for header, we will say \\\"none\\\".\\n(keys typing)\\nAnd then it separated with comma.\\nSo for sep, we set that equal to comma\\nsince it's a column delineated file.\\nOkay, so now let's assign names to the columns.\\nSay \\\"df.columns\\\".\\n\\nAnd then we'll set that equal to a dictionary,\\nwhich contains column names.\\nSo the first column name is going to be \\\"Sepal Length\\\".\\nThe second column name is going to be \\\"Sepal Width\\\".\\nNext, \\\"Petal Length\\\".\\nAnd then, the last column...\\nThen, there's two more columns,\\nwhich are \\\"Pedal Width\\\" and \\\"Species\\\".\\n\\nShould be \\\"Species\\\".\\nOkay, now let's just create an X and Y variable.\\nFor X, we want that to be a data frame\\nthat contains the predictor variables.\\nSo in this case, we're going to use the iloc method.\\nSo we're going to call that off of the data frame\\nby saying \\\"df.iloc\\\".\\n\\nAnd then we're going to select only the first four columns.\\nSo we're going to say \\\":,0:4\\\".\\nOkay. And then we actually only want the values.\\nSo we'll just type \\\".values\\\" here.\\nAnd for our target, our Y variable,\\n\\\"y = df.iloc\\\"\\nand we're going to select the column\\nat the column index position four.\\n\\nSo we'll just put \\\":,4\\\".\\nAnd again, we only want the values.\\nSo then we're going to write \\\".values\\\".\\nAnd then let's go ahead\\nand just print out the first five records.\\nSo to do that, we'll say \\\"df\\\"\\nand then we'll select the first five records here.\\nRun this.\\nAh. \\\"PD is not defined.\\\"\\nThat makes sense because I didn't run this at the top here\\nwhen I opened the notebook.\\n\\nSo you always have to remember that even if your notebooks\\ncome preloaded, you, of course,\\nhave to run the cells in order\\nto import your libraries and modules that you need.\\nSo, okay, I ran everything here.\\nAnd then, now,\\nwe have the first five records in our dataset.\\nSo we've already gone ahead\\nand split that data frame into a set of X variables\\nas well as a Y variable.\\nSo now, let's begin looking at the Tukey boxplot.\\nWe can call the boxplot function off\\nof the data frame in order\\nto generate a boxplot automatically.\\n\\nSo to do that, we'll just say \\\"df.boxplot\\\".\\nAnd then we'll pass a parameter\\nthat says, \\\"return_type=dict\\\" for dictionary\\n'cause we want to return a dictionary.\\nAnd then plot this out with a plot function here.\\n\\\"plt.plot\\\". Run this.\\nOkay, so here we have a Tukey boxplot\\nfor our four numeric variables from our data frame.\\n\\nSo I'll go ahead and show you how\\nto actually use this boxplot to detect outliers.\\nSo you see here, we have a boxplot\\nand do you see those points that lay beyond the whiskers?\\nNow, those are our potential outliers.\\nWhat I did was I took a quick note of\\nwhere those outliers were found.\\nThat's the Sepal width column\\nand it's the values that are greater than four\\nor less than 2.5, approximately.\\n\\nLet's look a little closer at these values.\\nI'm going to use filtering in comparison operators\\nto isolate these values from the rest of the data frame.\\nSo let's go back over to our coding demonstration\\nand let's isolate Sepal Width.\\nSo let's get back over into our coding demonstration\\nand then we'll isolate Sepal Width.\\nTo do that, we'll just say \\\"Sepal_width = \\\"\\nand we'll set it equal to our X variable\\nthat we just created.\\n\\nAnd what we want to do is we want\\nto select our Sepal Width variable.\\nSo we're going to say \\\":,1\\\".\\nAnd then in terms of our outliers,\\nlet's create a second variable here called \\\"Iris Outliers\\\".\\nSo this would be \\\"iris_outliers\\\".\\nAnd we're going to say,\\n\\\"iris_outliers = Sepal Width greater than four\\\"\\nSo we'll just set this equal to a tuple,\\nwhere sepal_width is greater than four.\\n\\nAnd what we're actually trying\\nto do here is isolate the records\\nwhere this Sepal width is greater than four,\\nso that we can understand what's really happening\\nwith these data points.\\nNow, let's go ahead and print these out.\\nSo we'll say \\\"df\\\"\\nand then we want to print out the iris outliers.\\n(keys typing)\\nRun this.\\nOkay, it looks like I have a typo.\\nThis should be equals, not minus thing,\\nand I fixed that, rerun it, and great.\\n\\nSo we see that we have three records\\nthat have a Sepal width greater than four,\\nwhich makes sense\\nbecause, as you can see here,\\nthere are actually three small circles, right?\\n1, 2, 3, where they're outside the whiskers\\nthat are located at position of four here\\nand the Sepal Width variable.\\nLet's also isolate this value here that is below 2.05.\\n\\nSo what I'm going to do is I'm actually just going to copy\\nand reuse the code we just created here.\\n(keys typing)\\nOkay. And so all I need to do here is just change this\\nso that it is Sepal width less than 2.05 and then run this.\\nAnd as you can see,\\nwe have this one record here that is lower\\nthan the interquartile range\\nand it corresponds to this point here in the boxplot.\\n\\nLet me try to explain\\nwhat these records are actually telling us.\\nMoving the results over here,\\nI just wanted to point out\\nthat we now have the row index values for each\\nof the records that are coming back in looking suspicious\\nas outliers.\\nNow, I'm going to show you how\\nto do Tukey outlier labeling.\\nLet's go back over to the coding demonstration.\\nSo in this case, what we need to do is we need\\nto get some display settings.\\nSo we're going to say \\\"pd.options.display.float_format\\\"\\nand we're going to set that equal to a string value,\\nwhich contains a dictionary\\nand it's going to include a blank value.\\n\\nSo we'll create a string and then a dictionary.\\nSo \\\":1f\\\"\\nand then we're going to say \\\".format\\\".\\nOkay?\\nSo this is basically just setting up the display settings.\\nAnd let's create a X data frame.\\nWe'll call it \\\"X_df\\\"\\nand we'll set it equal to pd.\\n\\nAnd we're going to call the data frame constructor here\\nand pass in our X variable that we created earlier.\\nAnd then let's just print out a description\\nof these variables\\nthat are in the X data set that we created.\\nSo to do that, we're going to say \\\"print\\\",\\ncall the print function,\\nand then we're going to pass in X_df.\\nAnd off of that, we need to call the describe method.\\n\\nAnd then print this out.\\nNow, we have some descriptive statistics on each\\nof the variables in our data frame.\\nLet me explain to you what these actually mean.\\nLet's see how we can use them to find potential outliers.\\nThe interquartile range is the distance\\nbetween the third quartile and the first quartile.\\n75% is our third quartile, so we'll say\\nthat 3.3 minus 2.8, that's our first quartile.\\nThe difference between them is 0.5.\\nSo we multiply the interquartile range times 1.5\\nand we get a value of 0.75.\\n\\nTo find outliers from our first quartile,\\nwe'll just look at the value from the first quartile,\\nwhich here, it is 2.8,\\nand we will subtract out 0.75.\\nThis gives us a value of 2.05.\\nWe see that our minimum value is even less than that,\\nwhich means that it is suspicious for being an outlier.\\nFinding an outlier from the third quartile\\nuses the same approach.\\nIn this case, you would take the value\\nat the third quartile, which is 3.3,\\nand you'd add 0.75.\\n\\nThat gives us 4.05.\\nSince the max value in the Sepal Width column is greater\\nthan 4.05, we know that the Sepal Width is suspect\\nfor having outliers.\\nThat's it for univariate methods to finding outliers.\\nNext, I'm going to show you some multivariate methods.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4583162\",\"duration\":467,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Multivariate analysis for outliers\",\"fileName\":\"3006708_en_US_05_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"3 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":812,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to conduct multivariate analysis for outliers. This video covers box plots and scatter plot matrices.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15790874,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now let's talk about\\nmultivariate analysis for outliers.\\nUse multivariate methods to find outliers\\nthat only show up within combinations of observations\\nfrom two or more different variables.\\nThere are many different multivariate methods\\nto detect outliers.\\nWe are going to pick up where we left off\\nin the last section with the box plot,\\nand then I'm going to introduce you\\nhow to use scatterplot matrices to find outliers.\\nFor this demonstration,\\nwe're bringing in our standard libraries,\\npandas, matplotlib, and seaborn.\\n\\nSo all of these are already loaded in the notebook,\\nand I've also set the plotting parameters for matplotlib,\\nand I preloaded the iris dataset,\\nwhich we worked with in the previous demonstration.\\nSo all you need to do is run these code blocks.\\nAnd then let's just print out the first five records\\nin this data frame so we can get a look at the data.\\nAnd next, let's generate a box plot.\\nTo do that, we'll use seaborn's box plot function.\\nSo what you need to say is sns.boxplot.\\n\\nAnd for X, that's going to be equal to our species column.\\nSo you'll say X is equal to a string that reads species.\\nFor our Y variable, we're going to set that\\nequal to the sepal length label.\\nSepal length.\\nIn terms of the data we want plotted,\\nthat's going to be our data frame.\\n\\nSo we'll say data is equal to df.\\nAnd then with seaborne it gives us\\na lot of different color options.\\nSo in this case, we want our hue to be set equal to species.\\nAnd we'll set our color palette equal to HLS.\\nSo we'll say palette equal to,\\ncreate a string and write HLS.\\n\\nAnd lastly, we'll just say legend is equal to false.\\nOkay, so checking to see if I made any typos.\\nThe only thing I can see is that\\nthis should be boxplot instead of barplot.\\nSo let me change that.\\nI'm going to run this.\\nAnd great.\\nSo now we have a box plot.\\nAnd there are two things I want to point out\\nabout this box plot here.\\nOne is that we are plotting sepal length against species.\\nSo we're actually plotting two variables in one box plot.\\n\\nWhen we do that, the outlier falls out, as you can see here.\\nIt's passed the whiskers in the virginica species.\\nAnd this would be considered as\\nsuspicious for being an outlier.\\nNow let's look at the scatterplot matrix.\\nIt's really easy to generate\\na scatterplot matrix using seaborn.\\nSo we'll just say sns.pairplot.\\nAnd we're going to plot out our data frame.\\nSo we'll pass in df as the first variable.\\n\\nWe'll set our hue equal to species.\\nAnd again, we'll set our pallet equal to HLS.\\nWe run this.\\nAnd look how beautiful that is.\\nSo now we have a great scatterplot matrix.\\nLet me take you over to the other side\\nto explain what all of this actually means.\\n\\nSo we already know that our sepal width\\nvariable is suspect for outliers.\\nIf you look at each of the scatterplot matrices,\\nthere's an odd red point\\nthat doesn't fit any of the other clusters.\\nAnd so I've added a circle to that\\nand pulled it up from the data table.\\nThat's actually record 41.\\nSo I just jotted that down.\\nAnd I keep that in mind,\\nto investigate whether that's an outlier\\nand whether it needs to be removed.\\nAnd that's it for using\\nmultivariate outlier detection methods.\\n\\nNow let's look at applying Tukey outlier labeling.\\nThis is basically just a manual process for finding outliers\\nif we don't use the box plot.\\nSo in this case, what we need to do\\nis we need to get some display settings.\\nSo we're going to say pd.options.display.float format.\\nAnd we're going to set that equal to\\na string value, which contains a dictionary,\\nand it's going to include a blank value.\\n\\nSo we'll create a string and then a dictionary.\\nSo colon 0.1 F.\\nand then we're going to say dot format.\\nOkay.\\nSo this is basically just setting up the display settings.\\nAnd let's create a X data frame.\\nSo we'll call it X_df,\\nand we'll set it equal to pd.\\n\\nAnd we're going to call the data frame constructor here\\nand pass in our X variable that we created earlier.\\nAnd then let's just print out\\na description of these variables\\nthat are in the X data set that we created.\\nSo to do that we're going to say print,\\ncall the print function, and then we're going to pass in X_df.\\nAnd off of that we need to call the describe method.\\n\\nAnd then print this out.\\nNow we have some descriptive statistics\\non each of the variables in our data frame.\\nLet me explain to you what these actually mean.\\nLet's see how we can use them to find potential outliers.\\nThe interquartile range is the distance between\\nthe third quartile and the first quartile.\\n75% is our third quartile.\\nSo let's say 3.3 here.\\nMinus 2.8.\\n\\nThat's our first quartile.\\nThe difference between them is 0.5.\\nSo we multiply the interquartile range times 1.5,\\nand we get a value of 0.75.\\nTo find outliers from our first quartile,\\nwe would just look at the value from the first quartile,\\nwhich is 2.8, and we would subtract out 0.75,\\nwhich gives a value of 2.05.\\nWe see that our minimum value is even less than that,\\nwhich means that it's suspicious for being an outlier.\\n\\nFinding an outlier from the third quartile\\nuses the same approach.\\nIn this case, you would take\\nthe value from the third quartile, which is 3.3,\\nand you'd add 0.75.\\nThat gives us 4.05.\\nAnd since the max value of the sepal width column\\nis greater than 4.05, we know that the supple width\\nis suspect for having outliers.\\nThat's it for univariate methods to find outliers.\\n\\nAnd next, I'm going to show you\\nmultivariate analysis for outlier detection.\\n\"}],\"name\":\"5. Exploratory Data Analysis\",\"size\":160035610,\"urn\":\"urn:li:learningContentChapter:4579303\"},{\"duration\":1595,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4589020\",\"duration\":648,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Cleaning and treating categorical variables\",\"fileName\":\"3006708_en_US_06_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1236,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about categorical encoding including one-hot, binary, and vectorizations.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":23468515,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's take another look\\nat categorical variables and why we might need\\nto treat categorical variables as well\\nas the options we have for treating them.\\nAs you recall, a categorical variable is a type\\nof variable that can take on only a limited\\nor fixed number of possible values.\\nFor example, fruit types is a categorical variable\\nas there are only a limited number of types of fruits.\\nSay for example, apples, oranges, lemons,\\nthere's not an infinite number\\nof fruit types, so it's categorical.\\n\\nIn the field of machine learning,\\nit's common to come across categorical variables\\nwhen addressing data science challenges.\\nTypically, machine learning algorithms are not equipped\\nto directly process categorical data.\\nTherefore, we have to transform this type of data\\ninto numerical formats that are compatible\\nwith machine learning algorithms.\\nThis transformation can be done through various methods,\\nincluding label encoding, one-hot encoding, among others.\\nThe conversion of categorical variables\\ninto numerical forms is known as encoding.\\n\\nIn this coding demonstration,\\nI will demonstrate two common encoding techniques,\\nlabel encoding and one-hot encoding.\\nWe'll be using the scikit-learn library\\nto implement these encodings.\\nDuring the coding demonstration,\\nwe will explore how to transform categorical variables\\ninto formats that are interpretable\\nby machine learning models.\\nLet's get started importing the required libraries\\nfor cleaning and treating categorical variables,\\nI've already imported numpy in Pandas,\\nand now we'll also import the other required libraries.\\n\\nOne thing I wanted to point out here is\\nthat from sklearn, the preprocessing module,\\nwe're importing LabelEncoder and OneHotEncoder.\\nWe will need both of these functions for this demonstration.\\nHere you can see I've already created\\nthe dataset that we will work with.\\nHere are the columns,\\nand as you can see in the gender column,\\nit has some missing values.\\nSo let's convert this dataset\\nto a data frame and then print it out\\nto get started working with it.\\n\\nWe'll call the data frame df\\nand we'll say df is equal to data frame constructor,\\nand then we'll pass in the dataset,\\nwhich is called data, and then print this out.\\nThere are different ways to handle missing data\\nand categorical variables.\\nIf there are just a few missing values,\\nthen we can drop the rows that contain the missing values.\\nOr if there are a lot of missing values in a column,\\nthen we can drop that column altogether.\\nWe can also replace the missing values\\nwith the most frequent value of that column or row.\\n\\nOf course, let's start by adopting the most logical way\\nof handling missing categorical data points.\\nIf you think about it here with this dataset,\\nyou cannot fill the missing values\\nin the gender column with the most frequent values\\nbecause there's a chance of assigning\\nthe wrong gender to a person.\\nSo in this case, we'll have to just drop the gender column.\\nTo do that, we'll say df is equal\\nto df.drop, call the drop method,\\nand then here we'll pass the column name gender\\nthat we want to drop as the first perimeter.\\n\\nAnd in the second perimeter,\\nwe will pass Axis equal to 1,\\nwhich refers to the columns of the data frame.\\nAnd we'll print this out, and as you can see,\\nthe gender column is dropped from the data frame.\\nNext, let's try to represent the information\\nthat's contained within the names field\\nsuch that it's represented by categorical numerical data.\\nThis will require a two-step approach.\\nFirst, we'll use label encoding\\nto create a numerical representation\\nof each value in the names field.\\n\\nAnd then after that, we'll use OneHotEncoder\\nto convert each value of these numerical values\\ninto its own unique categorical column.\\nFor labeling coding, we'll use\\nsklearn's label encoding function\\nto transform the names into categorical numerical values.\\nHow label encoding function works is\\nthat it encodes the target labels,\\nin this case, names, with values between zero\\nand N minus one, where N is the total number\\nof unique values in the variable.\\n\\nTo do this, first, we'll create an object\\nof the label encoder.\\nSo we'll say label_encoder\\nand we'll set this equal to label encoder.\\nThen we'll call the label encoders a fit function\\nand pass column with a categorical data in it.\\nSo we'll say label_encoder.fit\\nand we will pass in df['names']\\nbecause this is the variable\\nthat we want to have transformed.\\n\\nWhat this does is that it's going to create a numerical mapping\\nthat maps the names labels to categorical numerical values.\\nSo I'll run this.\\nNow we will generate the encodings\\nof the categorical variable by calling the transform method\\noff of the label encoding class.\\nSo let's call this label encoded names,\\nlabel_encoded_names\\nand we'll set this equal to our label_encoder\\nand we'll call the transform method off of that,\\nand we'll pass in again, we'll pass in our names column.\\n\\nSo df['names']\\nand print this out.\\nWhat this is going to do is it's\\ngoing to generate their encodings.\\nSo here, you see the output,\\nthe numerical encodings have been\\ngenerated for the names column.\\nNow we need to transform the categorical data\\nusing one-hot encoding.\\nIn one-hot encoding, each categorical value is converted\\ninto a new categorical column\\nand it is assigned a binary value 0 or 1\\nfor whether the data point is true or false for that value.\\n\\nLet's just try it out so you can see how it works.\\nFirst, we'll create the object of class onehot_encoder.\\nSo we'll say OneHotEncoder\\nand we want to pass a parameter\\nthat says sparse_output is equal to false.\\nAnd what we'll do is we'll call this the onehot_encoder\\nand then run this, then we'll just call the fit method\\noff of the class onehot_encoder.\\n\\nSo to do that, we'll say onehot_encoder.fit\\nand we'll pass in our data frame names column.\\nIt looks like I have a typo.\\nI'm missing a set of brackets here,\\nso I'm going to go ahead and add those,\\nand run this again.\\nWhat this has done is that it's performed\\none-hot encoded mapping of the categorical values.\\n\\nNow let's transform the data\\nby calling the transform function\\nand passing the categorical values.\\nWe'll call this onehot_encoded_names.\\nSo onehot_encoded_names\\nand we'll set it equal to our onehot_encoder.\\nWe'll call the transform method\\nand we'll pass in our data frame,\\nnames column, and run this.\\n\\nLastly, let's just save this as a data frame.\\nSo we'll say one-hot encoded data frame,\\nonehot_encoded_df.\\nWe'll call the data frame constructor.\\nFirst, we'll pass the output encodings,\\nso we'll say onehot_encoded_names here.\\n\\nThen we'll pass the mappings\\nof all the columns as column names.\\nSo we'll say columns is equal to\\nonehot_encoder.categories.\\nNext, we'll assign the original column\\nand the data frame as names.\\nSo we'll say onehot_encoded_df\\nand we'll select the names\\nand we'll set this equal to df[['names']] column.\\n\\nAnd we'll print this out\\nby saying onehot_encoded_df,\\nI'm just looking at the syntax real quick\\nbefore running this.\\nAnd okay, so now we have transformed our names\\ninto a set of categorical numerical variables\\nthat are represented in binary format here, as you can see.\\nSo each of the names has been represented\\nas its own categorical variable in the dataset.\\n\\nAnd then for where the value is\\nactually true for that data point,\\nwhich is actually Steve in the original dataset,\\nthat value gets a 1.0.\\nEvery other value in that column gets a 0.0.\\nAnd that's the basics of label encoding\\nand one-hot encoding.\\nNext, let's look at transforming dataset distributions.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4590005\",\"duration\":415,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Transforming data set distributions\",\"fileName\":\"3006708_en_US_06_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":584,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to transform data set distributions. This video covers preparing data for machine learning, normalization, and standardization.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13592207,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] The term data transformation\\nrefers to the practice of changing data\\nfrom its original state into a different format.\\nThis often includes turning raw data\\ninto a format that is clean and ready for use.\\nIn this coding demonstration, we're going to explore\\na variety of beneficial data transformations\\nand look into those scenarios in which they're necessary.\\nWe'll focus on two specific data transformation techniques,\\nnormalization and standardization.\\n\\nNormalization, also known as min-max scaling is a method\\nwhere data values are adjusted and scaled\\nto fall within a range of zero to one.\\nThis technique maintains the original distribution of values\\nwithout altering their ranges.\\nOn the other hand, standardization is a technique\\nthat re-scales data so that it has a mean value of zero\\nand a standard deviation of one.\\nThis effectively normalizes the distribution of the data.\\nKeep in mind that in machine learning,\\nnot every data set necessitates normalization.\\n\\nIt's only required when the features\\nwithin the data set have varying ranges.\\nThe decision to use either normalization or standardization\\ndepends on the specific problem at hand\\nand the machine learning algorithm you're using.\\nThere is no strict rule that dictates the use\\nof normalization or standardization.\\nA practical approach is to initially feed\\nthe machine learning model with raw data,\\nas well as both normalized\\nand standardized versions of the data.\\n\\nBy evaluating the performance of the model\\nunder these different conditions,\\none can determine the most suitable type\\nof data transformation for the given scenario.\\nLet's take a look inside of Jupyter.\\nIn this demo, I'm going to show you\\nhow to transform dataset distributions.\\nAs you can see, I've already imported the required libraries\\nthat we're going to be using in this demonstration\\nwhich are numpy, pandas, matplotlib and sklearn.\\nFor pre-processing data,\\nwe're going to need the MinMaxScaler and the scale\\nfrom sklearn's preprocessing module.\\n\\nSo just pointing out that I have set those\\nand imported them here.\\nSo I'm going to run this.\\nAnd we're going to be using the mtcars data set\\nin this lecture.\\nSo I have imported that data set\\nand gotten that ready for us.\\nLet's look at the first five records\\nby calling the head method.\\nSo yeah, we've covered all of these things\\nin previous lectures and it just saves time\\nfor me to include these in the notebooks.\\nSo let's work with the miles per gallon column\\nwhich is represented by mpg.\\n\\nLet's see how we can transform its values\\nusing normalization and standardization.\\nThe first thing I want to do\\nis just plot the values of the mpg columns.\\nSo we'll just call the plot function, plt.plot,\\nand we'll pass in our data set, mpg column here.\\nAnd run this.\\nOkay, and then we have a visualization\\nof the distribution of the data points in the mpg variable.\\n\\nThat's what I wanted to create as a baseline\\nto compare this to as we work to transform\\nand normalize the data.\\nSo let's start first with normalization.\\nWe're going to normalize the values of the mpg column\\nusing sklearn's MinMaxScaler function.\\nSo we'll first create the object MinMaxScaler class.\\nWe'll say minmax_scalar,\\nand we'll set it equal to MinMaxScaler.\\n\\nAnd then what we want to do\\nis call the MinMaxScaler fit function.\\nSo we'll say minmax_scalar.fit,\\nand then we'll pass in the column, mpg.\\nAnd run this.\\nNow, if we call the MinMaxScaler's transform function\\nand then pass in the mpg variable,\\nthis will transform the values of the mpg column\\nso that they are distributed\\nas a series of numbers between zero and one.\\n\\nWe'll call it scaled_data, scaled_data.\\nWe'll set it equal to minmax_scalar.\\nWe'll call the transform method,\\nand we'll pass in our mpg column here.\\n(keyboard taps)\\nAnd then we'll just go ahead and plot this out\\nso you can see how the data has changed.\\nSo to plot it out, we'll use the plot function plt.plot,\\nand we'll pass in our scaled_data.\\n\\n(keyboard taps)\\nAnd run this.\\nNow, on its face, it looks just like the original plot,\\nbut if you look at the y-axis, the values have been rescaled\\nsuch that they fall between the values of zero and one.\\nWhereas in the original plot,\\nthe data points fell between 10 and 35.\\nBut there wasn't any distortion\\nin the range of the data points.\\nIt looks like the same distribution,\\nbut the values of the y-axis have just been scaled.\\n\\nNow, let's standardize the values of the mpg column\\nusing scikit-learn's scale function.\\nTo do that, we'll say standard_scalar is equal to scale,\\nstandard_scalar equal to scale.\\nAnd then let's pass in our mpg column.\\nSo we'll say, dataset mpg.\\nLet's just go ahead and plot this out, plt.plot,\\nand we'll pass in our standard_scalar, and run this.\\n\\nAnd again, the distribution of the data points\\nlooks identical to the previous two charts.\\nBut in this plot, we can see that the data points\\nhave been rescaled such that they have a mean equal to zero\\nand a standard deviation of one.\\nAs with normalization, the data points\\nhave not been skewed or distorted in any way.\\nNow that you know how to transform your data sets,\\nyou should be ready to get started\\nwith basic machine learning algorithms.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4579302\",\"duration\":532,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Applied machine learning: Starter problem\",\"fileName\":\"3006708_en_US_06_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":798,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to use the data in a ML problem by formatting predictors and labels, and then passing them to a simple ML algorithm.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":16862354,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's dig into the process\\nof preparing a dataset\\nfor training a machine learning model.\\nOnce we have our dataset ready,\\nwe'll proceed to train\\na fundamental machine learning model\\nusing this data we've prepared.\\nThe preparation of a dataset\\nfor a machine learning models encompasses several steps.\\nMany of which, we've already covered.\\nThese include collecting the data,\\nfiltering out irrelevant features,\\nand managing existing values.\\nBuilding on this foundation, the next crucial step is\\nto format the data in a way that makes it suitable\\nfor input into a machine learning model.\\n\\nThis involves separating the dataset into features,\\nthe inputs and labels, the outputs we want to predict.\\nAdditionally, the dataset needs to be split\\ninto training and validation sets.\\nThis partitioning allows for the training\\nof the machine learning model on one subset of the data\\nwhile the other subset is used for validation.\\nThis is helpful for assessing\\nthe model's performance and effectiveness.\\nIn the coding demonstration I'm about to walk you through,\\nwe'll be utilizing various modules from scikit-learn.\\n\\nInitially we'll apply scikit-learn's\\ntrain test split function to divide our dataset\\ninto a training set and a validation set.\\nFollowing that, we will train a decision tree classifier\\nusing our dataset.\\nLastly, we'll employ scikit-learn's metrics module\\nto assess the performance of our model.\\nLet's get started\\nand see how these steps unfold in practice.\\nI've already imported the required libraries\\nthat we will use in this demo.\\nThose are pandas and sklearn.\\n\\nAlso, please note that\\nI'm importing metrics from sklearn here,\\nas you can see right here,\\nand that I've preloaded the iris dataset for you\\nthat we've been working with earlier demonstrations.\\nIris is a species of flowering plants.\\nIts dataset contains five columns,\\nwhich include petal length, petal width,\\nsepal length, sepal width, and species type.\\nWe'll try to predict the species type\\nusing the other features within the dataset.\\n\\nSo let's just start off here\\nby calling the head method off of this dataset.\\nLet me make sure I run this code block here\\nto import our libraries\\nand then we'll call the head method.\\nGive that a chance to run.\\nOkay, great.\\nSo this is the basic setup of the data\\ninside of the dataset.\\nThese are the first five records.\\nSo let's start off by just taking a look\\nat the unique species of iris flowers\\nthat are represented within this dataset.\\n\\nTo do that, we're going to call the unique method.\\nSo we'll say dataset.Species.unique\\nand run this.\\nAnd what we see here is that there are three types\\nof iris species in this dataset,\\nsetosa, versicolor, and virginica.\\nNext, we'll separate features and labels from the dataset.\\nThe second, third, fourth, and fifth columns\\ncontain features that describe the flower.\\n\\nAnd the sixth column contains the species labels.\\nFirst, let's separate the features\\nand save them as an X variable.\\nSo we'll say X is equal to dataset.iloc,\\nand then we'll tell it to return all of the rows\\nand only the second through fifth columns.\\nSo we'll pass in one through five here and print this out.\\n\\nAnd what we can see is that\\nthe dataset X now contains all\\nof the features from the dataset,\\nbut it no longer contains the label, the species label.\\nNow what we need to do is we need to separate the label\\nand save it as a Y variable.\\nSo we'll say Y is equal to dataset.iloc.\\nAnd in this case we want to select all of the rows,\\nbut only the sixth column.\\n\\nSo we'll pass a five here and then print this out.\\nAnd as you can see now we have all of the labels,\\nall of the species labels,\\nbut then none of the other variables in the dataset.\\nNext, we'll split the features and labels\\ninto training and test sets.\\nFor this, we'll use sklearn's train test split function.\\nAnd the training set will be used\\nto train the machine learning model.\\nThe test set will be used\\nto test the accuracy of our trained model.\\n\\nLet's write the code.\\nWe'll say x_train, x_test,\\ny_train and y_test.\\nThese are our placeholders.\\nAnd then we'll set them equal\\nto the train test split function.\\nSo it's train_test_split.\\nAnd then what we need to do is pass in our variables.\\n\\nSo first we're going to pass in our X dataset.\\nThen we'll pass in our target variable, which is Y.\\nIn the third parameter, we'll pass in the ratio\\nof the test data from the whole dataset.\\nSo here we'll need to say test_size equal to 0.3.\\nAnd what this means is that 30% of the data\\nwill become the test set\\nand 70% of the data will be the training set.\\n\\nAnd then let's set a seed for a random state.\\nWe'll just set random_state equal to zero,\\nand that's so that you get the same results on your screen\\nas we get in the demonstration here, we'll run this.\\nAnd now it's time to train a decision tree classifier\\non the training dataset.\\nFirst we'll create an object\\nof decision tree classifier class.\\n\\nSo to do that, we'll say clf is equal to.\\nAnd then we'll call DecisionTreeClassifier.\\nAnd then second, we're going to call the fit function.\\nSo we'll say clf.fit,\\nand then we'll pass in our x_train and y_train.\\nRun this.\\nOkay, because the Decision tree classifier is now trained,\\nwe can use it to predict labels for our test set.\\n\\nTo do that,\\nwe'll say y_predict,\\nand then we'll set it equal to clf.predict.\\nAnd we will pass in our x_test here\\nand then print this out.\\nAnd what you see as a result is that\\nyou see all of the predicted labels on the test dataset.\\n\\nThe last thing I want to show you how to do is\\nto evaluate our classifiers performance\\nby comparing the predicted labels\\nwith the original labels of the test set.\\nWe'll use the accuracy metric to evaluate the results.\\nAnd the function that we'll use is\\nsklearn's accuracy score.\\nSo we'll say accuracy is equal to metrics.accuracy_score\\nand we'll pass it in our y_test\\nand our y_predict variable.\\n\\nAnd then let's just print this out.\\nAnd when doing so, we'll create a little label here\\ncalled accuracy,\\nand we're just printing out our accuracy.\\nSo just pass that variable in and run.\\nAnd as you can see, the train model\\npredicted the labels of the test set\\nwith more than 90% accuracy.\\n\"}],\"name\":\"6. Getting Started with Machine Learning\",\"size\":53923076,\"urn\":\"urn:li:learningContentChapter:4583165\"},{\"duration\":5391,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4588028\",\"duration\":145,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introduction of web scraping\",\"fileName\":\"3006708_en_US_07_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":223,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about data scraping from web sources to obtain data from multiple online sources. Learn why web scraping is needed in data science.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3689857,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Web scraping is a cornerstone technique\\nin data science.\\nWeb scraping automates the extraction of data from websites,\\nthus allowing us to gather large amounts\\nof information really quickly.\\nThe tool we use for this purpose is known as a web scraper.\\nToday, we'll dig into how web scrapers transform\\nthe raw data from websites into a format\\nthat's readily available for analysis.\\nYou might wonder why web scraping is so important.\\nThe internet is a treasure trove of data, stock prices,\\nsports statistics, product details, and more.\\n\\nManually collecting this data is a daunting task, though.\\nThat said, it's essential for many business activities\\nincluding market research and analytics.\\nWeb scrapers streamline this process\\nby efficiently gathering data that's crucial\\nfor organizational decision making.\\nWeb scraping offers several advantages.\\nIt enables the automatic extraction of fast data sets\\nwhile also significantly reducing manual efforts\\nand associated costs.\\nThis method is not only time-efficient,\\nbut also cost-effective.\\n\\nDeploying multiple web scrapers simultaneously\\naccelerates data collection while minimizing\\nthe human error, which is common in manual data gathering.\\nSo how does web scraping work?\\nThe process begins with identifying the target website.\\nWeb scrapers send requests to retrieve\\nthe site's HTML content,\\nwhich includes all of its data and code.\\nThe next step involves pinpointing\\nand extracting the necessary data\\nfrom the specific HTML text.\\n\\nThis extracted data is then cleaned, structured,\\nand stored in a database for further use.\\nWeb scraping has a diverse range of applications.\\nIt's a critical part of generative AI, machine learning,\\nand data analytics.\\nIt offers insights for market research\\nand competitive analysis.\\nWeb scraping is also instrumental in website migrations\\nwhere data is transferred from one site to another.\\nAll this said, it's important to recognize legal\\nand ethical boundaries.\\n\\nNot all data is permissible to script,\\nespecially when it involves confidential\\nor personal information.\\nEthical scraping respects privacy\\nand complies with legal standards.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4589021\",\"duration\":577,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Python requests for automating data collection\",\"fileName\":\"3006708_en_US_07_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":834,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about the basics of the requests library, how connections are established, response headers, body, content-types, and metadata extraction.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":17782415,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now it's time to talk\\nabout the requests library.\\nRequests is a Python library which is used\\nto make all sorts of HTTP requests.\\nIt's a human friendly HTTP library.\\nRequests Library provides a lot of customizable features\\nthat can be used for all sorts of different types of tasks.\\nAnd in this coding demonstration we're going to work on next,\\nI will teach you how to make requests using HTTP methods\\nand how connections are established.\\n\\nWe'll also look at to use requests to get response headers,\\ncontent types, and response content.\\nSo let's get started.\\nLet's start this demonstration\\nby importing the required libraries.\\nSo in this case, it's going to be the requests library.\\nWe'll say import_requests.\\nRun this.\\nAnd one of the most common HTTP methods is get.\\nthe get method gets or retrieves data\\nfrom a specific resource.\\n\\nLet's make a get request using the requests library\\nand check its response.\\nSo we'll say response\\nis equal to requests.get\\nand then we'll pass in the URL\\nhttps://www.python.org\\nto retrieve data from the original Python website's URL.\\n\\nLooks like I missed a Y here, so I'll add that.\\nAnd then print this.\\nThis needs to be a string.\\nSo let me add single quote\\naround each side of the string here.\\nAnd then we run it.\\nAnd when we run this code,\\nwe see that it returned a Response (200).\\nThis 200 response means that the connection\\nwith the website was successful.\\n\\nNow let's dive a little deeper\\ninto the response of that request.\\nWhen we make a request to a server like this,\\nit returns extra information with a response called headers.\\nHeaders contain all of the metadata of the URL\\nfor which we made the request of the server.\\nLet's print the response header.\\nSo we'll say response.headers\\nand run this.\\nAnd what you see is all of the headers information\\nthat is returned in the response.\\n\\nGoes on quite a ways here.\\nYeah, there's quite a bit of information.\\nOne type of information\\nthat's returned in headers is the content type.\\nContent type indicates the media type\\nof the returned content.\\nFor example, if the returned content is a simple HTML page,\\nthen its content type will be text/html.\\nSimilarly, if the returned content is a PDF file,\\nthen its content type will be application/pdf.\\n\\nLet's check the content type of the response object.\\nSo we'll say response.headers\\nand then we want to look here at the content type,\\nContent-Type\\nand run this.\\nAnd now we see that the content type\\nof the response object is text/html\\nwhich indicates that the returned content\\nis a simple HTML page.\\n\\nIn general, we use Python request\\nto fetch content from a server.\\nResponse content is the information\\nabout the server's response that's delivered back to us\\nwhen we send a request.\\nIf we want to return the content of the response in bytes,\\nwe'd say response.content like this.\\nresponse.content\\nAnd what you're seeing here is that we have returned\\nall of the content of the response in bytes.\\n\\nNow let's compare sequences in lines of text.\\nTo do that, we will need to import the diflib module.\\nSo we will say import_diflib\\nand run that.\\nNow let's create two sequences that we can compare.\\nThe first will be called flines\\nand we'll set that equal to a string that reads,\\nHello.\\n\\nHow are you?\\nI am fine.\\nAnd then the second sequence\\nwill be an object called glines,\\nand that will be equal to a string that reads,\\nHow are you, Lillian?\\nI am doing well.\\nOkay, so simple.\\n\\nI'll put a period here at the end\\njust for good grammar, at the end of the word fine.\\nAnd run this.\\nAnd next we need to create a differ object,\\nwhich we can use to compare the sequences of text\\nand find differences between them.\\nWe'll call this differ object d.\\nWe'll set it equal to diflib.Differ class,\\nand to compare the sequences,\\nwe need to call the compare method off of the d object.\\n\\nLet's call this whole thing diff\\nand we'll set diff equal to d.compare.\\nAnd then we want to compare our flines\\nagainst our glines.\\nSo we'll pass those names in.\\nThen we need to iterate over the sequence\\nand print out the differences.\\nOne problem here though,\\nbefore moving forward, I see a typo.\\nI have a stray dot here, so let me get rid of that.\\n\\nAnd then iterating over sequences\\nto print out the difference.\\nTo do that, we're going to use a for loop\\nthat iterates over the generator that's returned by compare.\\nSo we will say for line\\nin diff\\nprint(line)\\nWe'll call the print function\\nand we'll just have it print out the line.\\n\\nI need to add a colon here.\\nAnd then I'll run this.\\nOkay, it looks like I have a typo.\\nAh, okay, so I called it fline.\\nI forgot an S when I was creating these objects.\\nSo I'll just add an S here.\\nSo F line is equal to, \\\"Hello, how are you? I'm fine.\\\"\\nAnd then rerun this code block.\\nAnd there we go. We have our output.\\n\\nAnd let's take a look at what this actually means.\\nFirst off, what you're seeing here\\nis that each iteration has yielded a line\\nthat has been compared and printed to the console.\\nThe lines from the generator can include information\\nsuch as which elements are present in one sequence,\\nbut not the other, which are common to both,\\nand which are present in the second sequence,\\nbut not the first.\\nOn the bottom here you can see a notification\\nthat the output is truncated\\nand we can view it as a scrollable element\\nby clicking on this link here, which I'll do now.\\n\\nAnd then let's examine these results.\\nNow, if you look closely, we can see that the elements\\nthat are present in the first string,\\nbut not in the second string,\\nare marked with a negative sign.\\nSo for example, this word hello.\\nLet's look back.\\nHello is in the first line, but it's not in the second line.\\nHello is in flines, but it's not in glines.\\nAnd that difference is indicated\\nwith a negative symbol here.\\n\\nFor elements that are present in both strings,\\nthey're given no symbol or just a blank space.\\nSo for example, the word how.\\nHow exists in both flines and glines.\\nSo it's assigned the symbol\\nof a blank space in the output.\\nAnd for elements that are in the second string,\\nbut not the first, you could probably guess this,\\nbut those are indicated by a positive sign.\\n\\nSo for example, here the word Lillian has plus signs.\\nAnd as you can see,\\nthe word Lillian is in the glines variable,\\nbut it's not in the flines variable.\\nThat's why it's been assigned a plus sign as an indicator.\\nThat's the basics of what you need\\nto know about Python requests.\\nNow let's look at the beautiful soup object.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4588029\",\"duration\":1144,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"BeautifulSoup object\",\"fileName\":\"3006708_en_US_07_03_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1857,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to work with objects. This video covers the Beautiful Soup library and BeautifulSoup objects.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":42977993,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's look at web scraping with Python.\\nI'm about to show you how to scrape data from the internet.\\nBut before jumping in, let me give you a brief introduction\\nto web scraping by explaining how it's useful.\\nImagine you're a small business owner\\nand you've got a blog.\\nYou decide that you want to create a new resources page,\\nand on that page you want\\nto include every link from your blog.\\nWhat would you do?\\nGo through each page of the blog manually\\nand pull all of the links?\\nThat would take forever.\\n\\nWhat you could do instead is use Python\\nto automatically go through your blog for you\\nand extract every link from every page.\\nThat way you could just copy\\nand paste the links onto your new resource page\\nand it would be a lot more efficient.\\nI've seen environmental engineers\\nwho use Python to scrape web data from weather station pages\\nin order to gather sufficient data for hydrology analysis.\\nI've also seen Amazon vendors\\nwho scrape web data from competing Amazon vendor pages\\nso that they can use that data\\nto populate their product descriptions\\nusing a semi-automated approach.\\n\\nI've seen humanitarian volunteers scrape web data\\nfrom a foreign country's census site\\nso that the data could be used\\nto quickly form a resource allocation plan.\\nAnd last, but of course not least, without web scraping,\\nthere would be no generative AI.\\nScrape content in the form of copy and images\\nis a baseline necessity of training generative AI models.\\nWithout the source data that's scraped from the web\\nThere would be no chatGPT, Midjourney and what have you.\\n\\nIn other words, web scraping is useful\\nfor an almost unlimited number of applications.\\nIn the coding demo that's coming up,\\nI'm going to teach you about objects in Beautiful Soup\\nand how to work with them.\\nLater in the course, I'm going to teach you to work\\nwith parse data, scrape a webpage, and save your results.\\nThere are four main object types in Beautiful Soup.\\nThose are BeautifulSoup object, tag object,\\nNavigableString object and common object.\\n\\nThe BeautifulSoup object is a representation\\nof the document you're scraping as a whole.\\nIt's easily navigable and searchable.\\nTag elements correspond to XML\\nand HTML elements in an original document.\\nYou can navigate the reference data using tag attributes.\\nA NavigableString object is\\nto add a bit of text within tag.\\nBeautiful Soup uses NavigableString class\\nas a container for bits of text.\\n\\nAnd lastly, the comment object.\\nThe comment object is a type of NavigableString object\\nthat you can use for commenting your code.\\nIn the coding demonstration that's coming up,\\nI'm going to teach you about these objects in Beautiful Soup\\nand how to work with them.\\nIn this coding demonstration,\\nI'm going to teach you about objects in Beautiful Soup\\nand how to work with them.\\nLater in the course, I'm going to teach you how to work with\\nparse data, scrape a webpage, and save your results.\\n\\nBefore getting started here, I just want to check\\nto make sure that our version of Python is compatible\\nwith the demonstration we're about to do.\\nSo this notebook was written for Python 3.10\\nand let's just check the version we're running here.\\nSo we'll say import sys\\nand then let's print sys version.\\nSo we'll say sys.version and run this.\\n\\nAnd it looks like we have version 3.10,\\nso we're good to go.\\nNow let's go ahead\\nand import our Beautiful Soup into the Jupyter Notebook.\\nSo we'll say from bs4 import BeautifulSoup\\nand run this.\\nGreat, so now we have Beautiful Soup to work with\\ninside of our Jupyter Notebook.\\n\\nNow what I've done for this demonstration\\nis I'm providing you an HTML document\\nso you don't need to type all of this stuff out of course.\\nAnd so your Jupyter Notebook\\nis coming loaded with this HTML.\\nAnd we're going to use it to begin exploring\\nthe different types of objects within Beautiful Soup.\\nAll you have to do with this is run the block.\\nScroll to the end and then run it.\\n\\nLet's start by looking at the Beautiful Soup constructor.\\nBy default, the constructor will attempt to detect\\nwhat parser type you need based on the document\\nobject you pass.\\nLet's pick a parser for our constructor instead.\\nTo do that, we'll simply call the Beautiful Soup constructor\\nand we're going to pass in our_html_document.\\nThat's what we just created when we ran the block prior.\\n\\nAnd then for the second argument,\\nwe're going to tell Beautiful Soup exactly what type\\nof parser we want it to use to parse our data.\\nSince our data is html,\\nwe'll pass the html parser.\\nSo create a string that reads html.parser.\\nLet's set this whole thing equal to our_soup_object.\\nThis will be our soup object.\\nAnd then let's just print that out.\\n\\nSo we'll say print and then print(our_soup_object).\\nRun this.\\nOkay, so this is the output.\\nIt's all of our HTML.\\nBy default, the Beautiful Soup object\\nis a format of UTF eight,\\nwhich can be sort of difficult to read\\nbecause it doesn't have much formatting.\\nOne great way to make the output easier to read\\nis to prettify the soup object.\\nThe prettify method will turn a Beautiful Soup parse tree\\ninto a nicely formatted unicode string\\nwith each HTML or XML tag on its own line.\\n\\nLet's print out the first 300 characters of our soup object.\\nWe'll call the print function\\nand then we'll pass in our_soup_object\\nand then we'll call the prettify method off of that.\\nAnd we only want the first 300 characters,\\nso we'll just select [0:300]\\nand run this.\\n\\nGreat, so that's actually a lot easier to read\\nthan the output we got earlier.\\nIf you look back up here, it was kind of a big blob\\nand now at least we have some structure.\\nNow let's look at tag objects.\\nFirst, we'll create tag names.\\nSo let's create another Beautiful Soup object\\ncalled soup_object.\\nAnd to generate this object,\\nwe'll call the Beautiful Soup constructor.\\n\\nAnd let's just pass in a tag.\\nSo we'll create a string\\nand we'll say that this is h1 attribute_1\\nequal to heading level one.\\nSo this is actually heading level one tag\\nand we're saying that the attribute_1 = \\\"Heading Level 1\\\".\\n\\nAnd this heading should read\\n>Future Trends for IoT in 2018<.\\nAnd then we will close the </h1> tag.\\nAnd lastly, I want to pass in our html parser.\\nSo we'll say html.parser.\\nAnd let me check this index on this really quickly.\\n\\nShould be, okay.\\nSo now we have an h1 tag\\nand it's got an attribute of heading level one\\nand then it reads Future Trends for IoT in 2018.\\nNow what we need to do is go ahead\\nand create a tag variable.\\nWe'll call it say tag= soup_object.h1.\\nThis essentially tells Beautiful Soup\\nthat the tag's name is h1,\\na reference to the HTML we passed in.\\n\\nSo let's go ahead and print this whole thing out.\\nSo to do that, we'll call the type function\\nand we'll pass in our tag and hit run.\\nAnd so what you can see here is\\nthat our soup_object.h1 is actually a tag.\\nSo we named it tag,\\nbut when we call the type function,\\nit actually prints out here as a tag element.\\nAnd so we do indeed have a tag.\\nNow let's actually print out this tag\\nand see what it looks like.\\n\\nTo do that, we will call the print function\\nand we'll pass in our tag.\\nAnd as you can see, it returns a string that reads h1.\\nAnd that makes sense, right?\\nNow, let's see what happens when we call the tag name.\\nLet's say tag.name, print this out.\\nAnd of course it's also h1.\\nSo the name of our tag is actually h1.\\nAnd if you wanted to replace the tag name h1\\nwith heading one instead, you can do that.\\n\\nYou would just set the tag.name\\nand set that equal to heading one instead.\\nHere I'll show you.\\ntag.name = 'heading 1'.\\nPrint this out\\nand you can see that we've actually changed the tag name.\\nSo instead of it reading h1 as it does up here,\\nit actually reads heading 1.\\n\\nSo that's how you change the name of a tag.\\nLet's just also print this out really quick\\njust for clarity sake.\\nSo we'll say tag.name\\nand we get heading 1 and that's great,\\nwe changed the tag name.\\nNow let's look at tag attributes.\\nA tag can have any variety of attributes.\\nYou can access the tag's attributes\\nby treating the tag like a dictionary.\\nIn our example, the tag's attribute is attribute_1.\\n\\nSo let's just go ahead\\nand create a soup object, soup_object\\nand we'll set it equal to our BeautifulSoup constructor.\\nAnd then let's just take the tag that we created above,\\nI'm going to copy and paste it in.\\nWe need the quotes\\n'cause it should be a string.\\nAnd then we're going to use the same parameter of html.parser.\\n\\nAnd then let's create a tag variable.\\nSo we'll say tag is equal to soup_object.h1\\nand then we'll print this whole thing out.\\nSo we have our tag and it's been printed out here.\\nAnd imagine for example,\\nif you select attribute_1 from the tag object,\\nit returns to string that reads Heading Level 1.\\nThat is directly from the markup we passed into\\nBeautiful Soup constructor.\\nSo let's just try this out.\\n\\nWe'll say tag and then we'll select our attribute_1.\\nLet's see what we get back.\\nOkay, I missed a t here, attribute.\\nOkay, so fix that and then run this.\\nAnd cool, so that's called Heading Level 1.\\nThat's what prints out.\\nTo return a dictionary that contained\\nall of the tag attributes\\nyou'd simply call the attrs method.\\n\\nSo let's try that out here.\\nWe'll say tag.attrs.\\nAs you can see, this tag has only one attribute,\\nso you only get back one key value pair.\\nYou can easily add an attribute to a tag\\nby simply attaching an attribute labeled to a tag object.\\nLet's try that now.\\nSo we'll say tag\\nand then we'll select attribute_2\\nand we'll assign that a value of Heading Level 1,\\nwhich will be a string object, Heading Level 1\\nand I'll put an asterisk here.\\n\\nNow I have a little typo.\\nI got a dot that we don't need, so I'm going to remove that.\\nAnd then let's call the attrs method off of that tag.\\nRun this.\\nAnd then let's just print the tag.\\nSo those attributes both appear as part of the h1 tag.\\nPretty interesting.\\nWe can actually delete an attribute\\nfrom a tag object.\\nYou would just say del tag\\nand then select the attribute you want to delete.\\n\\nSo in this case, we'll delete attribute_2\\nand print this out again.\\nAnd now you can see attribute_2 is missing.\\nVery easy.\\nLet's also go ahead and just delete attribute_1.\\nI'm going to copy this and paste it.\\nAnd then just go ahead and change out the one for two.\\nRun both of these.\\nThen let's call attrs.\\nSo we'll say tag.attrs and run this.\\n\\nAnd now you can see we've deleted all of the attributes\\nand we get back an empty dictionary.\\nNow I want to show you how to navigate a parse tree\\nby using tags.\\nTo navigate a specific portion of the tree,\\nyou'd simply write the name of the tag you're interested in.\\nFirst things first though,\\nwe're going to start by importing our HTML document.\\nAnd this is the same document we used earlier\\nin your Jupyter Notebook.\\nIt's coming preloaded.\\nI'm just going to go back up to the top\\nand I'm going to copy this code block\\nand bring it down so we can reuse it.\\n\\nAnd let's create a soup object.\\nWe'll call it our_soup_object\\nand we'll set that equal to the BeautifulSoup constructor\\nand we'll pass in our_html_document\\nand an argument that reads html.parser.\\nGoing to run this.\\nNow to retrieve certain tags from within the parse tree,\\nall you need to do is write the name of the tag.\\n\\nSo if you want to pull up the title element\\nfrom the HTML document,\\nyou would just say our_soup_object.head,\\nso our_soup_object.head.\\nAnd this returns the head tag\\nthat contains the document title.\\nSo as you can see here, the document title is IoT Articles.\\nYou can also achieve the same outcome\\nby using the title tag.\\nSo let's just check that out.\\n\\nWe'll say our_soup_object.title,\\nrun that and get the same thing,\\nexcept that now we're missing the head tag, right?\\nBut we still get our article title.\\nIf you wanted to pull up this name of the article,\\nwell first let's look and see\\nwhat part of the tree that's actually located in.\\nSo let's go back up to the top here.\\nAnd it's held within the body tag,\\nbut there's a lot of other stuff in the body tag too.\\n\\nSo to narrow and further we can specify\\nthat it's within the body tag of the b tag.\\nSo here's the first b tag.\\nGo back down here and then just try\\nand access the title that way.\\nSo we'll say our_soup_object.body.b\\nand run this.\\nWell as you can see, the title of the article\\nis the same as what's been printed out.\\n\\nSo we were able to access it using tags.\\nJust to show you what we'd get if we were\\nto write in the b element,\\nI'll say our_soup_object.body.\\nAnd then what we're getting here\\nis the entire body of the HTML.\\nIt's a lot of text,\\nand so it's not really that useful\\nfor isolating portions of that text.\\nTo retrieve only the tags that are associated with lists,\\nyou could say our_soup_object.li.\\n\\nAnd then you get only the tags\\nthat are associated with lists.\\nAnd if you wanted to retrieve the first tag\\nthat contains a web link, you could say\\nour_soup_object.a and run this,\\nand then you would get our first link in the article,\\nwhich is a bit.ly link.\\nNow that we finished with tag objects,\\nlet's start looking at the NavigableStream object.\\nWe'll cover that in our next coding demonstration.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2715033\",\"duration\":660,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"NavigableString objects\",\"fileName\":\"3006708_en_US_07_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":928,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to work with objects. This video covers the NavigableString objects.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":26814035,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now that we've finished tag objects,\\nlet's start looking at the NavigatableString object.\\nBefore getting started here, I just want to check to make sure\\nthat our version of Python is comparable\\nwith the demonstration we're about to do.\\nSo this notebook was written for Python 3.10.\\nLet's just check the version we are running here.\\nSo we'll say import sys,\\nand then we'll print sys.version.\\n\\nAnd run this.\\nOops.\\nThis needs to be\\na new line here.\\nOkay, run this.\\nOkay, cool.\\nSo we have version 3.10, so we're good to go.\\nNext, let's just go ahead and import BeautifulSoup.\\nSo to do that, we'll say from BS4\\nimport\\nBeautifulSoup.\\n\\nRun this.\\nAnd now we have our BeautifulSoup library\\nimported into our IPython environment.\\nNow in this demonstration I'm going to show you how to work\\nwith NavigatableString objects.\\nNavigatableString objects are used as containers\\nfor chunks of text that are stored inside of tag objects.\\nSo let's just go back to our example from a previous lecture\\nwith our soup object,\\nand we'll create a variable here called soup object.\\nSoup_object.\\n\\nAnd we'll set it equal to\\nBeautifulSoup constructor.\\nAnd then within the constructor, we'll pass in an H1 tag.\\nAnd so to do that we'll create a string\\nand an open tag for an heading one element.\\nSo opening tag H1,\\nattribute_1 is equal to,\\nand then we'll set that equal to heading level one.\\n\\nLet's take that L out here,\\nand then close the string,\\nand then close this.\\nAnd we want this to read future trends for IoT\\nin 2018.\\nAnd then we'll close this H1 tag.\\nAnd then we need to be sure to pass our HTML parser\\njust to tell BeautifulSoup how to interpret\\nthe document we've passed in.\\n\\nIt does look like there's a typo,\\nso I'm missing a single quote here.\\nAnd then that should have cleaned that up.\\nSo, now let's create a tag,\\nand we'll set the tag equal to\\nour soup object\\nH1.\\nAnd then let's use the type function and pass in our tag.\\nAnd this is to verify that we actually indeed\\nhave created a tag object.\\n\\nSo I'll run this.\\nAs you can see,\\nwe have indeed created a tag object here.\\nSo let's verify the name of that tag.\\nIt should be H1,\\nbut let's double check by saying tag.name.\\nAnd yeah, indeed it is H1.\\nWe have an H1 tag,\\nbut if you just wanted to isolate the string object\\nfrom within this tag object,\\nthen what you could do is you could just say tag.string.\\n\\nSo tag.string and run this.\\nUh-oh, typo.\\nTag.string.\\nCool, so here we have it.\\nThat's our string within this tag.\\nIt's right here.\\nIt reads Future Trends for IoT in 2018.\\nI want to show you here how tag string is\\nactually a separate object of its own.\\nSo let's go ahead here and call the type function,\\nand then we'll pass in our tag.string.\\n\\nAnd run this.\\nAnd what you're seeing here is that the tag string\\nis actually a NavigatableString.\\nSo let's play with this a little bit.\\nI'll create a new variable,\\nand let's call that variable Our NavigatableString.\\nOur_Navigateable_String.\\nAnd we'll set it equal to our tag string.\\n\\nAnd then let's print it out.\\nSo basically what this is saying\\nis that our NavigatableString\\nis now this future trends in IoT in 2018.\\nIf you wanted to replace the string object\\nfrom within the NavigatableString,\\nyou can just call the replace\\nwith method off of the NavigatableString\\nand then pass in a replacement string.\\nSo let's try that out.\\nWe'll just replace this future trends for IoT in 2018\\nwith not a number.\\n\\nTo do that,\\nfirst, we'll pull up our NavigatableString object,\\nand then we'll call the replace with method.\\nAnd we'll pass in a string that reads NAN\\nfor not a number.\\nAnd then we'll print this out by saying tag string.\\nAnd as you can see, our string is\\nnow simply just not a number.\\nWe have replaced the future trends in IoT with not a number.\\n\\nOkay, so let's look at\\nhow we can utilize NavigatableStrings.\\nI'm giving you here this HTML document,\\nand we used this in a prior demonstration,\\nbut it's coming preloaded in your notebook,\\nso you don't have to try to type all of this out.\\nBut what we're going to do is we're going to convert this\\ninto a parse tree like we did in the previous section.\\nSo just to start things off, let's just run this cell.\\nWe'll go ahead and create our soup object,\\nand we'll set it equal to our BeautifulSoup constructor.\\n\\nAnd then we'll pass in our HTML document\\nand define our parser as the HTML.parser.\\nAnd then run this whole thing.\\nIf there's one or more string object within a parse tree,\\nyou can easily isolate them.\\nOne way to do this would be calling the\\nstripped string generator to return all of the strings\\nwithin the object where strings consisting entirely\\nof white spaces are ignored,\\nand white space at the beginning\\nand end of the string is removed.\\n\\nSo for this example,\\nfor each string object in the parse tree,\\nthis stripped strings generator passes through,\\nstrips white spaces,\\nand then prints out each string\\nthat contains a printable representation.\\nSo let's try this out here.\\nWe'll say for string\\nin our BeautifulSoup object,\\nour soup object.stripped_strings.\\n\\nWe want to print representation.\\nSo R-E-P-R.\\nAnd then of the string.\\nSo I'll pass in string.\\nThis is going to print a representation of the string,\\nand then we will run this.\\nOkay, so now you can see\\nthat our strings have been pretty much cleaned up.\\nWe just have a list of strings here without the tags\\nor any of the markup within the body\\nof the series of strings.\\n\\nThe last thing I want to show you in this demonstration\\nis how to access parent tag objects within a parse tree.\\nSo let's create a new object called First Link.\\nFirst_Link.\\nAnd then we'll set it equal to the A tag.\\nSo for our soup object,\\nwe'll reference the A tag.\\nOur soup object.A,\\nand then we'll call the print function\\nand pass in our first link.\\n\\nRun this.\\nUh-oh, this was changed to append\\nwith the autopopulater.\\nSo let's just fix that and run it again.\\nOkay, great.\\nSo this is actually the first link in our document\\nand the text that contains it.\\nSo in our document,\\nthis text here would actually be hyperlinked or clickable,\\nand it would redirect to a Bitly link.\\nIf we wanted to access the parent of that first link,\\nwe would just say, first link.parent.\\n\\nParent.\\nAnd we see that now we have the parent tag\\nof this first link.\\nNow, the NavigatableString object\\nof the first link is a string that reads\\nlast month Ericsson Digital invited me.\\nLet me show you.\\nWe'll say first link.string.\\nWe're going to take this string,\\nand we're basically going into this link\\nby pulling only the string from within it, right?\\nSo that is now printed out as the string object.\\n\\nAnd lastly, we can also retrieve the parent\\nof the NavigatableString object.\\nTo do that, we would say first link.parent.\\nSo in this case, the parent of the NavigatableString\\nis the A tag, which is sort of self-evident, right?\\nSo now you know how to work with objects in BeautifulSoup.\\nWe're going to get into using BeautifulSoup\\nfor data parsing.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4584143\",\"duration\":837,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Data parsing\",\"fileName\":\"3006708_en_US_07_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded, please mask the bottom deck from 00:00 - 10:13 in pickup\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1527,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to parse data. This video covers parsing data, getting data from a parse tree, and searching and retrieving data from a parse tree.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":34432705,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's look at working with parsed data\\nin beautiful soup.\\nI've broken the demonstration to come into three sections,\\nparsing data, getting data from a parse tree and searching\\nand retrieving data from a parse tree.\\nParsing data is where you'll pass an HTML\\nor XML document to a Beautiful Soup constructor.\\nThe constructor converts the document to Unicode\\nand then parses it with a built-in HTML parser.\\nWell, HTML parser, by default that is.\\n\\nLooking closer at searching and retrieving data.\\nI'm going to show you the find all method.\\nAnd this method searches a tag in its descendants\\nto retrieve tags or strings that match your filters.\\nThere are several methods for searching\\nand filtering a parse tree.\\nThe ones that I'm going to show you now are the name argument,\\nkeyword argument, string argument, lists, Boolean values,\\nstrings, and regular expressions.\\nYou can pass any of these arguments into the find all method\\nto use as filters and return either strings or tags.\\n\\nI'll show you in our demo.\\nData parsing is super simple with Beautiful Soup.\\nYou just pass in an HTML or XML document\\nto the Beautiful Soup constructor.\\nThe constructor converts the document to Unicode\\nand then parses it with a built-in HTML parser.\\nSo the first thing we need to do is just\\nto import our libraries.\\nAnd so your Jupyter notebook is coming preloaded\\nwith Beautiful Soup\\nand the urllib as well\\nas the regular expression library here.\\n\\nSo all you need to do is just run this code block\\nand you have your libraries available.\\nSo let's start by reading in some data.\\nWe're going to use the URL Lib library to do that.\\nSo we're going to say with\\nURL lib.request.URLopen,\\nand within this function, we're going to pass in a string\\nof a web address that has our HTML.\\n\\nSo that's https://Raw GitHub user content.com/bigdatagal/\\ndata-mania-demos/master/IOT2018.html.\\n\\nAnd then close the string.\\nSo again, that's https://raw GitHubusercontent.com/\\nbigdatagal/data-mania-demos/master/IOT- 2018html.\\nAnd we want this to be read as response\\nand we want HTML to be set equal\\nto the response read function.\\n\\nSo we'll say HTML equals response.read.\\nI need to HTML,\\ncheck the syntax really quick.\\nSo we need to have a colon here\\nand then I'm just going to bring this back up\\nand press enter so it's structured properly.\\nAnd then, okay, run this.\\n\\nSo now what we've done is we've loaded data into our\\nJupyter Notebook.\\nSo let's create a soup object, we'll call it soup,\\nand we'll say soup is equal to beautiful soup constructor.\\nAnd we'll pass in our HTML\\nand then we'll pass a perimeter that says it should be read\\nwith the HTML parser.\\nSo create a string that reads HTML.parser.\\n\\nThen let's just print the type out here.\\nSo we'll say type\\nand pass in our soup object and run this.\\nOkay, looks like I have a stray bracket here.\\nSo clean that up, run it again. And here we go.\\nAs you can see, we have created a Beautiful Soup object.\\nNow let's move into data parsing.\\nFirst thing's first, let's go ahead\\nand just prettify it so we can kind of get an idea\\nof what's in there and not have a bunch\\nof HTML that's totally unformatted.\\n\\nSo let's call the print function.\\nAnd we'll pass in soup.prettify and then let's just go\\nahead and print out only the first 100 characters.\\nSo we'll say zero:100\\nand run this.\\nAnd this is just a little preview of what we've got inside\\nof our soup object.\\nLet's practice getting data from a parse tree.\\nLet's start by getting the text out of our HTML\\nand isolating the text from within that HTML code.\\n\\nTo do that, let's just create an object called text_only\\nand we'll call the get text method off of our soup object.\\nSo we'll say soup.get_text.\\nAnd then print it out.\\nPrint our text only.\\nOkay, and then let me just check the syntax really quick.\\nWe've got a stray dot there,\\nso let's just get rid of that and then run this.\\n\\nAnd here's our output. That's nice and pretty easy to read.\\nNow I'm going to show you how to use this data\\nand basically search and retrieve data from a parse tree.\\nI've left some basic information\\nfor you within the notebook about searching\\nand retrieving data from a parse tree.\\nAnd we're going to be using the find all method.\\nThe find all method searches a tag in its descendants\\nto retrieve tags or strings that match your filters.\\nBasically, you can just pass any\\nof these arguments into the find all method to use\\nas filters and then return either strings or tags.\\n\\nSo the first thing I want to show you is how\\nto retrieve tags by filtering the name arguments\\nand name argument search for tags\\nby filtering based on the name tag.\\nSo let's practice really quickly.\\nWe'll just say soup\\nand we'll call the find all method off of that.\\nAnd then we want to pass in the tech LI.\\n\\nAnd run this.\\nHere's our output.\\nWhat this has done is it's gone through our parse tree\\nand it's basically filtered out all of the tags\\nthat have the name LI and printed only those.\\nSo you see we have a bunch of LI tags here.\\nYou can also retrieve tags by filtering\\nwith the keyword argument.\\nSo let's try that out real quick.\\nThis basically is going to search the parse tree for tags\\nby filtering based on tag attribute.\\n\\nSo what we need to do is call the find all method off\\nof our soup object,\\nand we'll go ahead and create a keyword filter.\\nWe'll call that ID equal to\\nlink seven.\\nSo we're basically saying we want all of the tags\\nthat have the identification link seven as a keyword.\\nSo when we run this,\\nwhat we're actually getting is only one tag, which happens\\nto be the one associated with the ID link seven.\\n\\nIt's a link to online courses for IOT.\\nYou can also retrieve tags by filtering\\nwith string arguments.\\nSo this is where you search for tags\\nby filtering based on exact stream.\\nSo again, let's call the find all method off\\nof our soup object.\\nAnd then this time, let's pass a string\\nthat reads OL and run this.\\nAnd so here what it's done is it's passed through our\\nparse tree and it's basically filtered out all of the code\\nthat fell within the OL tags.\\n\\nSo you can see here there's an opening tag here\\nand a closing tag here.\\nSo it's just passed through the entire document\\nand retrieved the tags within the OL tags.\\nNow let's retrieve tags by filtering with lists objects.\\nAgain, we will call the find all method\\noff of our soup object.\\n\\nAnd this time we'll pass in a list of tags.\\nThe first one will be the OL tag.\\nAnd then let's make the next one a B tag\\nand run this.\\nOkay.\\nYou can see now that we have filtered out from our\\nparse tree only the text from within the tags that fall\\nwithin the B tag and the OL tag.\\nYou can retrieve tags by filtering with regular expressions,\\nand what this function does is it searches for tags\\nand strings simply by filtering based on regular expression.\\n\\nSo let's go ahead and let's create a variable called T\\nand we'll say T is equal to\\nre, regular expression.compile,\\nAnd we'll pass in a string called T.\\nAnd then let's create a for loop.\\nWe'll say for tag in,\\nand then we'll call the find all method off\\nof the soup object.\\nSo we'll say soup.find all,\\nand we'll pass in again T.\\n\\nFor each of these tags, we want to print the tag name.\\nSo we'll call the print function\\nand pass in tag name.\\nTag.name,\\nrun this.\\nAnd so now we have retrieve tags by filtering\\nwith regular expression here, which was T.\\nYou can also retrieve tags by filtering\\nwith a Boolean value.\\nSo let's just practice that real quick.\\n\\nI'm going to copy this code that we just wrote here\\nso we can reuse it.\\nAnd then in this case, what we want to do is we want\\nto filter based on the Boolean value true here.\\nSo we're going to change that T to true.\\nAnd then just run this again.\\nAnd here's our output.\\nThat's basically how you filter with Boolean values.\\nYou can retrieve web links by filtering with string objects.\\n\\nLet me show you how to do that real quick.\\nSo for this, let's just create a new for loop\\nand we'll say for link in\\nand all the find all method, off of the soup object.\\nAnd then we'll pass in an A, a string that reads A.\\nBasically what we're doing here is we are retrieving all\\nof the links that are associated with an A tag.\\nFor each of those, we want to print them out.\\n\\nSo we'll call the print function\\nand then we'll say link.get,\\nand then let's just ask it to get the Href,\\nand run this.\\nOkay, here we have a list of links.\\nSo basically what this has done is it's gone\\nthrough our entire soup object and isolated only the links\\nand printed them out in a list.\\nYou can also retrieve strings by filtering\\nwith regular expression.\\n\\nSo let me show you how to do that really quick.\\nAgain, we're going to use the find all methods.\\nSo we'll say soup.find all,\\nand this time we wanted to find where string is equal to.\\nAnd then we want to call the compile function\\nfrom regular expression.\\nSo that's re.compile,\\nand then we'll pass in a string\\nthat reads data and run this.\\n\\nOkay, so that's a bit long as you can see here,\\nbut what it's actually done is it's filtered our parse three\\nby the regular expression data.\\nNow that you've made it this far,\\nyou've basically covered all of the mechanics\\nof scraping web data with Beautiful Soup.\\nSo next I'm going to show you how\\nto use this stuff in action.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4590006\",\"duration\":792,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Web scraping in practice\",\"fileName\":\"3006708_en_US_07_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1615,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to scrape web data. This video covers reading and writing data from the internet.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":32738581,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let me show you web scraping and action.\\nIn the following demonstration,\\nI'm going to show you how to scrape a webpage\\nand then save your results to an external file.\\nLet's get started.\\nSo your Jupyter Notebook is coming\\nwith the standard libraries for beautiful soup\\nalready loaded in it, and you've also got URL Lib,\\nand regular expression library preloaded.\\nSo all you need to do is run this\\nand what we're going to do is we're going to scrape\\na page from analytics.usa.gov.\\n\\nSo to start, we're going to create an R variable,\\nand we're going to say R is equal to urllib.request.urlopen,\\nand then we're going to pass in a string\\nthat is the URL we want to be having data read from.\\nSo that's going to be https://analytics.usa.gov.\\n\\nAnd then we're going to call the read method off\\nof this whole thing.\\nAnd next, we're going to create a soup variable,\\nand we're going to set it equal\\nto the beautiful soup constructor.\\nAnd then we want to pass,\\nour first perimeter is going to be our R variable.\\nAnd then our second parameter will be the HTML parser\\nto tell Beautiful Soup to use the HTML parser\\nto read the data.\\n\\nAnd we'll call the type function\\nand pass in our soup object just to double check\\nthe type of our soup object,\\nwhich of course, should be a beautiful soup.\\nSo I'll run this and yeah, there we have it.\\nIt's a beautiful soup object.\\nSo just remember here that you can use any web link you want\\nto basically scrap data from any webpage on the internet.\\nNow, I'm going to show you how to script a webpage\\nand save your results.\\n\\nFirst, let's start by printing out our soup object.\\nSo we're going to call the print function,\\npass in soup.prettify.\\nAnd then let's just take a look\\nat the first 100 characters.\\nI'll run this and okay,\\nso we're seeing the very tip top of the webpage\\nthat's located at the analytics.us.gov url.\\nNext, what we should do is just use the find\\nall function to find all of the A tags\\nand then retrieve from within them\\nthe A tag values from within those.\\n\\nTo do that, we're going to create a loop.\\nSo it'll be a for loop\\nand for each link in soup.find all,\\nwe're looking for the A tags.\\nAnd then we want to print, oh,\\nlooks like I forgot a colon here at the end of this.\\nOkay, so then for each of those,\\nwe want to print the link and then we need to get the A.\\n\\nSo that's going to be link.get.\\nAnd within this get method, we're going to pass a perimeter\\nthat reads href and run this.\\nOkay, cool. So now we have a list of links.\\nAnd what this has done is it's actually gone\\nthrough the analytics.usa.gov webpage,\\nand it's looped through all of the text on that page\\nand basically printed out only the web links.\\n\\nBut if you wanted to see what that entire body\\nof text actually looks like,\\nthen you can use the get text method.\\nSo let's just try that out really quickly.\\nWe're going to say print, and we'll pass in our soup object.\\nAnd off of that, we will call the get Text method\\nand run this.\\nOkay, so there is a lot of content.\\nSo what I'm going to do is just click this option\\nas view it as a scrollable element.\\n\\nAnd here is the body of text that's sitting on that webpage.\\nWe have scraped it directly from the webpage in real-time.\\nThe benefit of using this for loop\\nthat we created here is it basically went through all\\nof the text and got us exactly what we needed,\\nwhich was the links instead of us having to kind of pick\\nand choose through the body of text.\\nAnd in this case, I'm not even seeing the hyperlinks.\\n\\nSo if you just want to scrape links from a webpage,\\nthen you might as well use this method up here.\\nLet's go ahead and just prettify this,\\nso we can kind of take a look at it more easily.\\nTo do that, we'll call the print function,\\nwe'll pass in our soup object, soup.prettify\\nand then let's just look at the first 1,000 characters.\\n\\nSo we'll, 0:1000 and run this.\\nOkay, so here, we're seeing the links printed out,\\nand it's a lot more manageable to read,\\nand it's a lot easier to read than the output from above.\\nNow, what I want to do is create a for loop\\nto pass through our soup object and find all of the A tags\\nthat have an attribute of href.\\nSo we'll say for link in soup.find all.\\n\\nAnd the first parameter will be a string\\nthat reads A with attribute equal to href.\\nSo we're going to say attris equal to,\\nand then we're going to create a dictionary,\\nand we're going to pass a string that reads href.\\nAnd then for all of these tags that are returned,\\nwe want the loop to match against them irregular expression\\nthat reads HTTPS and print out only those.\\n\\nSo to make that happen,\\nwe're going to call the compile function\\nfrom the regular expression library, so that's re.compile.\\nAnd within that,\\nwe're going to pass a string that reads HTTP.\\nAnd then for any results that match this expression,\\nwe just print them out.\\nSo we're going to call the print function and pass in our link.\\nAll right, and I'm going to check the syntax here\\n'cause it looks like something is off.\\n\\nYeah, I am missing the closing parentheses here\\nto close out the tuple.\\nOkay, so I'm going to run this,\\nand that's a long list of tags\\nwith links within them.\\nLet's just take a look at what the data type is here.\\nSo to do that, we'll say,\\ntype and pass in our link object.\\nSo as you can see, this is actually a tag object,\\nand what we basically have is we have all of our A tags\\nthat have an attribute of href\\nand also have an HTTP batch within them.\\n\\nIt isn't useful for you\\nto have this result stuck within a Jupyter Notebook though,\\nso you'll want to know how to actually save this\\nas an external file.\\nTo do that, we're going to create a new text file\\ncalled parsed data.\\nAnd so, we're going to create a file variable,\\nand say, file is equal to,\\nand then we'll call the open function.\\nWe'll pass in a string\\nthat reads parsed_data.text,\\nand the second parameter\\nwill be a string that reads W.\\n\\nWhat that's doing is it's telling Python\\nthat we want to write into that text file,\\nso W stands for write.\\nAnd then what we want to do is for each of the links\\nthat was just printed out,\\nwe want to print those now into the parsed data text file.\\nSo now, what we actually need to do is\\nthat we can just go ahead\\nand copy this code from above of our for loop.\\nWe'll copy this and then we'll just reuse it\\nfor efficiency's sake.\\n\\nI'm going to paste it here,\\nand it's just performing\\nthe same operation now as it did before.\\nBut instead of printing out here into Jupyter,\\nit's going to go ahead,\\nand it's going to generate a soup link,\\nand that's going to be a string.\\nSo instead of printing the link,\\nit's going to create a new variable called soup link.\\nAnd that link is going to be equal to a string.\\nAnd this string is going to be derived\\nfrom each link within the soup object.\\nSo what I'm going to do here is\\nthat I'm going to move this down,\\nand I'm going to say soup_link is equal to str\\nand then I'm going to pass in a link there.\\n\\nAnd then for each soup link, we want to print that out.\\nSo we need to update this print function,\\nso that it's printing soup_link.\\nAnd then we're going to write into the file.\\nSo we're going to say file.write,\\nand we're going to pass in our soup link, soup_link.\\nAnd what this loop is going to do is that it's going\\nto pass it the entire beautiful soup object,\\nand it's going to find all of the links\\nand print them out until it finds no more links,\\nand then it's going to flush the file and close the file.\\n\\nSo to make that happen, we say file.flush,\\nfile.flush,\\nand file.close.\\nAnd let me just check really quick\\nfor any issues with the syntax.\\nOkay, we'll run this and great.\\nOkay, so we have a list of tags\\nwith the links inside of them.\\n\\nSo essentially, this is\\nwhat should be written into our text file, correct?\\nSo let's check that.\\nI could show you the shortcut of where\\nto find the file, but I also want to show you how\\nto use the present working directory command\\nto show you where to go to retrieve that text file.\\nSo you need to pull up your present working directory,\\nand if you just call the command, %pwd and run this,\\nit's going to tell you exactly where you can go\\nto find the text file that was just printed.\\n\\nSo it's in our folder\\nthat we're actually working within in code spaces.\\nThe extension is printed out here for us.\\nAnd then of course, the shortcut is you\\ncould just go up to this explorer,\\nand since we're working in the notebooks folder,\\nyou can actually just find the file written here.\\nSo let's look at that and then you can see,\\nokay, here are our links.\\nIt looks like they're not quite as nicely formatted,\\nbut it's the same information\\nthat has been printed out in the Jupyter Notebook,\\nexcept for now it's a text file,\\nwhich can, in times, be more convenient.\\n\\nThe only thing I would mention here is\\nthat you can still see a bunch of stray tags.\\nAnd a lot of times when you're doing web scraping,\\nno matter how much data formatting you do,\\nthere's always these stray characters.\\nBasically, a lot of times,\\nthere are data processing requirements\\nafter you scrape the data.\\nSo expect to spend some time data munging\\nafter you do web scraping.\\nBut if you ever find yourself again in a position\\nwhere you can't get data from a website\\nbecause it's been placed on different pages\\nor in weird formatting, remember how to use beautiful soup\\nto scrape the data for you.\\n\\nAnd in the next lecture,\\nyou're actually going to learn how\\nto build a whole web scraping application\\nthat will print out links for you in such a way\\nthat they're perfectly formatted.\\nAnd so, stay tuned because that's next.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4586145\",\"duration\":1236,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Asynchronous scraping\",\"fileName\":\"3006708_en_US_07_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":1985,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about writing parallelized scrapers with asyncio.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":43183549,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's look at asynchronous web scraping\\nand how we can use this method to\\nspeed up the process of extracting data.\\nA normal web scraper can take a lot of time\\nto extract large amounts of data,\\nor in some cases, it might just fail altogether.\\nThis usually happens when a web scraper makes a request\\nand then waits for the response\\nbefore jumping to the next request.\\nDuring this time,\\nthe CPU remains idle,\\nwhich wastes a lot of CPU time.\\nCircumventing this process,\\nasynchronous web scraping allows us\\nto make multiple requests simultaneously\\nwithout waiting for the response of each request.\\n\\nIt also reduces CPU idle time.\\nIn the demonstration of asynchronous scraping\\nthat's coming up,\\nwe will use the aiohttp and the asyncio libraries.\\nAiohttp is a client and server side library\\nthat allows us to make asynchronous requests.\\nOn the other hand,\\nasyncio is a Python library,\\nwhich is used to write concurrent code.\\nLet's take a look at it inside of Python.\\n\\nFor this demonstration,\\nwe're going to be working with new libraries.\\nOne is aiohttp, and the other is asyncio.\\nSo just to be careful,\\nlet's go ahead and do a pip install of each of those\\nto make sure you've got them installed in your environment.\\nSo we'll start with pip install aiohttp,\\nand run this.\\n\\nAnd then let's do pip,\\npip install asyncio,\\nand run this.\\nOkay, great.\\nNow, your notebook is already coming preloaded\\nwith the libraries that you'll need.\\nAnd we'll mainly be working with aiohttp and asyncio,\\nbut also with BeautifulSoup, CSV,\\nand the regular expression library.\\n\\nAll you need to do to import those\\nis just to run that code block there.\\nAnd since we're working within a Jupyter Notebook,\\nthere's already an event loop that's running on the backend.\\nSo we cannot start a new event loop.\\nFor this reason,\\nwe need to run nest asyncio.\\nWhat this module does is that it patches asyncio\\nto allow a nested use of asyncio.\\nSo first we'll do a pip install.\\n\\nAnd we want to do a pip install of nest-asyncio.\\nand then let's import nest_asyncio,\\nand we'll call it Supply Function.\\nSo to do that,\\nwe'll say nest_asyncio.apply,\\nand we'll run this.\\n\\nNext, let's write an asynchronous python function\\nthat scrapes all of the links\\nfrom a given webpage's HTML content\\nand saves them into a CSV file.\\nThis can be especially useful\\nfor gathering data for web analysis, SEO monitoring,\\nor even just cataloging content.\\nSo we'll begin by defining our asynchronous function,\\nwhich we will call scrape and save links.\\nWe'll use the async keyword\\nto define an asynchronous function.\\n\\nSo we'll say async def scrape_and_save_links(text):.\\nAsynchronous functions are part\\nof asynchronous programming\\nand they allow us to handle long waiting operations\\nlike network requests more efficiently.\\nThey let other parts of your program run\\nwhile waiting for these operations to complete.\\n\\nInside the function,\\nthe first thing we'll do is parse the HTML content\\nwith BeautifulSoup.\\nSo we'll say soup equal to,\\ncall the BeautifulSoup constructor.\\nWe'll pass in our text object\\nand we'll define the parser as HTML.parser.\\nNow, we want to save these links to a file.\\nTo do this, we'll open a file in append mode.\\nIf the file doesn't exist, it will be created.\\n\\nIf it does exist, we'll add to the end of it.\\nWe don't want to add any unintended new lines.\\nSo we'll set new line equal to a blank string,\\nfile equal to open,\\nand then we will pass a string that reads CSV_file.\\nThe next parameter will be a string that reads A.\\nAnd then lastly,\\nour new line needs to be equal to an empty string.\\n\\nWith the file opened,\\nwe create a CSV writer object.\\nThis object is responsible for converting our links\\ninto a format that's suitable for a CSV file,\\nwhich by convention uses commas to separate items.\\nSo here we'll say, let me move this up a little.\\nOkay, writer equal to CSV.writer.\\n\\nAnd we pass in our file\\nand we just set our delimiter equal to a string\\nthat's got a comma in it.\\nNext we'll loop through all the A tags in our Soup object.\\nThe final method looks for these tags\\nand with the attrs parameter,\\nwe specify a regular expression to match the href attributes\\nthat start with HTTP.\\nThis way, we're only getting actual web links.\\nSo here we'll say for link in Soup.findall\\nwe'll pass in our A tag\\nand then we'll say attrs equal to dictionary\\nthat contains href.\\n\\nAnd then we'll pass our\\nregular expressions compile function.\\nSo re.compile,\\nand we'll ask it to look for HTTP.\\nThe colon at the end of this.\\nEach link we find is extracted using link.get('href').\\nEach link we find is extracted using link.get('href').\\nAnd we use our CSV writer to write this link into our file.\\n\\nSo we'll say link is equal to link.get('href')\\nand then writer.writerow\\nand we'll pass in our link.\\nAnd lastly, we need to close the file.\\nFailing to close the file could lead\\nto data not being written correctly\\nor the file being left open unnecessarily,\\nwhich is a resource link.\\n\\nSo to close the file, we will write file.close,\\nand then let's just run this whole thing.\\nOkay, so here we have our function.\\nNext we're going to define another piece\\nof our web scraping toolkit,\\nwhich will be the fetch function.\\nThis asynchronous functions responsible\\nfor making web request\\nand then passing the content it retrieves\\nto our scrape and save links function.\\nLet's start by defining our function with the async keyword.\\n\\nThis indicates that it's an asynchronous function\\nand it allows us to use the await inside the function,\\nwhich is essential for performing\\nasynchronous IO operations.\\nSo we'll call the fetch function\\nand we want to fetch our session\\nand the URL.\\nNow, we'll enter a try block.\\nThis is where we'll perform our web request.\\nThe reason we use a try block is\\nbecause network operations are unpredictable.\\n\\nThere might be connectivity issues,\\nthe server might not respond,\\nor there could be a myriad of other issues that could arise.\\nSo we'll say try...\\nInside the try block,\\nto send a get request to the URL\\nwe passed into the function.\\nThe session.get method is an asynchronous method,\\nso we'll use the async with statement.\\nWe'll say async with session.get(url) as response.\\n\\nOne thing I want to point out here is\\nthat the async with statement ensures\\nthat the session is properly closed\\nafter we're done with it even if an error does occur.\\nOnce we have the response from our get request,\\nwe want to retrieve the text of that page.\\nWe do this with await response.text.\\nThe await keyword is used to wait for the operation\\nto complete without blocking the entire program.\\nSo we will say text is equal to await response.text.\\n\\nWith the text of the response in hand,\\nwe now want to scrape the links from it.\\nHere we use the asyncio create task\\nto kick up our scrape and save links function.\\nThis function call creates a new task that runs concurrently\\nwith other tasks including the main program.\\nSo we'll say task is equal to asyncio.create_task,\\nand then scrape and save links(text).\\n\\nWe don't want to move on until\\nwe've actually scraped and saved the links,\\nso we await the task to ensure\\nit completes before proceeding.\\nThis is a key point,\\neven though we're doing things concurrently,\\nsometimes we need to wait for one task to finish\\nbefore starting another.\\nSo we'll say await task here.\\nAnd lastly, we have an except block.\\nThis is our safety net.\\n\\nIf anything goes wrong with a network request\\nor the scraping, instead of crashing our program,\\nwe catch the exception and print out the error message.\\nThis is just a general best practice for debugging\\nand ensuring the robustness of your program.\\nSo we'll say except exception as e:\\nprint(str) passing e,\\nand that's our fetch function.\\n\\nOh!\\nOkay. Looks like there's a syntax error.\\nSo let me take this async in and move it over.\\nMove this line over this line.\\nTab it over, tab await over.\\nOkay.\\nJust clean up the indentations a bit.\\n\\nOkay. Yeah.\\nSo it was just a matter of cleaning up the indentations\\nand that's our fetch function.\\nIt's designed to handle web request asynchronously,\\nscrape the content for links\\nand manage errors gracefully.\\nWith asyncio, this function will work efficiently\\nas part of an asynchronous python application,\\nfetching data and processing it\\nwithout blocking other operations.\\nNow we're ready to write a function\\nthat orchestrates the whole web scraping operation.\\n\\nWe'll name this function scrape,\\nand its job will be to manage multiple URLs\\nand ensure that we fetch and process them concurrently.\\nThis is where the true power\\nof asynchronous programming lies\\nhandling multiple IO bound tasks at once\\nwithout waiting unnecessarily for each one to complete\\nbefore starting the next.\\nHere's how we do it.\\nSo we say async def scrape,\\nwhich will accept a list of URLs to process.\\n\\nWithin this function,\\nwe'll initiate an empty list named tasks.\\nThis list will store the future tasks that we will create\\nand then execute concurrently.\\nEach task will be a web scraping operation for a single URL.\\nSo here we'll say tasks equal to,\\nand we'll just put an empty list.\\nNext we set up an asynchronous context manager using\\naiohttp client session.\\n\\nAiohttp is an asynchronous HTTP client for Python,\\nwhich allows us to make multiple HTTP requests concurrently\\nby using async with, we ensure that the session is closed\\nautomatically once all operations\\nwithin the block are completed.\\nSo we'll say async with aiohttp.ClientSession\\nas session\\nand colon.\\n\\nNow, we loop over each URL in the URL's list.\\nFor each URL we call the previously defined fetch function,\\nwhich fetches the URL's content and processes it.\\nWe append the resulting task, a coroutine object\\nto our tasks list.\\nSo we'll say for URL in URLs,\\ntasks.append(fetch(session,url)).\\n\\nAfter we have iterated through all the URLs\\nand created a task for each, we use asyncio.gather\\nto run all of these tasks concurrently.\\nAsyncio.gather takes a list of coroutines\\nand schedules them to run concurrently\\nby prefixing tasks with an asterisk.\\nWe're unpacking the list so\\nthat gather receives individual tasks as arguments.\\nSo we will say await asyncio.gather,\\nand then we'll pass in an asterisk and then tasks.\\n\\nNow let's make sure our indentation is correct here.\\nSo this should actually be moved up one.\\nOther than that, we need to also add a colon here.\\nAnd then it looks pretty good, so I'll run it.\\nOkay, no problems.\\nOne thing I want to point out here is\\nthat the await keyword is crucial.\\nIt means that the scrape function will wait\\nuntil all the fetch tasks have been completed.\\n\\nEach fetch task involves sending a request to a URL,\\ngetting the response and then passing that response\\nto scrape and save links,\\nwhich saves the links into a CSV file.\\nOnce await asyncio.gather tasks completes,\\nwe know that all the URLs have been processed\\nand the links have been saved.\\nSo here we are at the concluding portion\\nof our web scraping session.\\nUp to this point,\\nwe've built all of the individual components we need\\nfor our web scraping application.\\n\\nWe have the scrape and save links function\\nto extract links from the HTML content\\nand save them to a CSV file.\\nThe fetch function to get the HTML content from our URL\\nand the scrape function to manage all\\nof our fetch calls concurrently.\\nNow it's time to use them.\\nLet's create a object called URLs\\nand we're going to set it equal to a list of URLs.\\nLet's make the first URL,\\nhttps://analytics.usa.gov.\\n\\nAnd then our second URL will be\\npython.org.\\nSo we'll say https://www.python.org.\\nAnd lastly about we use LinkedIn,\\nso https://www.linkedin.com.\\n\\nI'm going to do a forward slash just to make sure\\nall of our ducks in a row here, clean up the formatting.\\nAnd the next line is where we actually start\\nour scraping operation.\\nSo we'll say asyncio.run(scrape),\\nwhere URLs is equal to URLs.\\n\\nThose are the URLs we wrote into the list above.\\nAnd run this.\\nOkay, it looks like I missed the N here.\\nIt should be asyncio.\\nOkay, run this.\\nSo it looks like it's found some objects\\nthat are not callable,\\nbut we should still have\\nall of the links saved in the CSV file.\\nSo let's go ahead and go into our explorer here\\nand look for a CSV file.\\n\\nOkay, so it looks like we have a problem\\nwith one of our functions.\\nSo what I'm going to do is I'm going to go back up\\nand just check this syntax really quickly.\\nAnd what I can see here is that\\nthis indentation needs to be moved out.\\nAnd then also this A needs to be changed to a capital A.\\nAnd then we will run this code block again,\\nand okay, fix the problem.\\n\\nAnd now all of these links from these webpages\\nhave been scraped asynchronously.\\nThey will be saved in the CSV file.\\nSo we can look over here in the explorer section\\nand you'll see we have a CSV file.\\nAnd here is the CSV file with\\nall of the links from each of the three pages\\nthat we referenced in our list.\\n\"}],\"name\":\"7. Data Sourcing via Web Scraping\",\"size\":201619135,\"urn\":\"urn:li:learningContentChapter:4579304\"},{\"duration\":2756,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4584144\",\"duration\":314,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introduction to Streamlit\",\"fileName\":\"3006708_en_US_08_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":401,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Get introduced to the spectrum of dashboarding utilities for Python, including Streamlit, Voila, Panel, and Dash. Learn why Streamlit is preferable.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7107321,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Streamlit, Dash, Voila, and Panel\\nare the leading frameworks\\nwithin the Python dashboarding ecosystem.\\nLet's discuss the features of these frameworks.\\nI'll state a design assumption.\\nThen we'll decide which framework\\nis best for that assumption.\\nStreamlit is a Python library\\nthat's specifically built for machine learning engineers\\nand data scientists.\\nStreamlit makes it easy to create and share\\nbeautiful custom web apps\\nfor machine learning and data science projects\\neven if you don't have any prior knowledge\\nof web development.\\n\\nin the matter of a few minutes,\\nyou can use it to build and deploy powerful\\ndata applications.\\nSome of the primary features of this framework\\nare it only supports Python language.\\nIt supports all of the Python plotting libraries\\nlike Matplotlib, Seaborn, Plotly, what have you.\\nAnother nice thing about Streamlit\\nis it's an open source framework.\\nWe do not need any web development knowledge\\nto build a web application using Streamlit,\\nand it's also incredibly easy to use and manage,\\nrequiring very, very little bit of code.\\n\\nIt runs on its own server\\nand it doesn't support Jupyter Notebook,\\nbut it can be deployed on most of the deployment servers.\\nNext, there's Panel.\\nPanel is another Python library\\nthat lets you create custom,\\ninteractive web apps and dashboards\\nby connecting user-defined widgets\\nto plots, images, tables, or text.\\nThe best thing about Panel\\nis that you can build a simple dashboard application\\nfor a complicated system in Jupyter Notebook,\\nand you don't need to switch tools along the way.\\n\\nHere are some of the main features of Panel.\\nJust like Streamlit, Panel also supports Python language.\\nIt supports all of the main Python Ploting libraries\\nlike Matplotlib, Seaborn, and others.\\nIt's an open source framework\\nand it can be used to create multi-page web applications.\\nA nice thing about Panel\\nis we don't need any web development knowledge\\nto create web applications using it.\\nPanel offers amazing design flexibility,\\nand you can use it to create your templates as well.\\n\\nIt supports Jupyter Notebooks,\\nand with it you can create an end-to-end application\\nwithin a notebook environment.\\nLastly, Panel can easily be deployed\\nto most deployment platforms.\\nNext, there's Voila.\\nVoila is a tool that lets you turn any Jupyter Notebook\\ninto a standalone web application.\\nIt allows you to create interactive web pages,\\nand it has great support for widgets, such as IPyWidgets.\\n\\nHere are some of the more prominent features\\nof Voila.\\nIt supports multiple languages, such as Python, C++,\\nand Julia.\\nIt also supports most of the Python plotting libraries.\\nIt's an open source framework\\nand you don't need any web development knowledge\\nto create web applications using Voila.\\nVoila makes it really easy to create basic dashboards,\\nbut unfortunately it has limited design flexibility.\\nIt only offers a few templates,\\nbut it does give you an option to create your own templates.\\n\\nThe nice thing about Voila\\nis it has exceptional support for Jupyter Notebooks.\\nLastly, Voila can be easily deployed\\nto most of the deployment servers.\\nAnd last but not least, there's Dash.\\nDash is a web application development framework from Plotly.\\nIt's built specifically for developing data applications.\\nDash offers users a very easy way\\nof developing dynamic dashboards.\\nSome of its main features are that it supports\\nPython, R, and Julia languages.\\n\\nDash is primarily built for using\\nwith Python's Plotly Library.\\nIt explicitly supports multi-page web applications,\\nand it's an open source framework.\\nThat said, basic knowledge of HTML is needed\\nfor developing a web application.\\nIt's really easy to create a basic dashboard application\\nusing Dash and Dash offers incredible design flexibility.\\nUnfortunately, though it doesn't provide any support\\nfor Jupyter Notebooks.\\n\\nOne more thing about Dash\\nis that it's got plenty of deployment options\\nand can be deployed on most of the deployment servers.\\nNow, let's assume we need to develop a dashboard\\nfor a non-technical audience.\\nIn this case, Streamlit would be an ideal\\ndata dashboarding solution simply because\\nit's more simple and structured than other options.\\nIf you're looking for a more mature\\ndata dashboarding solution\\nand your primary goal is to develop dashboards\\nfor non-technical users,\\nthen Streamlit should be your choice.\\n\\nMoving forward, we'll get started working with Streamlit\\nand exploring its features.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4588030\",\"duration\":168,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Environment setup\",\"fileName\":\"3006708_en_US_08_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"1 pickup recorded\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":true,\"graphicsIncluded\":true,\"rawDurationSeconds\":261,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Get a basic introduction to Streamlit and how to import the required packages.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5665410,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] You know, most Streamlit developers claim\\nthat it's really easy to set up Streamlit\\nand that you can use it\\nto build a web application in minutes using Python.\\nHow about let's test out that claim to see if it's true.\\nWe'll start by setting up Streamlit,\\nand once we have that up and running,\\nwe'll explore some of its more prominent features.\\nStarting first with the installation process,\\nStreamlit works fine with Anaconda,\\nso we'll install it in an Anaconda environment.\\n\\nTo install Streamlit, we'll need the following commands.\\nWe'll need pip install streamlit, streamlit hello,\\nand pip install upgrade protobuf.\\nLet's open the terminal.\\nIn this demonstration,\\nwe're going to be working inside the terminal here.\\nSo for building Streamlit applications,\\nyou always need to do a pip install\\nof any libraries you'll need when building them.\\nAnd so in this case, we're just working with Streamlit.\\nSo we would say, pip install streamlit.\\n\\nRun this.\\nAnd then once we have that installed,\\nwhat we need to do is we need to import it\\ninto our environment.\\nSo we're going to say import streamlit as st.\\nAnd what we're actually doing here is we're building\\na very, very simple Streamlit application\\nthat just writes the phrase Hello World onto a webpage.\\nSo in order to do that, we're going to call the right function\\nand pass in a string that reads Hello World.\\n\\nSo we'll say st dot write.\\nAnd then Hello World.\\nOkay, and so we should be good to go.\\nIt's a very simple routine.\\nAnd then to run this, what we need to do\\nis we need to say streamlit,\\nspace run, space,\\nand then we need to copy the file path\\nof the Python file we're working in.\\nSo in this case, it's 0802B.\\n\\nSo I open up Explorer, I right click on the file name,\\nI copy path, and then I can just paste it down here.\\nOkay, so Chrome is asking me\\nif that's okay with me to paste.\\nAnd so I say allow.\\nAnd then I hit Enter to run.\\nAnd then here you click this button\\nto open it up in a browser.\\nAnd here is your very first Streamlit application\\nthat reads, Hello World.\\n\\nCongratulations.\\nNext I'm going to show you how to use Streamlit\\nto start building charts and visualizations.\\n\"},{\"urn\":\"urn:li:learningContentVideo:4590007\",\"duration\":546,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Create basic charts\",\"fileName\":\"3006708_en_US_08_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":966,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to create basic charts in Streamlit, including a simple line chart, bar chart, and pie chart.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":19553621,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Okay,\\nwe're going to be using pandas, NumPy, Streamlit,\\nand Matplotlib.\\nAnd as with any Streamlit coding demo inside of Codespaces,\\nwe need to do a pip install of the required libraries.\\nSo let me just do that real quick.\\nDo pip install pandas.\\nSo the next thing to do is pip install numpy.\\n\\nAnd then we need to pip install streamlit.\\nAnd lastly, pip install matplotlib.\\nOkay, so now, we should have all of our libraries\\navailable inside of our environment.\\nSo the next thing we need to do\\nis just to import them.\\nSo, we'll start off by saying\\n\\\"import pandas as pd,\\\"\\nand then, \\\"import numpy as np,\\\"\\n\\\"import streamlit as st,\\\"\\nand \\\"import matplotlib,\\\" and we need the pyplot module,\\nso we'll say \\\"matplotlib.pyplot as plt.\\\"\\nNext, we'll create a dataframe with random numbers,\\nand set the column names.\\n\\nOkay, so I'm going to take this terminal here,\\nand just minimize it,\\nso we have more working space.\\nOkay, so first we'll define the column names.\\nWe'll do call_names equal to a list.\\nThe first column name will be called \\\"column1,\\\"\\nand then the next will be column2,\\nand the third is, of course, column3.\\n\\nAnd now let's create a DataFrame called \\\"data.\\\"\\nAnd we'll set data equal to the DataFrame constructor.\\nAnd within it, let's pass the random number generator.\\nAnd we'll create a numpy array with size 30 rows\\nand three columns, containing random numbers.\\nAnd after that, we'll set the column names.\\nSo for the random number generator,\\nlet's use np.random.randint, pass in a 30,\\nwhich basically says that we want to have 30 datapoints.\\n\\nAnd then let's define the size of our DataFrame here,\\nso again, we want 30 rows and three columns,\\nso we'll just say \\\"size=,\\\"\\nand then pass in 30 and three.\\nAnd then lastly, we need to define the column names.\\nSo to do that, we'll just say \\\"columns=col_names.\\\"\\nOkay, so I have a typo there, where that needs to be names.\\n\\nThis is the part\\nwhere we're going to now create the line chart.\\nFor this, we'll use Streamlit's line chart function.\\nSo that's st.line_chart,\\nand we'll pass in our DataFrame data.\\nAnd above the chart, let's print chart name,\\nwhich will be \\\"line graph.\\\"\\nSo we just create a string here.\\nAbove the chart, we write a string called \\\"line graph:\\\"\\nwith a colon at the end.\\n\\nLet's run this code and visualize the line graph.\\nSo to do that, we need to go down to our terminal here.\\nI'm going to bring it back up so we can see a little better.\\nAnd we're going to say, \\\"streamlit run,\\\"\\nand again, we need to copy the file path for this file.\\nSo we're working in 080_3b.\\nGo to the explorer, and then copy the path here.\\n\\nPaste, and then hit enter.\\nAnd then open in browser, and click the button.\\nAnd we created this chart in Streamlit,\\nusing only one line of code,\\nwhich shows really how easy it is to use.\\nAnd in the next step I'd like to show you\\nhow to create a bar chart.\\nSo, for this,\\nwe're going to use Streamlit's bar chart function.\\nI think the easiest way to do this, actually,\\nwill be just to copy and paste the code up here.\\n\\nOkay, so let's change out the title.\\nIt's going to be \\\"bar graph.\\\"\\nAnd then the function is now, instead of st.line_chart,\\nit's st.bar_chart.\\nAnd then we can go back to the terminal,\\nand I'm just going to copy and paste the run code here,\\nenter it in, hit enter.\\nSo it's running.\\n\\nOkay, so after we update the code here,\\nand then what we can do is we can just go back over\\nto our application in the web browser.\\nAnd you see that our bar chart has been added,\\nwhich is a nice, dynamic bar chart.\\nAnd it's been added directly below the line graph.\\nNow, Streamlit doesn't have its own pie chart function,\\nbut we can use a pie chart in Matplotlib,\\nor other Python visualization libraries,\\nand pass it into Streamlit's chart function\\nfor Matplotlib to display in Streamlit.\\n\\nTo display the pie chart in Streamlit,\\nlet's first create a simple dataset\\ncontaining types of animals, and their heights.\\nSo, we'll create a variable named \\\"animals.\\\"\\nSet it equal to a list, with the name cat, cow, and dog.\\n\\nSo these are the types of animals\\nrepresented in our animal variable.\\nAnd then let's assign them some heights.\\nSo we'll create another variable named \\\"heights,\\\"\\nand we'll set that equal to another list,\\nwhich will be 30, 150, and 80.\\nOkay.\\nSo now, let's just take this dataset\\nand use it to create a pie chart.\\n\\nSo let's create a label for the pie chart first.\\nWe'll do that by creating a string,\\nand just writing \\\"pie chart:\\\" with a colon in it.\\nAnd then, now we'll need to use Matplotlib\\nto create the pie chart.\\nSo we'll say, \\\"fig and ax is equal to plt.subplots.\\\"\\nGoing to move this up a little just so you can see it better.\\n\\nAnd then we'll call the pie method.\\nSo we'll say \\\"ax.pie,\\\"\\nand the first variable we'll pass in here will be heights.\\nAnd then for our labels,\\nlets set that equal to the animals.\\nNow let's pass the pie chart\\nin Streamlit's pie plot function.\\nSo to do that, we'll say \\\"st.pyplot,\\\"\\nand we'll pass in the figure object.\\n\\nSo now we've written all of our code.\\nIn order to see this displayed in our web browser,\\nthat's also of course very simple.\\nWhat you need to do is just go over to the browser,\\nand then hit the refresh.\\nAnd then you can scroll down, and you can see the pie chart\\nnow shows up.\\nIn this demo,\\nwe displayed three different types of charts in Streamlit\\nusing less than 10 lines of code,\\nwhich is really impressive.\\nNext, we're going to do a deeper dive\\ninto line charts in Streamlit.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2714163\",\"duration\":522,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Line charts in Streamlit\",\"fileName\":\"3006708_en_US_08_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":811,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Explore various options for displaying line charts in Streamlit.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":17244047,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's take a deeper dive\\ninto creating line charts in Streamlit.\\nSo as with the other Streamlit demonstrations\\nthat we're doing here inside of code spaces,\\nwe need to do a pip install of our libraries.\\nSo for this demonstration,\\nour libraries will be time NumPy Panda Streamlit,\\nand matplotlib.\\nWe don't need to do a pip install for time,\\nbut we do need to do a pip install NumPy\\npip install\\npandas.\\n\\nThis is just to be super sure that\\nyou have the libraries available inside your environment.\\nDo pip install Streamlit.\\nAnd then lastly, pip install matplotlib.\\nLet's see, there was a small typo here,\\nso I'm going to change that O to B and rerun this.\\n\\nOkay, good.\\nSo now we have our libraries available to us.\\nNow we need to import them into\\nour Python file here.\\nSo we'll start by importing time.\\nSo I say import time,\\nimport\\nnumpy as np,\\nimport pandas as pd,\\nimport streamlit as st,\\nand import matplotlib.pyplot\\nas plt.\\n\\nNext,\\nlet's create a line chart\\nwhich will grow over time\\nand we'll use the line chart add rows function\\nto grow the chart.\\nSo we'll start by first generating a numpy array\\nwith a single random value.\\nCall it rows.\\nSo we'll say rows is equal to np.random.randn.\\nIn here we will pass the shape of array one cross one.\\n\\nNow let's print the chart name.\\nWe'll call it Growing Line Chart.\\nSo to do that we just write a string\\nand pass in the title that we want for the chart.\\nTo create the line chart using Streamlit,\\nwe need to use the line chart function.\\nSo we'll say st.line_chart\\nand we'll parse in our rows.\\n\\nAnd then let's set this whole thing equal to chart.\\nAfter that, we'll create a loop\\nand we'll grow the line chart in the loop.\\nSo we'll say for i in range,\\n1 through 100, do this.\\nNext, let's generate a random number\\nand append it to the array.\\nThen we'll parse this array to add rows function.\\n\\nSo we'll say new_rows\\nequal to row zero\\nplus np.random.randn,\\nand parse the shape of the array, which is a one by one.\\nNow let's add this array to the chart.\\nTo do that, we'll call the add rows method off\\nof the chart object.\\nSo let's say chart.add_rows\\nand we'll parse in our new rows.\\n\\nAfter that, we'll set rows equal to new rows\\nand then we'll call time.sleep to stop the loop\\nfor five milliseconds.\\nSo here we'll say rows = new_rows.\\nAnd then we'll end this with time.sleep.\\nAnd we will parse in 0.05 for five milliseconds.\\n\\nOkay, so that needs to be a period instead of a comma.\\nSo I'm just going to kind of look through here\\nand make sure my syntax is okay.\\nThis is pretty simple code of course,\\nbut okay so now we go back to the terminal\\nand we need to say streamlit run\\nand we'll copy our file path here,\\npaste it in.\\n\\nOkay, so we have to allow pasting in the chrome browser\\nand then hit enter.\\nIt's going to run it\\nand then click the open in browser button.\\nAnd there is our growing line chart.\\nAnd streamlit really gives us a flexibility\\nof creating graphs\\nin any other Python visualization library\\nand then allowing you to show them inside of streamlit.\\nSo let's create a line chart using mapplotlib.\\n\\nFirst we'll create an array with random values.\\nSo let me go back over to our Python file.\\nAnd so we'll create an array called values\\nand we'll set it equal to np.random.rand,\\nwe'll pass in a 10.\\nThat's just saying that we want the length of the array\\nto be 10 data points.\\nNext, let's just create a little title for this chart.\\n\\nSo we'll call it matplotlib's line chart.\\nAnd\\nokay, it looks like\\nthis comma here isn't going to work\\n'cause it closes the string,\\nso I'll just take that out\\nand okay, that'll be our title.\\nNext, let's plot the values with mapplotlib.\\nSo we'll say fig and ax.\\n\\nWe'll set these objects equal to plt.subplot,\\nsubplots,\\nand then we'll call the plot method off of our ax object.\\nSo ax.plot\\nand we'll pass in our values here.\\nNow let's parse the line chart into streamlit\\nusing the pyplot function.\\nSo we'll say st.pyplot\\nand we'll parse in our figure object.\\n\\nSo then the next thing you need to do\\nis just let's go back over to our webpage\\nand we can hit refresh.\\nAnd now we have got our matplotlib line chart.\\nHere's our growing, here's our growing chart,\\nand then we've got our max matplotlib line chart.\\nAnd so it looks like\\nwe have far too many of these matplotlibs line charts\\nso let's go back to the code\\nand see if we can figure out what went wrong there.\\n\\nOkay, so after looking over the code here a little bit,\\nI couldn't find anything wrong with it\\nand so I went and just refreshed the screen\\nand the mapplotlib line chart\\nworking as expected.\\nSo you can see now here we only have one chart,\\nand after doing a little bit of research,\\nI discovered that this is actually\\na known issue with the context manager for mapplotlib.\\n\\nIn any case, it cleaned itself up here\\nwithin our streamlit, as you can see\\nand so we are good to go.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2715034\",\"duration\":691,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Bar charts and pie charts in Streamlit\",\"fileName\":\"3006708_en_US_08_05_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":1065,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Explore various options for displaying pie charts and bar charts in Streamlit.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":22156396,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] As usual, for using Streamlit in Codespaces,\\nwe need to do a pip install of our required libraries.\\nSo in this demonstration, we're going to be using Pandas,\\nNumPy, Streamlit, and Matplotlib.\\nSo I'll just do a quick pip install of each of these.\\nPip install numpy,\\npip install streamlit,\\nand pip install matplotlib.\\n\\nOkay, so great.\\nNow we have the libraries that we need.\\nI'm going to minimize the terminal here\\nand start working inside of the Python file.\\nWe'll need to first import our libraries, of course,\\nso we will import pandas as pd,\\nimport numpy as np,\\nimport streamlit as st,\\nand import matplotlib.pyplot\\nas plt.\\n\\nAnd in a previous Streamlit demonstration,\\nwe created a bar chart using Streamlit's bar chart function.\\nThis time, let's create a bar chart in Matplotlib\\nand display it in Streamlit.\\nThe bar chart we'll create\\nis a group bar chart comparing the heights\\nand weights of the animals.\\nFirst, let's define the data for the charts.\\nSo we're going to say animals.\\nSo our first variable,\\nand it's equal to\\na list of animal types.\\n\\nSo we'll have 'cat', 'cow',\\nI'm going to go through and just get the placeholders ready\\nfor 'dog' and 'goat'.\\nOkay, now let's create a second variable called heights.\\nWe'll set it equal to a list of numbers.\\nThat'll be 30, 150,\\n80, and 60.\\n\\nAnd then their weights in kilograms\\nwill be set equal to 5, 400,\\n40, and 50.\\nAnd we'll start first by defining the subplots.\\nSo fig, ax are equal to\\nplt.subplots.\\nNext we'll define the label locations\\nand the width of the bars.\\n\\nSo for that, we'll say x is equal to,\\nand then we'll call the arrange function.\\nSo that's np.arrange.\\nAnd we'll pass in length, so len.\\nAnd then we want the len to be heights.\\nSo we'll pass in our heights object,\\nand we'll set our width here equal to .40.\\n\\nWidth equal to 0.40.\\nNow let's draw the first set of the group of bars\\nfor the animal's heights.\\nTo do that, we will call the bar method\\noff of the ax object.\\nSo ax.bar.\\nAnd here, we'll first pass the position of the bars.\\nSo x minus 0.2.\\n\\nAnd then we'll pass the heights list next.\\nSo first let's just say x minus 0.2,\\nand the next parameter will say heights and width.\\nAnd then let's set the color of this bar\\nequal to red.\\nOkay, and then I'm going to copy and paste this code\\nto make a second group of bars\\nfor the animal's weights.\\n\\nSo let's change the position\\nto x plus 0.2 here.\\nAnd then we will replace the heights variable\\nwith the weights variable.\\nAnd then we can change the color here to orange.\\nNext, we'll set a legend for this chart.\\nSo to do that, we need to call the legend method\\noff of the ax object.\\nSo ax.legend.\\n\\nAnd then we'll pass in a list\\nwith the titles we want in the legend.\\nSo the first will be height,\\nand then the second will be weight.\\nThey should, of course, be in strings.\\nRephrase, and these labels should,\\nof course, be in strings.\\nNow we'll set the labels\\nand their positions on the x-axis.\\n\\nSo to do that we'll call the set_xticks method\\noff of the x object.\\nWe'll pass in our x variable.\\nAnd then let's also set some tick labels.\\nSo to do that we'll use the set\\nxtick labels method.\\nSo we'll say ax.set_xticklabels.\\n\\nAnd then we want our labels to be from our animals variable,\\nso we'll pass our animals variable.\\nAnd so here we'll pass the list of animal names.\\nAnd now let's call Streamlit's pyplot function\\nto display the Matplotlib chart in the Streamlit app.\\nSo to do that we'll say st.pyplot,\\nand we'll pass in our fig object.\\n\\nLet me just take a quick look at the syntax here.\\nDon't see any obvious problems,\\nso let's go ahead and then just run this.\\nSo what I'm going to do is in the terminal,\\nsay streamlit run.\\nWe need to copy the file path here,\\ncopy path, paste it in, hit Enter.\\nAnd then click the Open in browser button.\\n\\nOkay, amazing.\\nto plot out a Matplotlib function\\non an actual web browser.\\nNow let's look at how to do the same thing in a pie chart.\\nLet's go back over to our Python file.\\nAnd then what we're going to do\\nis we're going to create an advanced pie chart of some animals\\nand their heights and centimeters.\\nAnd we'll create the pie chart using Matplotlibs library,\\nand then display it using Streamlit again,\\nof course, like we did with the bar chart.\\n\\nIn the pie chart, the pie slices will be ordered\\nand plotted counterclockwise.\\nA wedge of a pie chart can be made to explode\\nfrom the rest of the wedges of the pie chart\\nusing the explode parameter of the pie function.\\nSo let's define that now.\\nWe'll say explode equal to, and then create a list,\\nand we'll say 0.2, 0.1,\\n0.1, 0.1.\\n\\nNow let's create the plot.\\nSo we'll say plot_pie, ax.\\nBoth of these objects are equal to\\nplt.subplots.\\nAnd then we'll call the pie method\\noff of the ax object,\\nso ax.pi.\\nAnd we'll pass in the height\\nof the animals, so heights.\\n\\nAnd then we'll set explode equal to\\nour explode object that we just created.\\nWe want our labels to be set equal to\\nthe list of names in our animals object.\\nSo we'll put labels equal to animals.\\nAnd next we'll pass in autopct as a parameter,\\nwhich enables you to display the percentage values\\nusing python string formatting.\\n\\nSo we'll say autopct\\nis equal to, and then we will write a string\\nthat says %1.1f%%.\\nSo %1.1f%%.\\nAnd that's it's string format.\\nThe next parameter is the shadow parameter.\\n\\nAnd so we're going to set shadow here equal to true.\\nIn the next line, we will call the axis method\\noff of the ax object.\\nSo we'll say ax.axis.\\nAnd then passing a string that reads equals.\\nIt should be equal, not equals.\\nSo I'll take that S out.\\nThe equal here acts as a keyword.\\nEqual aspect ratio ensures that pie is drawn as a circle.\\n\\nNext we'll call the pyplot function.\\nSo that's st.pyplot.\\nAnd we'll pass in our plot_pie object.\\nSo I'll go over to our browser and hit Refresh.\\nWow, that's amazing.\\nLook at that gorgeous pie chart.\\nIt looks so much better\\nthan what you can create in default,\\nin basic Matplotlib.\\n\\nAnd also another added advantage\\nis that this is showing now on a webpage\\nthat you can share with other users,\\ninstead of just having it stuck inside\\nof your Jupyter Notebook environment.\\nNow let's look at creating statistical charts in Streamlit.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2715035\",\"duration\":515,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Create statistical charts\",\"fileName\":\"3006708_en_US_08_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":740,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to create statistical charts in Streamlit. This video covers histograms, boxplots, and scatter plots.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":18753819,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] On this coding demo,\\nwe're going to create statistical charts using streamlit.\\nAnd so as usual, we need to do a pip install\\nof the libraries we need.\\nSo that'll be streamlit, seaborn, matplotlib,\\nand pandas, and sklearn in this case.\\nSo we'll start with a pip install of streamlit,\\nand then do a pip install seaborn,\\npip install matplotlib,\\npip install pandas,\\nand then we'll do pip install sklearn.\\n\\nOkay, so that should be pip install scikit-learn.\\nOkay there, now we have installed\\nall of the libraries we need for this demonstration.\\nSo let's import them in our Python file.\\nSo we'll say import streamlit as st,\\nimport seaborn as sns,\\nimport matplotlib.pyplot as plt,\\nimport pandas as pd.\\n\\nAnd then for the data for this demonstration,\\nwe're going to use the iris dataset.\\nSo we're going to import that from scikit-learn.\\nSo we'll say from sklearn.datasets\\nimport load_iris.\\nAfter that, let's load the iris data.\\nSo to do that, we'll create an object called iris_data\\nand we'll set it equal to load_iris function.\\n\\nNow we'll load iris dataset into a data frame.\\nWe'll call it data.\\nSo we'll say data =,\\nand then we will call the DataFrame constructor.\\nSo that's pd.DataFrame,\\nand we'll pass in our iris_data.data,\\nand then let's pass the column names.\\nSo we'll say columns = iris_data.feature_names.\\n\\nFirst, we'll create a histogram chart from the data frame\\nusing seaborn and streamlits pyplot function.\\nSo first let's just write fig = plt.figure,\\nand we'll also be using the histplot function from seaborn.\\nSo we'll say sns.histplot,\\nand we'll pass in our data frame.\\n\\nSo we'll say data=data,\\nand the number of bins\\nwe want to create in the histogram.\\nSo we'll say bins here=20.\\nNow let's show the histogram and the streamlit app.\\nTo do that, we would just call the pyplot function.\\nSo that's st.pyplot,\\nnow passing our fig object.\\nAnd just looking really quickly for any typos,\\nI see that I need to change this E to an O, load_iris.\\n\\nAnd aside from that, it looks pretty good.\\nSo the next thing I need to do\\nis just to say, streamlit run,\\ncopy the file path,\\npaste it in,\\nand hit Enter.\\nOkay, and then we'll open in browser.\\nSo our application is running.\\nLook how pretty that is.\\nSo with respect to colors,\\neach of the four features,\\nsepal length, sepal width, petal length, and petal width\\nare represented by a different color in the histogram.\\n\\nAnd this makes it easier to distinguish\\nbetween the distributions of each feature.\\nSo we have 150 data points\\nthat have been bend into 20 separate intervals.\\nAnd the value of this histogram\\nis that it provides a visual summary\\nof the numerical data in the dataset,\\nwhich allows us a quick understanding of the distribution\\nof measurements across the iris species.\\n\\nSo for example, these peaks here in the histogram\\nrepresent the most common measurement ranges\\nwhile the spread indicates variability within the data.\\nNext, let's create a boxplot on the iris dataset.\\nSo to do that,\\nwe'll say fig = plt.figure.\\nThen we'll call seaborn boxplot function.\\nSo that's sns.boxplot,\\nand we'll pass in our data.\\n\\nSo here we'll say data=data,\\nand then we will call streamlits pyplot function\\nto plot this out.\\nSo that's st.pyplot,\\nand we'll pass in our fig object.\\nAnd then go over to our browser and hit refresh.\\nOkay, so nice.\\nNow we have a nicely labeled boxplot.\\nAnd this is similar to the boxplot we explored\\nearlier in this course when we were discussing\\nhow to identify outliers in a dataset.\\n\\nSo you can see these points here past the whiskers\\nin the sepal width field.\\nThese points are highly suspect\\nfor being outliers in the dataset,\\nand you would want to examine and treat those data points\\nbefore moving into machine learning.\\nThe last plot we'll draw is a scatterplot.\\nSo we'll just create another fig object,\\nfig = plt.figure.\\n\\nAnd then we're going to use seaborn's scatterplot function.\\nSo that's sns.scatterplot,\\nwe'll say data=data,\\nand then plot this out using streamlits.\\nSo that's st.pyplot,\\nand passing our fig object.\\nAnd then I'm going to go back over to the browser\\nand hit refresh.\\n\\nAnd now we have a scatterplot\\nof all of the 150 data points\\nthat are contained within the iris dataset.\\nThey are color coded according to feature,\\nso this makes it easier for us\\nto get at glance view of the distribution of each feature\\nand basically what the values of the features are\\nwithout having to dig too much into the actual data itself.\\n\\nAnd that's it for Streamlit.\\n\"}],\"name\":\"8. Collaborative Analytics with Streamlit\",\"size\":90480614,\"urn\":\"urn:li:learningContentChapter:4583166\"},{\"duration\":70,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4588031\",\"duration\":70,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Next steps\",\"fileName\":\"3006708_en_US_09_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":93,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1694005,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In terms of next steps,\\nI definitely encourage you to go over to the Python\\nfor Data Science Essential Training,\\nIntroduction to Machine Learning Course.\\nThat course was built specifically\\nas the perfect follow up for this one.\\nAlso, I want to encourage you to go ahead\\nand start practicing your new data skills\\nby working to create efficiencies in your daily workflows\\nwith your employer or in your business.\\nLastly, I want to invite you to join our community,\\nThe Convergence.\\n\\nThe Convergence is an online community space\\nthat's dedicated to empowering operators\\nin the data industry by providing news and education\\nabout evergreen strategies, late-breaking data\\nand AI development, and free or low cost upscaling resources\\nthat you need to thrive as a data leader,\\nthat you need to thrive as a leader\\nin the data and AI space.\\nTo join us in The Convergence, simply visit this page,\\ndata-mania.com/newsletter, and drop your details\\nso that we can send you a kickoff email\\nthat's loaded to the brim with free goodies\\nto get you ahead in your data career.\\n\\n\"}],\"name\":\"Conclusion\",\"size\":1694005,\"urn\":\"urn:li:learningContentChapter:4589022\"}],\"size\":942069028,\"duration\":27863,\"zeroBased\":false},{\"course_title\":\"Python Data Analysis\",\"course_admin_id\":2825705,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":2825705,\"Project ID\":null,\"Course Name\":\"Python Data Analysis\",\"Course Name EN\":\"Python Data Analysis\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"Data science is transforming the way that government and industry leaders look at both specific problems and the world at large. Curious about how data analysis actually works in practice? In this course, instructor Michele Vallisneri shows you how, explaining what it takes to get started with data science using Python.&lt;br/&gt;&lt;br/&gt;Michele demonstrates how to set up your analysis environment and provides a refresher on the basics of working with data structures in Python. Then, he jumps into the big stuff: the power of arrays, indexing, and tables in NumPy and pandas\u00e2\u20ac\u201dtwo popular third-party packages designed specifically for data analysis. He also walks through two sample big-data projects: using NumPy to identify and visualize weather patterns and using pandas to analyze the popularity of baby names over the last century. Challenges issued along the way help you practice what you've learned.&lt;br/&gt;&lt;br/&gt;Note: This version of the course was updated to reflect recent changes in Python 3, NumPy, and pandas.\",\"Course Short Description\":\"Interested in using Python for data analysis? Learn how to use Python, NumPy, and pandas together to analyze data sets large and small.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":4666188,\"Instructor Name\":\"Michele Vallisneri\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Theoretical Astrophysicist at NASA Jet Propulsion Laboratory\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2020-03-11T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"Yes\",\"LIL URL\":\"https://www.linkedin.com/learning/python-data-analysis-2,https://www.linkedin.com/learning/python-data-analysis-2020-revision\",\"Series\":\"Deep Dive (X:Y)\",\"Limited Series\":null,\"Manager Level\":\"General\",\"LI Level\":\"Intermediate\",\"LI Level EN\":\"Intermediate\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Data Science\",\"Primary Software\":\"Python\",\"Media Type\":\"Video\",\"Has CEU\":\"Yes\",\"Has Exercise Files\":\"Yes\",\"Visible Duration\":9035.0,\"Visible Video Count\":41.0,\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":185,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2293496\",\"duration\":120,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Get started in data analysis with Python\",\"fileName\":\"2825705_00_01_WL30_Welcome\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"Using Python, you can analyze data rapidly, using powerful tools adopted by a large and helpful community.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":23028870,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - Data science, it powers so much of modern life,  \\n the internet, social media, artificial intelligence.  \\n But also on a personal level, the statistics  \\n from your Fitbit or the next song recommended by Pandora.  \\n And, truly, data science is driving a personal  \\n and social evolution.  \\n We're constantly learning and getting better  \\n and accomplishing monumental goals.  \\n However, do you feel like you're missing the boat?  \\n Maybe you're watching all these advances,  \\n but you don't really know how to get in the game.  \\n And you wonder, \\\"What goes on under the hood?  \\n \\\"How does someone one do data science?\\\"  \\n You don't know where to start.  \\n Do not worry, this is where I can help.  \\n My name is Michele Vallisneri,  \\n and I'm a research scientist at NASA.  \\n I use data science concepts and tools every day  \\n to analyze astronomy datasets,  \\n and my tool of choice is Python.  \\n It's an expressive and pragmatic computer language  \\n that has its own spirit and style.  \\n And it's supported by a diverse and helpful user community.  \\n My goal with this course is to get you started  \\n with data science, and more specifically, data analysis  \\n with Python, in a friendly and approachable way.  \\n It's not all encompassing.  \\n I don't recommend applying for a PhD program  \\n right after this course, but it will get you started,  \\n and I really hope inspired.  \\n That's what matters, and that's what you need,  \\n a jumping off point.  \\n I will take you through the foundations  \\n of doing data analysis with Python.  \\n We will look at the most important programming constructs,  \\n data structures, and third party packages.  \\n With this, you will be able to complete  \\n simple data analysis tasks, and you will be ready  \\n to move on to more advanced topics.  \\n I like to teach by example rather than in the abstract,  \\n so throughout this course, we will write  \\n and execute practical code and analyze real-world data.  \\n So let's enter the friendly but exciting world  \\n of Python data analysis.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2295355\",\"duration\":38,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What you need to know\",\"fileName\":\"2825705_00_02_LA30_What\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"To learn the most from this course you need an elementary knowledge of the Python language, which you can obtain from other LinkedIn Learning courses. However, in this video, you can review Python's data structures, which are crucial to data analysis.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8474848,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - Before getting started with this course,  \\n you want to have a basic working knowledge  \\n of programming in Python.  \\n Although we will review the aspects of the language  \\n that are essential to any data analysis task,  \\n I will not teach you about every feature of Python  \\n that we will meet.  \\n It will also be helpful to have an understanding  \\n of basic mathematical and statistical concepts,  \\n for example logical operations, functions,  \\n averages, minima and maxima.  \\n If you are familiar with these topics I recommend you start  \\n with beginner level Python and statistics courses  \\n in the library or with a textbook  \\n that suits your learning style.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2294373\",\"duration\":27,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What's new in this update\",\"fileName\":\"2825705_00_03_LA30_New\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"Like the original version, this course covers the foundations of data analysis with Python\u2014data structures, NumPy, pandas, matplotlib\u2014using practical real-world examples. The course is updated to reflect recent changes in the interfaces of those modules, to explore new features in recent versions of Python 3, to emphasize insightful visualization, and to introduce pandas with a modern pedagogical approach.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6090412,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - This is a new version of this course  \\n in which I have incorporated user feedback  \\n from many learners like you.  \\n Like the original version  \\n this course covers the foundations  \\n of data analysis with Python,  \\n data structures and the num pi, pandas,  \\n and map log packages  \\n using practical, real world examples.  \\n I have also updated the course  \\n to reflect changes in those modules  \\n and new useful features in recent versions of Python 3.  \\n \\n\\n\"}],\"name\":\"Introduction\",\"size\":37594130,\"urn\":\"urn:li:learningContentChapter:2294386\"},{\"duration\":840,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2295356\",\"duration\":155,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Install Anaconda Python on OS X\",\"fileName\":\"2825705_01_01_XR30_installosx\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"Anaconda is a popular Python distribution that includes many useful packages. In this video, learn how to download and install Anaconda on your Mac.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5237373,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] For this course,  \\n we need an up to date installation of Python 3,  \\n and a few third party packages including Jupyter, NumPy,  \\n Pandas, and Matplotlib.  \\n In this video I show you how to install everything you need  \\n on MacOS 10.  \\n If you are a Windows user,  \\n feel free to jump to the next video.  \\n Later I will also show you how to use Python in the cloud  \\n using only your web browser.  \\n If you already use Python on your machine  \\n and you know how to install extra packages, please do so.  \\n Otherwise I suggest you follow me  \\n and install the free Anaconda Python Distribution,  \\n which includes everything that we will need.  \\n To install we go to anaconda.com.  \\n We find the download link at the top.  \\n Scroll down to our platform  \\n and select the Anaconda graphical installer  \\n which is currently at version 3.7.  \\n Any later version will also work fine.  \\n As of January 2020, Python 2 is no longer supported,  \\n so you should definitely be using version three  \\n which is mature, efficient,  \\n and introduces many exciting new features  \\n compared to version two.  \\n Once the download has completed,  \\n we click on the installer  \\n and proceed through a standard installation  \\n which will require several clicks.  \\n We are asked also  \\n whether we wish to install the PyCharm IDE,  \\n which is very complete and powerful.  \\n We will not be using it for this course,  \\n but you can give it a try.  \\n We can trash the installer,  \\n and we can now try out our new python installation,  \\n by opening up a terminal and typing Python.  \\n This gets us into the standard Python shell  \\n where we can write an execute code interactively.  \\n The prompt informs us  \\n that this is indeed the Anaconda version of Python 3.7.  \\n It's traditional to just say hello.  \\n We can also verify that all the packages we need  \\n are already installed by attempting to import them.  \\n NumPy, Pandas, and Matplotlib.  \\n No news is good news.  \\n All done.  \\n We are now ready to start experimenting with Python.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2295357\",\"duration\":172,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Install Anaconda Python on Windows\",\"fileName\":\"2825705_01_02_XR30_installwindows\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Anaconda is a popular Python distribution that includes many useful packages. In this video, learn how to download and install Anaconda on your Windows PC.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4789286,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Narrator] For this course,  \\n we need an up to date installation of python three.  \\n And a few third party packages.  \\n Including jupyter notebook, numpy, pandas,  \\n and matplotlib.  \\n In this video, I show you how to install everything  \\n you need on Windows.  \\n However, you can also experiment with Python in the cloud  \\n from your web browser.  \\n I explain how to do so at the end of this chapter.  \\n If you are already using Python on your machine  \\n and you know how to install extra packages,  \\n you are free to do so.  \\n Otherwise, I suggest you follow me  \\n and install the free Anaconda Python distribution.  \\n Which includes everything that we need.  \\n To install, we go to the anaconda.com website,  \\n we find a download link on the top.  \\n Verify that we have the right platform.  \\n And then download the 64 bit graphical installer.  \\n This is currently at version 3.7,  \\n but any later version will also work fine.  \\n Once the download has completed,  \\n we click on installer and proceed through an installation.  \\n This requires a few clicks.  \\n (mouse clicks)  \\n As of January 2020, Python two is no longer supported.  \\n So you should definitely be using version three.  \\n Which is mature, efficient, and introduces  \\n many exciting new features compared to version two.  \\n The final phase of the installation takes a few minutes.  \\n It looks like the setup is finished,  \\n and we now can get started with Python.  \\n We can now try out our new Python  \\n by opening up the Anaconda prompt,  \\n and typing python.  \\n This gets us into the standard python shell,  \\n where we can write and execute code interactively.  \\n The prompt, informs us that this is indeed  \\n the Anaconda version of Python 3.7  \\n It's traditional to say hello.  \\n We can also verify that all the packages that we need  \\n are already installed by attempt to import them.  \\n Numpy, pandas, and matplotlib.  \\n No news is good news.  \\n All done.  \\n We are now ready to start experimenting with python.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2295358\",\"duration\":251,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Working with Jupyter Notebooks\",\"fileName\":\"2825705_01_03_XR30_jupyter\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Jupyter Notebooks is a very powerful environment for exploratory data analysis. In this video, learn how to open, create, edit, and run Jupyter Notebooks.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6607421,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Narrator] Jupyter notebooks,  \\n offer a very convenient way to write code,  \\n run it, and collect the results including plots,  \\n in a single document.  \\n You can even write formatted text and equations.  \\n This is my favorite way of using Python,  \\n because it lets me experiment with data and code,  \\n document my work, and go back to it later.  \\n You start Jupiter notebook from the Anaconda navigator,  \\n by clicking on launch.  \\n If you don't have an Anaconda,  \\n you can go to a terminal, and type Jupyter notebook.  \\n A web browser opens, and I can choose  \\n if I wish to load an existing notebook from the file system,  \\n or to start a new one,  \\n which I do at the top right of the screen.  \\n New, Python three notebook.  \\n Here's the notebook. See the green box at the top?.  \\n It's a cell. It's ready for me to write some Python,  \\n for instance,  \\n (keyboard typing)  \\n The customary equating.  \\n I execute the code, by pressing Shift + Enter,  \\n or Shift + return depending on your keyboard.  \\n The output is printed immediately below it.  \\n I can click inside the cell,  \\n modify it,  \\n and execute it again, with shift enter.  \\n If a cell has a blue border,  \\n I need to press ENTER before I can edit it.  \\n Or I can click inside a different cell, to edit that one.  \\n In addition to a simple command,  \\n I can write a function over multiple lines.  \\n The editor, will color the Python source keywords,  \\n appropriately.  \\n Again, I execute with Shift + Enter  \\n cells, can also contain text rather than code.  \\n For that, I can use the drop down menu cell,  \\n to change the cell type to markdown.  \\n Then I click in the cell and start writing.  \\n Markdown is a simple but powerful text format  \\n that can do basic formatting, bold, italic,  \\n bullet points, hyperlinks, and even mathematical formulas.  \\n For instance, bold, (keyboard typing), with two asterisks.  \\n (keyboard typing)  \\n URLs, which are recognized,  \\n (keyboard typing)  \\n a bullet point,  \\n and a formula in the LaTeX language, between dollar signs.  \\n When I execute the cell, with shift Enter,  \\n everything is formatted in prettified.  \\n To copy a cell, I can click inside it,  \\n or use the arrow keys to select it, and then press  \\n escape C, to copy and then escape V, to paste.  \\n To cut a cell, I can do escape X  \\n To delete a cell ctrl M, and then D, and again D,  \\n Jupyter wants to be sure  \\n you really want to delete that cell.  \\n Or you can use the menus, of course,  \\n I gave you only the most useful keyboard shortcuts,  \\n but there are many more, look under the Help menu.  \\n The notebook is saved periodically for us,  \\n but I can also do that at any time with command S,  \\n ah, and I can give this notebook a name,  \\n such as learning Jupyter,  \\n by clicking at the top of the window.  \\n Go find the Jupyter notebook documentation,  \\n at Jupiter.org.  \\n You will also learn about Jupyter lab,  \\n and even more powerful,  \\n and complete interface than notebooks.  \\n For this course however, we will stay with Jupyter notebook.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2295359\",\"duration\":92,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Using the exercise files\",\"fileName\":\"2825705_01_04_XR30_exercisefiles\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2371318,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] For most of the videos in this course,  \\n we will be working through one of the Jupyter Notebooks  \\n that I prepare for you.  \\n For each notebook, we will go through the path  \\n and code that it contains.  \\n We will discuss what the code does  \\n and why I wrote it that way.  \\n We will execute it and examine the resulting output.  \\n At any time, you're welcome to pause the video,  \\n inspect the code, make changes  \\n and run your own experiments.  \\n All the notebooks are collected in your exercise files,  \\n organized by course chapter.  \\n They are the files with .ipynb file ending.  \\n You will also see some of the data files  \\n that we will be analyzing.  \\n In Chapter Five, I have included a subfolder  \\n named Downloaded.  \\n These are files that we will download  \\n from the web using Python,  \\n but I am including them here  \\n in case something goes wrong with the download.  \\n We open a notebook by finding it  \\n within the Jupyter Notebook file browser  \\n and clicking on it.  \\n The notebook cell that we're discussing and running  \\n will always be the one enclosed by a blue or green box.  \\n Once we're done with the notebook,  \\n you can close the browser tab  \\n and shut down the notebook in the Jupyter file browser  \\n by clicking on the box and then clicking Shutdown.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2293497\",\"duration\":170,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Using Python in the cloud\",\"fileName\":\"2825705_01_05_XR30_cloudpython\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"You do not need to install Python on your computer to use it for data analysis. In this video, learn how to use the Microsoft notebook server to run Python code in the cloud.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5073696,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] An exciting, recent development  \\n in the Python ecosystem is that it has become possible  \\n to run notebooks in the cloud using services  \\n such as Google Colaboratory and Microsoft Azure Notebooks.  \\n With both, you get a rather functional environment for free.  \\n If you'd like to follow along with this course  \\n using Python in the cloud,  \\n I suggest you use Azure Notebooks.  \\n Let me show you how to.  \\n We start at notebooks.azure.com.  \\n We sign in, which you can do using  \\n an existing Microsoft account or creating one.  \\n You may also be asked to create a user name  \\n for your profile.  \\n In Azure Notebooks, notebooks are organized in projects,  \\n so we create a new one.  \\n I will call it Python data analysis.  \\n The easiest way to get the exercise files  \\n into Microsoft Azure is to upload the zip file  \\n that contains all of them.  \\n The Upload button is here on the top right.  \\n So I go find a file and upload it.  \\n To unzip the archive, I open the Azure terminal.  \\n I type cd library to move into the project directory  \\n and then unzip exercise\\\\ files.zip.  \\n This will take a moment.  \\n Once it's done, we can type exit,  \\n and close this tab.  \\n If I refresh the window,  \\n I see that the exercise files have appeared.  \\n We are now ready for our course.  \\n For instance, I can look inside the exercise files  \\n for chapter two and select the first notebook.  \\n Azure Notebooks can be somewhat slow when loading  \\n and writing files, but don't worry.  \\n You'll get there.  \\n Also, as we speak, the default Python on Azure is 3.5,  \\n which is too old for this course.  \\n Hopefully that will change soon.  \\n In the meantime, please remember to switch the kernel  \\n to Python 3.6 after you open each notebook.  \\n You do that from the Kernel menu, go into Change kernel,  \\n and select Python 3.6  \\n There are many options out there to use Python in the cloud.  \\n Please feel free to use the one that works out best for you.  \\n \\n\\n\"}],\"name\":\"1. Installation and Setup\",\"size\":24079094,\"urn\":\"urn:li:learningContentChapter:2294387\"},{\"duration\":1819,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2295360\",\"duration\":425,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Warmup with Python loops\",\"fileName\":\"2825705_02_01_XR30_loops\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"There are many use cases in data analysis where you need to operate on data in a sequential fashion. In Python, you do so with looping constructs. In this video, learn how to refresh the Python loop syntax by solving a simple problem\u2014finding all the ways in which you can break a dollar into a set of coins.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11735820,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] We will begin every video  \\n by importing a standard set of Python modules that we need.  \\n This is a typical list.  \\n It's expedient to load often used modules  \\n by giving them a shorter alias.  \\n NP is the community standard for NumPy, PD for pandas.  \\n We also load this simple commander entity interface  \\n to matplotlib, pyplots,  \\n and finally, we ask the Jupyter notebook  \\n to keep the plots that we make in the notebook itself  \\n instead of opening a different window.  \\n So I shall now execute this cell  \\n by pressing Shift + Return.  \\n If you've worked in programming before,  \\n you must be familiar with loops.  \\n They allow us to automate repetitive operations.  \\n And loops are a good topic  \\n to start a quick review of the Python core language,  \\n which will focus on the features that are most important  \\n to work with data.  \\n So, how exactly do loops work in Python?  \\n We see them in a concrete example.  \\n Consider the combinatorial problem of breaking a U.S. dollar  \\n into all possible combinations of coins.  \\n For instance, $1 coin,  \\n two half-dollar coins,  \\n one half-dollar coins and two quarters, and so on.  \\n You can already see how we're going to need  \\n several nested loops for this.  \\n The basic structure of a loop in Python  \\n is for variable in iterable  \\n followed by a block that is executing multiple times  \\n with the variable that can own the values provided  \\n by the iterable.  \\n But what is an iterable?  \\n We can think of it as a black box  \\n that keeps providing new values until it runs out.  \\n A simple example is a Python container  \\n such as a list or a dict.  \\n Perhaps the most important iterable is range,  \\n which provides integer value from a start to a stop,  \\n exclusive of the stop.  \\n That means that range zero 10 counts from zero to nine.  \\n There are many reasons for this convention  \\n about the end value.  \\n For instance, by looking at the end value,  \\n we see immediately that the total number of elements  \\n in the range is 10.  \\n In the end, however, we just have to accept this  \\n as one of the building assumptions of Python.  \\n Range has a couple more interesting features.  \\n We can omit the stat value,  \\n and then it's assumed to be zero.  \\n We can provide a step argument to move through the range  \\n in increments larger than one.  \\n And if I give the step,  \\n I must also give the initial value to avoid confusion.  \\n Notice that the step is the third argument  \\n that I give in line four.  \\n Let's go back to the dollar.  \\n To generate all possible ways to break it up,  \\n we will use a very simple minder strategy.  \\n We will consider all possible candidate combinations,  \\n including the zero to one $1 coins,  \\n zero to two half-dollar coins,  \\n zero to four quarters, and so on.  \\n And at the end for each,  \\n we will check whether it adds up to a dollar.  \\n Therefore we need six nested loops.  \\n A loop within a loop within a loop.  \\n Luckily, it's Python that will keep track of them.  \\n This means that the first loop will be over range two.  \\n So, looping over zero and one.  \\n To keep the maximum number of each coin inside,  \\n we'll write it as range(1+1).  \\n We then proceed with the other coins, one loop for each.  \\n Each nested loop is indented with respect to its parent.  \\n Inside the innermost loop,  \\n we will check whether the total amount is $1.  \\n If so, we will add this combination to a list  \\n called combinations, which will start that.  \\n Let's try this out.  \\n And let's look at the results.  \\n The first few combinations seem to check out fine.  \\n It turns out there are 293 ways to get a dollar in change.  \\n That's a solution to the problem we posed.  \\n However, in data analysis,  \\n it often happens that the solution raises a new question.  \\n For instance, how many ways  \\n to make $2 out of change or three?  \\n Does the number of combinations increase linearly  \\n or quadratically with the amount we're breaking up?  \\n What we need to do, then, is to take the code we wrote  \\n and generalize it to answer those questions.  \\n Creating a function,  \\n we'll call it, say, find combinations,  \\n that will take a dollar amount in cents  \\n and return all possible ways to do it.  \\n Before we do so,  \\n we make a couple of changes to our code  \\n to make it faster and to make it easier to generalize.  \\n This is an example of refactoring.  \\n The first change is that we will not loop by count,  \\n but by value, using the step argument of the range.  \\n For instance, instead of looping over quarters  \\n with count 25 between zero and four,  \\n we will loop over the amount  \\n from zero to 100 in steps of 25.  \\n So, I have rewritten all the loops in these terms.  \\n As you see, the end of the range is always a dollar,  \\n 100 cents plus one.  \\n So the dollar is included.  \\n And the step is 100 for $1, 50 for half dollar,  \\n 25 for the quarter, and so on.  \\n The second change we're making  \\n is that we don't actually need  \\n the innermost loop over the pennies.  \\n As long as the total up to that point  \\n is less or equal than a dollar,  \\n we can always make up the difference.  \\n Therefore, we compute the dollar  \\n and write a different test.  \\n The result is again the same.  \\n Now we can take our code  \\n and turn it into the function find combinations  \\n by replacing the value 100 with a variable argument.  \\n We'll call it total.  \\n We do need to indent our code  \\n to make up the body of the function.  \\n So, here we go, $2.  \\n Or $3.  \\n You may be curious to know how fast this number grows.  \\n A plot will give us an idea.  \\n So let's plot 100 through 500,  \\n and we use a comprehension, which we will explain later,  \\n to find a corresponding number of combinations.  \\n Matplotlib will give us a quick plot.  \\n In fact, the number grows approximately  \\n as the fifth power of the total.  \\n You will find the loops are everywhere in data analysis,  \\n so it's good to get familiar with them.  \\n In Python, you can do a lot with the for construct  \\n and with range.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2294374\",\"duration\":319,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Sequences: Lists, tuples, and the slicing syntax\",\"fileName\":\"2825705_02_02_XR30_sequences\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"The built-in sequence types\u2014tuples, lists, ranges, strings, and buffers\u2014in which data elements are associated with indices, set the basic framework for accessing sequential data in Python. In this video, refresh the creation and indexing of sequences, review the important distinction between mutable and immutable types, and highlight the slicing interface.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8870275,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this movie, we are going to review lists.  \\n They are the quintessential Python container.  \\n They provide a way to store an arbitrary number  \\n of Python objects such as strings, floating-point numbers,  \\n other lists, or any other object  \\n and to access them using a numerical index.  \\n In Python lists are denoted by brackets and their elements  \\n are separated by commas.  \\n The length of a list is obtained with len.  \\n Indexing, individual list elements can be accessed by index.  \\n Starting with zero for the first element  \\n and ending at the length of the list minus one.  \\n This convention of starting from zero comes from C,  \\n the language that inspired Python  \\n and that was used to write the standard Python interpreter  \\n known as CPython.  \\n For instance, the first nephew is Huey.  \\n We can also look for the last nephew  \\n and we can even look for a nephew beyond the end of the list  \\n which in this case will yield an error.  \\n We can also index from the end.  \\n Starting at minus one and going down.  \\n That gets us Louie and Dewey.  \\n The bracket indexing notation,  \\n can also be used to reassign elements.  \\n Let's do that for all the nephews with a simple loop.  \\n Here we're just adding their last name.  \\n It's important to remember that lists do not need  \\n to have homogeneous content such as all strings  \\n or all numbers.  \\n We can mix it up.  \\n To add a single element at the end of the list,  \\n we use append.  \\n You see that here we are using Python  \\n in an object-oriented way.  \\n By accessing the method, specifically append,  \\n of the list object.  \\n It's so easy that we barely notice it.  \\n To add multiple elements in one go, we can use extend.  \\n To concatenate two lists, we use a plus.  \\n That's an example of operator overloading in Python  \\n where plus does different things for numbers and for lists.  \\n Last, we can insert elements at any position in a list  \\n using the insert method.  \\n How about removing elements?  \\n We can delete them based on their index with del  \\n or based on the value with remove.  \\n If we want a list sorted we can do this in place.  \\n So we modify an existing list with sort.  \\n Or we can make a new sorted list out of an existing one  \\n with sorted.  \\n Which demonstrates also how to sort backwards.  \\n All of this should be very basic to you  \\n if you work with Python in the past.  \\n Moving on to slices.  \\n Beyond working with individual list elements,  \\n we can manipulate them in groups.  \\n The convention is the same as for Python loops.  \\n The first index is included, the last is not.  \\n It's useful sometimes, to think of the indices  \\n as being placed at the edges of the cells in a list.  \\n We make an example based on the first few squares  \\n of the natural numbers.  \\n If we want the first two squares,  \\n we'd write a slice that goes from zero to two.  \\n There are a few more tricks that we can use in slicing.  \\n For instance we can omit the starting index  \\n to start at the beginning,  \\n omit the ending index to include elements to the end.  \\n Omit both, to get a copy of the list.  \\n Move through the indices in steps.  \\n And even use negative indices to count from the end.  \\n Slices can also be used to reassign a subset of items  \\n or to delete them.  \\n When we introduce NumPy arrays in chapter four,  \\n we will see that this basic slicing syntax carries over.  \\n So it's good to understand it fully on lists first.  \\n The empty list is written with an empty set of brackets  \\n and obviously it has length zero.  \\n Now for tuples, which look like lists  \\n but with parentheses instead of brackets.  \\n They are sometimes described as immutable versions of lists.  \\n We can do the same indexing and slicing tricks,  \\n but we cannot modify the elements or add new ones.  \\n One context in which one sees tuples often  \\n is tuple unpacking, where Python statements  \\n or expressions are automatically evaluated in parallel  \\n over a tuple.  \\n For instance, to assign multiple variables at once.  \\n The parentheses can even be omitted when there is no  \\n room for ambiguity.  \\n Tuples appear also when we iterate  \\n over multiple variables at once.  \\n For example using the enumerate iterator on a list.  \\n Which lets us loop over list index  \\n and list element together.  \\n We can also unpack a tuple to pass it to a function  \\n that requires multiple arguments such as three args.  \\n It takes a tuple if we prefix it with an asterisk.  \\n This ends our review of lists.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2296097\",\"duration\":317,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Dictionaries and sets\",\"fileName\":\"2825705_02_03_XR30_dicts\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Python dicts associate unique keys with values, while sets express collections of data items without duplication. These data types complement sequences in many common data-analysis tasks. In this video, review the creation, updating, and indexing of Python dicts and sets, and explore the changes to dict in Python 3.6 and later.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8565831,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] While lists give us a way  \\n to retrieve values by their index.  \\n Python dictionary or dicts  \\n associate keys with values.  \\n After my imports,  \\n let me write a simple dictionary.  \\n Dicts are written with curly braces,  \\n separating items with commas  \\n and prefixing them by their key in a column.  \\n For instance, the capitals of a few countries.  \\n Just as we do with lists,  \\n values are accessed with a bracket notation.  \\n But instead of a number, we're going to use a key.  \\n For instance,  \\n we may wish to look at the capital of Italy.  \\n The same notation can be used to add items to a dictionary.  \\n Accessing a nonexistent item results in a key error.  \\n We can also check  \\n whether an item exists or not  \\n with the in operator.  \\n So we have Italy, but not Germany.  \\n Combining two dictionaries requires  \\n a little more thought than combining two lists.  \\n Unlike lists, we cannot just use the plus to add them.  \\n That's because we need to specify what happens  \\n if both dictionaries define the same key.  \\n What we then do is to update a dict using another,  \\n which will replace existing items as appropriate.  \\n This happens in place and modifies the dict.  \\n Similarly to lists, we can delete items by key.  \\n In fact, keys do not need to be strings.  \\n Any Python object that is hashable may be used as a name.  \\n Hashable means that Python can convert it to a number.  \\n That's true for many types of objects.  \\n For instance, tuples  \\n which may be used to encode a birthday.  \\n We can see the internal representation  \\n of the keys with hash.  \\n Just very large numbers.  \\n The empty dictionary is written with an empty set of braces  \\n and has length zero.  \\n Looping over a dictionary  \\n is very similar to looping over a list.  \\n However, there are three different kind of loops  \\n you may want to write.  \\n The most straightforward syntax loops over the keys,  \\n for key in dictionary.  \\n Here, I'm using bold to denote language keywords.  \\n Whereas Roman words are the names of the variables  \\n that you will be using.  \\n You can loop also explicitly over the keys,  \\n you can loop over the values,  \\n and you can loop over the keys and values together.  \\n Let's see an example of each of these  \\n For country in capitals,  \\n we loop over keys.  \\n So will for country in capitals keys.  \\n Note that capitals dot keys is not a list,  \\n but a special iterator object.  \\n We can make it into a list though  \\n by feeding it to the list constructor list.  \\n The other two dict loops are over values, dot values,  \\n and over keys and values together using tuple unpacking.  \\n Beginning in Python 3.6 for the C Python interpreter,  \\n and in python 3.7, for the very language definition,  \\n the order of insertion is preserved for dicts.  \\n This means then when we loop over the keys or the items,  \\n we get them in the order  \\n in which we originally added them to the dict.  \\n That was not the case in previous versions of Python  \\n and in fact, the standard library defined a special object  \\n called order dict to preserve that order.  \\n That is not necessary now.  \\n Last, I want to mention sets.  \\n You can think of them as bags of item,  \\n which can be of mixed types  \\n and which do not have duplications.  \\n For instance, the continents.  \\n Sets are written with braces, but without columns.  \\n You can see that Africa only appears once,  \\n even if we had it twice when we define the set.  \\n We can check if an item exists in a set.  \\n We can add items  \\n or remove them  \\n or loop over the set.  \\n but there's no way to do indexing.  \\n Sets and especially dicts are very important in Python,  \\n since they underlie many aspects of the language itself.  \\n For instance, the methods and attributes of classes  \\n are stored internally in dicts,  \\n and a dict key based interface is also used  \\n in many third party packages, including pandas.  \\n So it's very good to become familiar with them.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2295361\",\"duration\":284,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Comprehensions\",\"fileName\":\"2825705_02_04_XR30_comprehensions\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Python comprehensions provide a legible and expedient way to create, transform, and filter sequences, dicts, and sets; you use these extensively in chapter 3. In Python 3, comprehensions largely replace functional calls such as map and filter. In this video, review the comprehension syntax, including multilevel comprehensions, and draw analogies to Python loops.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7527162,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In Python, especially when  \\n you're dealing with data, there are many cases  \\n where you want to iterate over a list  \\n or a dict performing operation on every element  \\n and then collect all the results in a new list, or dict.  \\n You can certainly do that with a loop.  \\n For instance, picking up the example  \\n from the last video,  \\n let's compute the first 10 squares,  \\n starting with an empty list and adding  \\n elements in the body of the loop with append.  \\n This works, but we can do better.  \\n We can be more pythonic, that is,  \\n we can respect Python's specific style and spirit.  \\n Python offers a great feature, comprehensions,  \\n that let us write shorter,  \\n more easily readable code,  \\n that achieves the same effect as the loop.  \\n In fact, the comprehension  \\n is a compressed version of the loop.  \\n Let's go through the steps to write one.  \\n It's a list we want, so we have brackets.  \\n Next, we have the loop.  \\n And then we backtrack to the beginning of the expression  \\n and we write code for the computation  \\n that we want to collect.  \\n In this case, taking the square.  \\n The result is the same,  \\n but we managed to write it  \\n in a very readable and efficient way.  \\n We can also filter the list of elements that we are creating  \\n by adding an if clause.  \\n For instance, we may want to collect only the squares  \\n that are divisible by four,  \\n which in fact, I need to do with the modulus operator.  \\n Again, quick and readable.  \\n In Python three, comprehension largely replace  \\n the map and filter built-in functions,  \\n which are important and so called functional languages,  \\n but did not really belong in Python.  \\n The syntax for dictionary comprehensions  \\n is also rather intuitive.  \\n For instance, let's create a dictionary  \\n that will get us the square  \\n of an integer from the integer itself.  \\n It's a dictionary, so we need braces.  \\n The loop part is the same  \\n for variable and iterable.  \\n But now, instead of the list items,  \\n we need to write key colon value pairs.  \\n We can also add an if clause if we want.  \\n I don't need one here.  \\n Here is the result in dict.  \\n Dict comprehensions are sometimes used  \\n to transpose an existing dict.  \\n Going back to our capitals,  \\n which we wrote as a dictionary index by country,  \\n we can get the countries index by capital.  \\n In the comprehension, we loop over the dict items,  \\n so we get tuples of country and capital,  \\n and we invert them by writing capital colon country.  \\n Sometimes, you see what look like naked  \\n comprehensions without the brackets.  \\n Those are in fact generator expressions,  \\n which are useful when you want to generate a sequence  \\n and consume the elements one by one  \\n without ever storing them in a list or a dict.  \\n For instance, to take the sum of the first 10 squares,  \\n we may write the interior part of our comprehension  \\n without the brackets and feed it directly to sum.  \\n Doing this, saves memory and time  \\n which is important if you deal with large amounts of data.  \\n In fact, the built-in range which we used earlier  \\n to demonstrate loops does something very similar.  \\n It never builds a list, but it keeps  \\n adding new values to the loop.  \\n If you don't currently use comprehension,  \\n I'm sure that if you try them  \\n you'll become addicted quickly.  \\n And you'll start doing all sorts  \\n of acrobatics, such as nested looping comprehensions.  \\n For instance, look at this nested loop,  \\n which creates a list of one,  \\n one two, one two three,  \\n one two three four, and so on.  \\n We can do the same with a nested comprehension  \\n just by writing the two loops  \\n in the same order in sequence.  \\n Comprehensions are incredibly useful  \\n to manipulate lists, dicts, and data.  \\n You should be familiar with both,  \\n understanding and writing them.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2293498\",\"duration\":474,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Advanced Python containers\",\"fileName\":\"2825705_02_05_XR30_advanced\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"The Python standard library offers powerful extensions of the built-in container types, which can help you write shorter and cleaner code. In this video, explore namedtuple, defaultdict, and the powerful data classes introduced in Python 3.7.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12995764,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] As I mentioned in the video about sequences,  \\n tuples are similar to lists,  \\n but we cannot change the arguments after creating the tuple.  \\n The formal way of saying that is that tuples are immutable.  \\n Tuples are very useful to store data records  \\n when we think that we are not going to modify the values.  \\n For instance, a list of people with their first names,  \\n last names, and birthdays.  \\n For each element in this list, for each person,  \\n we access the data fields by index.  \\n Zero for the name, one for the family name.  \\n This lets us write useful list comprehensions.  \\n For instance, to find all the people  \\n with a given birthday.  \\n We loop over every person in People,  \\n and for each we check if the third element  \\n of the tuple, index two, is July 15.  \\n I share a birthday with a very famous astronomer,  \\n Jocelyn Bell Burnell.  \\n Field access by index means that  \\n we have to remember which is which,  \\n creating the potential for bugs,  \\n and certainly reducing the expressiveness of our code.  \\n To help us out, the module Collections  \\n in the standard Python library offers  \\n a name tuple that lets you create a specialized object,  \\n a specialized tuple, that has a name and that  \\n associates labels with fields.  \\n For instance, a person type would be called Person  \\n and have fields first name, last name, and birthday.  \\n We can then create instances of the Name tuple  \\n by using the type and giving the field values sequentially.  \\n Or we can even use the field names,  \\n which lets us shuffle the fields if we need to.  \\n Name tuples have the advantage that they print nicely.  \\n Now let's look at accessing the fields.  \\n Indices still work.  \\n So we get a zero, one and two gives me  \\n first name, last name, and birthday.  \\n But field names using the object-oriented DOS syntax  \\n are now what we're going to use,  \\n because they make our intentions clear in the code we write.  \\n Let's convert our little database to name tuples.  \\n We can't use a standard tuple directly  \\n and feed it person type, because person type  \\n needs three arguments.  \\n Here, Python is complaining that two are missing.  \\n So this is a case where tuple unpacking comes useful.  \\n I use a star to unpack tuple zero into its three elements.  \\n To do all of them, we will, of course,  \\n use a list comprehension.  \\n Now our birthday search reads better.  \\n It shows me explicitly that I'm looking at  \\n the birthday field of each tuple.  \\n Python 3.7 introduced an alternative  \\n to tuples index for storing data records, data classes.  \\n To use them, we need to import data class  \\n from the data classes module.  \\n If you are for some reason in Python 3.6,  \\n you can still use data classes,  \\n but you need to install them explicitly as a package.  \\n For instance, with PIP Install.  \\n So I do my import, and this is how we would  \\n set up a personal record with first name, a string,  \\n last name, a string, and a birthday,  \\n and say again a string,  \\n with the default value for the birthday.  \\n If you're not familiar with Python classes,  \\n class decorators, the At data class that appears at the top,  \\n and if you're not familiar with that annotation  \\n the syntax will look somewhat alien.  \\n But basically, we're instructing Python  \\n to create the type of records that will have fields  \\n called first name, last name, and birthday,  \\n all of which will be strings.  \\n We'll create an instance of this class, a person,  \\n making the fields sequentially or by keywords.  \\n You see here that the field of that known birthday  \\n is applied, since I didn't provide a birthday  \\n to the definition.  \\n We can access the field by name but not by index.  \\n And again, the data class prints nicely.  \\n In contrast to name tuple,  \\n data classes are full Python classes,  \\n so we can define methods that operate on the fields,  \\n such as a method that returns a person's full name.  \\n If you're not familiar with object-oriented  \\n programming in Python, do not worry.  \\n We will not need this feature in what follows.  \\n Nevertheless, for person class two,  \\n calling full name as a method runs the code that we wrote  \\n and returns my full name.  \\n Data classes have many other useful features,  \\n such as freezing, disallowing modifications,  \\n ordering, allowing the comparison of class instances,  \\n defining data classes by inheritance, and a lot more.  \\n So I encourage you to learn more about them  \\n if you are somewhat familiar with  \\n object-oriented programming  \\n and specifically classes in Python.  \\n Now we move on to our last topic  \\n about data extractions in Python.  \\n In a video about dict, I discussed how  \\n a special variant of dictionary,  \\n collections order dict, is now much less useful  \\n because the standard dict preserves the order  \\n in which elements were inserted.  \\n There's another variant of dict that remains useful,  \\n collections default dict.  \\n The point here is to define a default for values  \\n that will be returned if we ask for a key  \\n for which there was no entry.  \\n More precisely, we have to provide a function  \\n that returns that default.  \\n Let me write out the function that we will use.  \\n We'll call it mydefault, and it will return a simple string.  \\n So here's my default dict.  \\n If I go in and ask for a key that doesn't exist yet,  \\n I will just get the default back.  \\n Not only, that key will be now part of the dictionary.  \\n This makes default dict useful  \\n when we want to build a dictionary  \\n where each key can correspond to a list of items.  \\n Let's make an example based on birthdays.  \\n With standard dict, we need to write code  \\n that behaves differently if the birthday  \\n has been seen already, and then we can append  \\n to the list of people with the same birthday  \\n or if the birthday has not been seen,  \\n in which case we need to create a list with one element.  \\n That is quite inconvenient.  \\n The repetition of code is annoying and prone to bugs.  \\n We'll use the full dict instead,  \\n and we will take advantage of the fact  \\n that list called as a function returns the empty list.  \\n So we can use it to provide a default.  \\n Thus, we can write birthdays  \\n as collection_defaultdict  \\n with list as the default maker,  \\n and then simply look over person in our name tuple  \\n and constraint to get in that key the birthday  \\n and appending to the resulting list.  \\n There are more useful container types  \\n in the standard library module collections.  \\n I encourage you to look them up  \\n and to use them in your work instead of  \\n reinventing those wheels.  \\n \\n\\n\"}],\"name\":\"2. Data Structures in Pure Python\",\"size\":49694852,\"urn\":\"urn:li:learningContentChapter:2294388\"},{\"duration\":824,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2294375\",\"duration\":67,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Anagrams overview\",\"fileName\":\"2825705_03_01_XR30_overview\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this chapter, practice Python data structures with a simple example of textual data: listing and analyzing anagrams in the English dictionary. In this video, explore the basic technique\u2014signature\u2014that you use to find anagrams, and outline your general strategy to map the technique to data structures.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1734395,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In chapter two,  \\n we have reviewed Python loops,  \\n data containers, and comprehensions.  \\n Now we will set them to work  \\n in a simple, practical project,  \\n finding anagrams in the English dictionary.  \\n As you know, two words are anagrams of each other  \\n when their letters can be rearranged  \\n to turn one word into the other.  \\n For instance, elvis and lives.  \\n We will use this simple strategy to find anagrams.  \\n We defined a signature of a word  \\n as the sorted list of its letters, including duplicates.  \\n So the signature of post would be opst.  \\n And then we find that spot, stop, tops, pots, and opts  \\n have the same signature as post  \\n and therefore they're all anagrams of each other.  \\n We're going to make a Python dict  \\n of all the words in the dictionary indexed by signature.  \\n And then to find out if a word has an anagram,  \\n we will just compute the signature  \\n and look it up in the dict.  \\n Let's begin.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2294376\",\"duration\":250,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Loading a dictionary\",\"fileName\":\"2825705_03_02_XR30_loading\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In many cases, parsing a text file manually is the fastest way to complete a simple data-analysis task. In this video, load a list of words from a file, and briefly explore Unicode strings in Python 3, which allow you to handle international character sets transparently.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6615853,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] We begin.  \\n By loading a list of words from a file.  \\n Your exercise files.  \\n Already contain a list that we can use as an example.  \\n The file is words.txt  \\n and it sits in the same folder as this Jupyter notebook.  \\n That file is, in fact, the nineteen thirty four dictionary.  \\n That is distributed with many UNIX systems.  \\n If you wish, you can find a better one and use that instead.  \\n Now in Python.  \\n We talk of idioms  \\n to refer to code constructs.  \\n That have become the preferred way  \\n to achieve a certain goal.  \\n A classical example is looping through  \\n all the lines of a text files.  \\n To do so.  \\n We open the file for reading.  \\n Let's open  \\n with a mode of \\\"R\\\"  \\n and then, we can use the file as an iterable.  \\n In a fold loop.  \\n Which has the result  \\n or giving us the lines one by one.  \\n For the moment.  \\n All that we will do with each line,  \\n is just collect it in a list.  \\n What did we get?  \\n More than two hundred thousand words.  \\n Let's look at the first few using slicing.  \\n As we learned in chapter two.  \\n That's good.  \\n I see two problems though.  \\n Every word ends in the new line character.  \\n Denoted in \\\"C\\\" and in Python  \\n as backslash \\\"n\\\".  \\n Also some of the words are capitalized.  \\n Which will interfere  \\n with our scheme of computing signatures.  \\n We can fix both issues using Python string methods.  \\n To strip leading and trailing whitespace.  \\n Including new lines.  \\n We can apply strip.  \\n let's take our own for example.  \\n The new line is stripped away.  \\n Now to switch the entire string to lowercase.  \\n Who use the method lower.  \\n So now.  \\n There's something more interesting to do  \\n in the body of a loop.  \\n Will append to the empty list  \\n is stripped and lowercase version of each line.  \\n Now I see a duplicate.  \\n Which comes from \\\"a\\\"  \\n appearing both in uppercase and lowercase.  \\n One way to get rid of duplicates  \\n is to build not a list but a set.  \\n so I will do my loop once more.  \\n Replace the empty initial list with the empty set  \\n and replace a pen with ADD  \\n and given that the body of the loop is just one line.  \\n There is an even more  \\n idiomatic way of writing it.  \\n You probably guessed it already as a comprehension.  \\n The comprehension will have  \\n the expression I want to collect  \\n and then the loop over lines in the file.  \\n Here it is.  \\n If we wish to restore the alphabetical order.  \\n We can just wrap the set  \\n in the Python built-in sorted.  \\n Which produces a list.  \\n A list however without duplications.  \\n We are now ready to make Anna Graham's.  \\n By the way.  \\n If you want to try in a different language  \\n such as French.  \\n You can follow along what we did  \\n with the appropriate file.  \\n The good thing is that in Python three  \\n all strings are natively unicode.  \\n Meaning that they can handle  \\n international character sets transparently.  \\n The characters are encoded internally  \\n using multiple bytes as needed.  \\n They only care that we need to take  \\n is to tell Python which encoding to use for  \\n the files we read and write.  \\n There are multiple mappings  \\n between character sets and bytes.  \\n So we need to know which one was used.  \\n Your exercise files include a French dictionary.  \\n Written using  \\n the iso-eighty eight five nine encoding  \\n also known as latin one.  \\n Let's read all of its lines.  \\n In this case instead of a loop.  \\n We use the built-in method with lines.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2293499\",\"duration\":329,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Finding anagrams\",\"fileName\":\"2825705_03_03_XR30_finding\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, use Python comprehensions to create Python dicts that let you list anagrams and sort them in various ways. These examples show the flexibility and legibility of Python comprehensions.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9579381,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] We pick up our exercise  \\n where we left it in the last video.  \\n We have made a sorted list of lowercase words.  \\n lets load it up.  \\n Now, remember our strategy of comparing signatures.  \\n Those are the sorted lists  \\n of the component letters of each word.  \\n We need a function to make them.  \\n Taking the string N again as an example,  \\n let's see what happens if we sort it.  \\n Which we do with the built-in sorted.  \\n Indeed, we get a sorted list of the letters,  \\n which is already the signature.  \\n We can use it to verify say  \\n that Elvis is an anagram of lives, but not of sings.  \\n For convenience, we will collate the list  \\n of characters back into a single signature to string.  \\n The way this is achieved in Python looks a little strange,  \\n since we need to call the method join on a string  \\n that specifies the connector so to speak of the join.  \\n If it's a dash, we get a-a-n-o-r.  \\n So the connector we really want is the empty string.  \\n We're ready to make a function  \\n that performs this operation in general.  \\n Now, I remind you that our anagram finding strategy  \\n is to build a dictionary of words indexed by signature.  \\n Of course, we could also try a brute-force search  \\n that loops to the dictionary,  \\n computes the signature for every word  \\n and compares it with the signature  \\n of the word we want to anagram.  \\n That's what find anagram does.  \\n This works and seems fast enough  \\n to see just how fast we can use the IPython magic %time.  \\n So 200 milliseconds, that is not a long time to wait  \\n but we become unbearable if we need  \\n to compute long lists of anagrams.  \\n So let's do this the smart way.  \\n As we said, we will build the Python dict  \\n of words indexed by signatures.  \\n In fact, the values in the dict will be set,  \\n that indeed, contain all the words with the same signature.  \\n We call it words by SIG.  \\n If you think about it,  \\n this is the perfect application for default dict.  \\n Which we introduced in chapter two.  \\n Since the first time that we meet the signature,  \\n we have to somehow produce an empty set  \\n and add to it, perfect.  \\n Perhaps we could perform one last filtering operation  \\n by removing signatures with a single word.  \\n After all, every word is an anagram of itself,  \\n but that's not very interesting.  \\n To do the filtering, we can use a dictionary comprehension.  \\n Remember, to iterate on both key  \\n and value we use dict items.  \\n Then with the clause to select non-trivial anagram sets  \\n will length greater than one.  \\n Excellent, we can allow the simple function  \\n find anagram first that looks up a word  \\n in the dict by signature.  \\n This works fine, let's see for my name.  \\n Nothing, I didn't really expect one,  \\n But perhaps we shouldn't get an error  \\n when no anagram is found just the empty set.  \\n To fix that, we'll use a try except close.  \\n And we'll catch the key error exception with  \\n accept key error and just return the empty set in that case.  \\n If you're not familiar with exceptions in Python,  \\n I encourage you to go look them up  \\n in the Python documentation.  \\n So let's try again to anagram my name and get the empty set.  \\n This new function is much, much faster.  \\n Now that we have set this machinery up,  \\n there are many interesting investigations we could do.  \\n For instance, how about finding the sets  \\n of anagrams with the longest words?  \\n We get that by sorting the signatures,  \\n which we get from keys applied to anagrams  \\n by SIG using length.  \\n So we use the sorted built-in,  \\n given a sorting key of Len and a sync for reverse order.  \\n These are the longest signatures,  \\n but to see the actual anagrams,  \\n we can wrap it in a list comprehension.  \\n The longest anagrams have 22 letters.  \\n Looking at this list though, I must say  \\n that these are compound medical words  \\n that are not too creative in anagrammatical terms.  \\n How about the set of anagrams with the most words?  \\n For this, we will sort the dict values instead of the keys.  \\n And again use Len as the sorting key.  \\n The two longest groups have 10 elements,  \\n though some of these words are not very easily recognizable.  \\n Well done, this completes our exercise.  \\n Next, let's put what you learned to good use  \\n and take on a challenge again about wordplay.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2294377\",\"duration\":49,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Palindromes\",\"fileName\":\"2825705_03_04_XR30_CH30_challenge1\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In the challenge, you are asked to find palindromic pairs in English, which are pairs of words that become each other when reversed. To do so, you can modify your anagram code.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1274685,\"solution\":false,\"welcomeContent\":null,\"challenge\":true,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (bright electronic music)  \\n - [Instructor] For your challenge,  \\n you should extend the anagram machinery  \\n that we built together to find all palindromic pairs  \\n of words in the English language,  \\n or at least, in our dictionary.  \\n That is, you should find pairs of words  \\n that become each other when we reverse  \\n the order of their letters.  \\n For instance, reward and drawer.  \\n That will also include true palindromes, such as radar,  \\n where the reverse of the word is the word itself.  \\n I'll give you a hint, to reverse a string,  \\n go back to what we learned  \\n about slicing sequences in Python.  \\n This challenge should take you 10 minutes.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2294378\",\"duration\":129,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Palindromes\",\"fileName\":\"2825705_03_05_XR30_SO30_solution1\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3805977,\"solution\":true,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Narrator] Here is my solution to the challenge.  \\n You were asked to find all palindromic  \\n pairs of words in the English dictionary.  \\n We start by loading our list of words  \\n with the one line of comprehension.  \\n We will explore the fact that if two words are palindromic,  \\n then they are also anagrams of each other.  \\n We will also need the code we wrote in this chapter  \\n to compute signatures  \\n and to associate words to signatures.  \\n In Python, there is no built-in function  \\n or method to reverse a string,  \\n but we can achieve it easily by slicing.  \\n The slicing step will need to be negative backwards.  \\n We will omit the slice start and stop  \\n to get the entire string.  \\n So for Mickela, we do a slice of colon, colon, minus one.  \\n We now look over all the word sets,  \\n one for each signature,  \\n and then overall pairs of words in the word set,  \\n checking if one of them equals the reverse of the other.  \\n It's a little annoying to write the loops,  \\n so that we are only checking the same pair twice,  \\n in reversed orders.  \\n One way is to check pairs on if they are sorted.  \\n Here's the list.  \\n It includes also the true palindromes  \\n where in order, reverse order,  \\n equals itself.  \\n But manual dis-sorts do not seem very common.  \\n I'm going to give you, also, more elegant solution.  \\n Using the stand a library module, intertools.  \\n Intertools includes an iterator combinations  \\n which return all combinations,  \\n say of two items,  \\n from a list of three.  \\n We can then simplify our solution,  \\n by again looping over word sets,  \\n and then by selecting pairs of words in the word set  \\n using intertools combinations.  \\n The code is cleaner and more expressive.  \\n But the solution is the same.  \\n \\n\\n\"}],\"name\":\"3. Wordplay: Anagrams and Palindromes\",\"size\":23010291,\"urn\":\"urn:li:learningContentChapter:2295370\"},{\"duration\":1412,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2294379\",\"duration\":195,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"NumPy overview\",\"fileName\":\"2825705_04_01_XR30_numpyoverview\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Whenever you need to manage large one-dimensional or multi-dimensional collections of homogeneous data, such as numerical arrays, you turn to the NumPy library, which provides speed and memory savings. NumPy is also a basic building block to interface with C or Fortran code. In this video, get introduced to the basic structure of NumPy arrays and the notion of dtype.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5335233,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this chapter, we introduce NumPy,  \\n a third-party package for Python  \\n that extends the language with multi-dimensional arrays  \\n that are fast, memory-efficient,  \\n and that can manage very large data sets.  \\n NumPy is an important part of the Python ecosystem.  \\n It has become a fundamental package for data analysis,  \\n and for any kind of mathematical application with Python.  \\n Let's talk about how NumPy arrays are different  \\n from Python containers, such as lists.  \\n Python variables are often described as labels.  \\n They are not little boxes in computer memory  \\n ready to receive a value.  \\n Rather, the values are independent objects  \\n with their own space in memory,  \\n and Python variables are labels or names  \\n that are attached to the values.  \\n So you can have more than one variable,  \\n referring to the same object.  \\n It's a very flexible mechanism,  \\n and it makes it possible to have lists and dictionaries  \\n with heterogenous elements.  \\n However, it's not very efficient when we need to  \\n deal with many values of the same type.  \\n In that case, you want to reserve space in memory,  \\n and store all the values side by side,  \\n and that's exactly what a NumPy array is.  \\n Organizing data in this way  \\n is both faster and more efficient in memory.  \\n It's also a necessary step if you want to  \\n interface Python with other languages,  \\n such as C or Fortran,  \\n which count on data being laid out in memory like this.  \\n In this slide, I'm showing you a schematic representation  \\n of a one dimensional and a two dimensional array.  \\n The actual data items sit side by side in memory,  \\n and they all have the same size.  \\n If there is one dimensional,  \\n we identify items by a single index,  \\n or two indices for a two dimensional array.  \\n The index ranges from 0 to one minus one,  \\n where N is the dimensional array.  \\n In the case of a two dimensional array,  \\n the dimensions can be different of course.  \\n Since, as we said, all the data items in an array  \\n need to have the same size,  \\n NumPy needs to be very precise about identifying data types.  \\n In fact, more precise than Python.  \\n While Python has just one type of integer,  \\n and one type of floating-point number,  \\n NumPY has several.  \\n NumPY identifies different types of integers,  \\n dependent on the number of bits  \\n that each of them takes up in memory,  \\n int8, int16, int32, and int64 the most common.  \\n There are also unsigned version of the integers.  \\n As for floating-point numbers,  \\n we have float16, 32, 64, and on some platforms 128.  \\n The most common is float64.  \\n There are other more special I-Types,  \\n complex numbers, booleans, true or false,  \\n bytes, unicode strings,  \\n for which you need to specify lengths,  \\n void, used for record arrays,  \\n and object, which is in effect a pointer  \\n to arbitrary Python objects.  \\n So let's see NumPY arrays in action.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2294380\",\"duration\":314,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating NumPy arrays\",\"fileName\":\"2825705_04_02_XR30_createarrays\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"There are various ways to create NumPy arrays, depending on your needs. In this video, learn to make empty arrays, to transform Python data structures into arrays, and to load arrays from files in various formats.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9152433,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] The easiest way to get  \\n a NumPy array is to load it from a file.  \\n NumPy recognizes several file formats,  \\n including, of course, simple text files.  \\n I have prepared one for you  \\n that describes a very well-known painting.  \\n The file is called monalisa.txt  \\n and it's included in your exercise files  \\n in the same directory as this notebook.  \\n Let's have a look at the content.  \\n We open the file with open in reading mode,  \\n and we use the readlines method  \\n to get all the lines of the file.  \\n So we see that we have 200 lines  \\n and that each line is a sequence of integers.  \\n NumPy loads the file without any trouble using loadtxt.  \\n The result is a two-dimensional array.  \\n If we display it, NumPy omits some rows and columns  \\n so it fits on the screen.  \\n We can query the object, the array,  \\n for the number of dimensions, which are two,  \\n the shape, which is 200 by 134,  \\n 200 rows by 134 columns,  \\n the total number of elements,  \\n and the type of element.  \\n In this case it's the very common 64 bit  \\n floating point number.  \\n Okay, so we have a two-dimensional array called monalisa.  \\n I wonder, is it an image that we can display?  \\n We can use the Matplotlib function imshow  \\n to display two-dimensional arrays as images.  \\n Although, we should perhaps use a better column map,  \\n such as gray.  \\n I've also prepared a colored version of the painting  \\n and I have saved it in NumPy's native binary format,  \\n which works across all platforms.  \\n The file is monalisa.mpy.  \\n This is now a three-dimensional array,  \\n where the last dimension is used to store  \\n red, green and blue components.  \\n Imshow understands this without any problem.  \\n We have lots of pixels, so we can make the image bigger  \\n by instructing Matplotlib to have a larger figure size.  \\n Five by eight refers to inches,  \\n although how that turns out in pixels  \\n depend on the resolution of your screen.  \\n We now know how to load the NumPy array.  \\n How about making one ourselves?  \\n The easiest way is to take a Python list  \\n or a nested list of lists and pass it to NumPy array.  \\n The data type is automatically set.  \\n And we can query the object as we did before.  \\n Another common way to create  \\n an array is to make an empty one.  \\n We give the shape and the data type.  \\n For instance, we can do a one-dimensional array  \\n a vector of length eight.  \\n Here d is just a shorthand for a float 64, an 8-byte float.  \\n We can do a two-dimensional array, a matrix.  \\n And we can query these objects for their metadata,  \\n and plot them on the same line.  \\n It is sometimes useful to make an array of zeros  \\n in the shape of another existing array.  \\n That's done with zeroes_like.  \\n Otherwise, we can make a really empty array.  \\n Here, the memory is allocated, but not even cleaned,  \\n so we get some nonsensical values.  \\n We can also create a regularly spaced array  \\n of number with linspace with specified extrema,  \\n for instance, zero and one,  \\n and the total number of elements, here 16.  \\n We can't show a one-dimensional array as an image,  \\n but we can certainly plot it with Matplotlib.  \\n Here, I will use a thick marker  \\n as specified by lowercase o.  \\n Instead of choosing the number of elements  \\n between two extrema as we did with linspace,  \\n we can use NumPy arange, which has the same convention  \\n as Python's built-in range.  \\n In this case, we have elements  \\n between zero and 1.5 spaced by 0.1.  \\n NumPy can also give us an array of random numbers.  \\n We just need to specify the shape.  \\n By default, we get numbers uniformly distributed  \\n between zero and one.  \\n If we plot them with color, they look suitably random.  \\n We could also use random.randint and NumPy random.randn  \\n to get either integers in a given range  \\n or normally distributed numbers.  \\n To close this video, let me show you  \\n how to save an array to a file.  \\n Np.save will create a cross-platform binary file.  \\n The file ending is conventionally .mpy,  \\n but it doesn't need to be.  \\n Numpy savetext, instead, will create a readable ASCII table.  \\n And if we load it, we see that it's all there.  \\n Now that we've created our arrays,  \\n let's see what we can do with them.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2294381\",\"duration\":320,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Indexing NumPy arrays\",\"fileName\":\"2825705_04_03_XR30_indexing\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In the analysis of large data collections, you often need to focus on subsets of data or to restructure its storage. In this video, learn how to select subarrays by specifying boundaries and strides\u2014slicing\u2014or by applying conditions to the data\u2014fancy indexing. Also, explore restructuring data\u2014for example, adding or removing axes\u2014and the distinction between views and copies.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9422227,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Let's see how we can access  \\n individual elements and ranges of elements in NumPy.  \\n We will demonstrate on our good old friend Mona Lisa.  \\n So let me load up the file.  \\n I remind you, this is a three-dimensional NumPy array  \\n with dimensions that correspond to  \\n height 1198 pixels with 804,  \\n and color three, for red, green and blue.  \\n Imgshow shows us the picture.  \\n The syntax to get that individual pixels  \\n is just an extension of Python list indexing,  \\n except that we can now include  \\n multiple indices among brackets.  \\n For instance, a point roughly in the middle  \\n would be on row 600, column 400  \\n and we grabbed the red component.  \\n If we wish to go to the bottom right corner,  \\n we may count back from the boundary of the array,  \\n just that we would do for a list.  \\n This should be the same as  \\n 1148, 754 and one.  \\n If we try to index elements beyond the boundary,  \\n we get an index error.  \\n And of course, we can use indexing to assign values  \\n to the elements.  \\n Once you get used to multi-indexing like this,  \\n you'll have the temptation of trying it  \\n on nested Python lists,  \\n but there it doesn't work.  \\n So let me demonstrate with a rather uninspired list.  \\n We cannot ask for element one comma two,  \\n rather, we need to ask for list number one,  \\n and then element two.  \\n One more reason to like NumPy arrays.  \\n Slicing also works in a very similar way to Python lists.  \\n For instance, we could grab a section  \\n in the middle of the painting  \\n from rows 400 to 800, columns 200 to 600s.  \\n Here's the detail.  \\n Often we want to grab the entire range  \\n over one or more axes, in which case,  \\n we can use the shorthand column for the full slice.  \\n There's an even shorter hand  \\n for multiple full slices, ellipses sign.  \\n We can also specify a step,  \\n which has the effect of reducing the resolution  \\n of the picture  \\n because we've taken every 20th pixel.  \\n See the black dot in the middle?  \\n It's there because earlier we assigned zero  \\n to all three color channels for pixel 600 and 400,  \\n the single pixel was invisible at higher resolution,  \\n but it's one of those selected by this slice  \\n with steps of 20.  \\n How about slicing backwards?  \\n That works too.  \\n And in this case, it inverts the rows.  \\n And if we mix slicing and indexing,  \\n we reduce the dimensionality of the array.  \\n In this case, it becomes just a vector.  \\n We can show a vector as an image or we can plot it.  \\n Note that fixing one of the indices  \\n is not the same as asking for a slice of one.  \\n In that case, the array remains two-dimensional.  \\n Slicing can also be used on the left side  \\n of an assignment statement.  \\n We can use this to modify elements in bulk,  \\n such as deleting a square in the top corner of the image.  \\n Doing so, assign the same value to all the pixels there.  \\n But we can also match slices on both sides  \\n of the assignment.  \\n So let's replace the white square  \\n with a random set of pixels.  \\n NumPy arrays also support  \\n an especially useful form of indexing  \\n that is not available with lists.  \\n This is known as fancy indexing.  \\n It means that we're using arrays to index another array.  \\n To demonstrate that,  \\n let me grab my lower resolution grayscale image.  \\n I'm going to threshold this image  \\n by first figuring out all the pixels  \\n that are darker than a certain value.  \\n The result is a two-dimensional Boolean array,  \\n the same size as monalisa_bw.  \\n I can then use this Boolean array  \\n to select the corresponding subset of pixels  \\n and modify only those.  \\n Here's the thresholded image.  \\n Finally, let me point out another very important difference  \\n between lists and NumPy arrays.  \\n Whenever you slice a list, you make a copy of it.  \\n Say I have a simple list of six elements  \\n and I take a slice of the first four.  \\n Assigning to the slice does not modify the original list.  \\n By contrast, a slice of a NumPy array  \\n is a new NumPy object  \\n that points to the same area of memory,  \\n but we modified meta data that represents  \\n the different boundaries.  \\n So if I assigned to the slides,  \\n I'm also assigning to the underlying object.  \\n If we want a true copy pointing to new memory,  \\n you have to make the copy explicitly.  \\n Acting on the copy does not affect the original.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2293500\",\"duration\":321,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Doing math with NumPy arrays\",\"fileName\":\"2825705_04_04_XR30_math\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"With NumPy, speed and agility arise from the ability to operate on entire arrays at once. In this video, learn how to perform mathematical operations that transform arrays or combine them together, while preserving or modifying their structure.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9501463,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] NumPy is extremely useful  \\n in numerical calculations.  \\n That's because in addition to packing numbers  \\n efficient in memory, NumPy makes it easy  \\n to perform mathematical operations  \\n with entire arrays.  \\n For instance, in a study of mathematical functions,  \\n we may start with a vector of equally-spaced real values  \\n between say zero and five times pi.  \\n Here it is.  \\n Note that with the list space, the extreme of zero  \\n and five pi are included.  \\n Then we may want to complete the sine  \\n of all these values.  \\n We cannot do this with a function  \\n in the standard math library.  \\n Math dot sin.  \\n But we can with the NumPy version,  \\n which is called the universal function for this reason.  \\n It can operate on any array, in element by element fashion.  \\n There is always another NumPy array  \\n with the same shape as X.  \\n Using map dot lib, I can now plot sine X against X.  \\n Specifying first the coordinates along the horizontal axis  \\n and then the coordinates along the vertical.  \\n Map dot lib takes care of setting the Y range automatically.  \\n By repeating the plot statement,  \\n I can show multiple functions together.  \\n Map dot lib will automatically cycle through colors  \\n so it can distinguish the lines.  \\n So let's try our sine together with a cosine  \\n and then logarithm.  \\n We can add labels to remind us which is which,  \\n and then collect the labels in a legend.  \\n There are many more options in map dot lib  \\n regarding the style of the lines,  \\n the formatting of the plot, and more.  \\n You can look at the documentation to learn more.  \\n We can also perform operations  \\n that involve more than one array  \\n and everything goes smoothly if we match array shapes.  \\n So let's make the cosine in addition of the sine,  \\n and let's try combining the two.  \\n By contrast, operations between arrays of different shapes  \\n generally fail.  \\n NumPy doesn't quite know what to do.  \\n There is one important exception,  \\n which is known as broadcasting.  \\n NumPy, when it can, makes sense of operations  \\n between arrays of different dimensions  \\n rather than shapes.  \\n The simplest case, which is rather intuitive,  \\n is just to add a single number to an entire array.  \\n We see that W is offset by 1.5 with respect to sine X.  \\n The plot would also be self-explanatory.  \\n Let's go up to two dimensions  \\n and then we'll straighten our friend, the Mona Lisa.  \\n The image is 200 rows by 134 columns.  \\n I can multiply every column by a different number  \\n by making a vector of length 134.  \\n In this case, NumPy matches the second dimension  \\n of the array, with a single dimension of the vector.  \\n I want to show you the result  \\n side by side with the original image.  \\n So I need to do a little work with math dot lib.  \\n I will start with a larger figure size,  \\n figure and fix size,  \\n and then I set up one row  \\n with two plots using subplot.  \\n The arguments to subplot go one row, two columns,  \\n and then the subplot that I want to make.  \\n Multiplying the array in this way  \\n had the effect of applying horizontal gradient.  \\n What about the other way around?  \\n If I make a vector of length 200,  \\n you'd think we could apply it on the left  \\n to multiply every row by a single value.  \\n That however doesn't work.  \\n What works is to add a new dimension of length one  \\n to the gradient vector, which we can do  \\n with a special indexing notation, NP dot new axis.  \\n The shape of Y grad is now 200 by one.  \\n NumPy broadcasting then matches the first dimension  \\n of the two arrays, Mona Lisa BW and Y grad,  \\n and broadcasts the second dimension of Y grad  \\n to fill up the corresponding range in Mona Lisa.  \\n The result is the vertical gradient.  \\n NumPy supports many other useful mathematical operations,  \\n including fast fully transforms, random numbers,  \\n statistics, interpolation, and linear algebra.  \\n If you need any of them,  \\n you can go read the NumPy documentation.  \\n There is one thing I want to show you here.  \\n It's that since version 3.5,  \\n Python implements a special matrix multiplication operator.  \\n The at symbol, which is put to good use by NumPy.  \\n For instance, we can use it  \\n to make the dot product of two vectors.  \\n A and B, each of three elements.  \\n This is the same as writing NumPy dot dot AB.  \\n Or we could write the products of a three by three matrix  \\n in a three vector.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2293501\",\"duration\":262,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Special arrays: Records and dates\",\"fileName\":\"2825705_04_05_XR30_special\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"NumPy offers very convenient facilities for managing data of different types together\u2014record arrays\u2014and to handle dates with the datetime64 dtype. In this video, learn how to create and use record arrays and datetime64 data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7397157,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In the last video for this chapter,  \\n I want to show you two NumPy features  \\n that are not always covered in tutorials,  \\n but they're still very useful.  \\n One is record arrays, where we can mix different data types  \\n and give descriptive names to fields.  \\n The other is date time objects,  \\n which as the name says, can encode a date and time.  \\n Let us load up a simple example of a record array,  \\n which I have prepared in the NumPy binary format.  \\n This is a partial David Bowie discography.  \\n Each entry shows the record name, the date of release  \\n and the top rank in the UK charts.  \\n Let's look at the data type.  \\n It's a list which shows the name of each field  \\n and the actual D type.  \\n For title, it's U32, which denotes a Unicode string  \\n of maximum length 32.  \\n For release, it's M eight brackets D  \\n which denotes a date time object with granularity of days.  \\n It could be as small as a nanosecond in fact.  \\n The eight refers to the size in bytes  \\n of each date time object.  \\n Last, the top rank, is an eight byte integer data type.  \\n If you're wondering about the less symbol in each of these,  \\n those refer to the endianness of the data types,  \\n the order in which the bytes are stored in memory.  \\n Inter-processors are little endian.  \\n So what can we do with a record array?  \\n Each record looks like a Python tuple,  \\n and we can extract the elements as we would for a tuple  \\n but we can also modify them.  \\n We can also use a dict like interface  \\n using the field names in brackets.  \\n This, in fact, will also get us a full column.  \\n To create a record array,  \\n you have to be a little careful in specifying the D type.  \\n It's useful to go read about data type descriptors  \\n in the NumPy documentation.  \\n But let's try one.  \\n We specify a subset of the information  \\n we have in discography, just the title and release date.  \\n This array is empty right now,  \\n but we can copy over the two columns.  \\n Here we see that the title strings  \\n have been truncated to 16 characters.  \\n Since all the data is stored contiguously in memory,  \\n the lengths that we prescribe for the fields  \\n are very important and set the limit for the data  \\n that we can store.  \\n We see also that we specified a finer granularity  \\n for the date time object, seconds,  \\n although it's all zeros because the discography array  \\n didn't have that information.  \\n We move on to the date time object,  \\n which is called datetime64  \\n to avoid confusion with the object in the standard library,  \\n and also to remind us that each element takes 64 bits.  \\n We can initialize date time object with strings,  \\n and we can give it as much detail as we want,  \\n just the year, a full date,  \\n and date and hour minute combination, or even beyond that.  \\n Date times can be compared,  \\n so noon came before 6 p.m. on that day, like any day.  \\n Date times can also be subtracted,  \\n resulting in a time delta object,  \\n which here is specified in minutes.  \\n The nice thing about these date time objects  \\n is that they are understood across NumPy.  \\n For instance, we can use the NumPy function diff,  \\n which computes the differences between  \\n successive elements of a vector  \\n to see how long it took David Bowie  \\n to come up with a new record after each one.  \\n Ziggy Stardust was especially quick.  \\n Another example may be making a range of these.  \\n NumPy in a range understand date times.  \\n And we see that the last day is excluded  \\n consistently with the conventions of range and a range.  \\n This functionality is extended even further in Pandas.  \\n And in fact, the whole idea of record arrays  \\n has a much stronger implementation in Pandas DataFrames.  \\n We'll learn about those later in this course.  \\n \\n\\n\"}],\"name\":\"4. Arrays with NumPy\",\"size\":40808513,\"urn\":\"urn:li:learningContentChapter:2295371\"},{\"duration\":1606,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2295362\",\"duration\":64,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Overview of use case\",\"fileName\":\"2825705_05_01_XR30_weatheroverview\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this chapter, practice NumPy by loading, modifying, and plotting weather data from the National Oceanic and Atmospheric Administration. In this video, examine the files that you are going to analyze and outline your analysis goals.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2103321,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this chapter,  \\n we are going to experiment with NumPy  \\n on a real world use case,  \\n analyzing weather data from NOAA,  \\n the National Oceanic and Atmosphere Administration.  \\n The GHCN, the Global Historical Climatology Network Daily  \\n is an integrated database of daily climate summary  \\n from land surface stations across the globe.  \\n Many in big cities for instance.  \\n Climate summaries, in this case,  \\n means variables  \\n such as the minimum and maximum temperatures,  \\n the total precipitation, and so on.  \\n The data files that we will be using  \\n can be obtained from the NOAA website.  \\n Together we will download a station list  \\n and use it to locate temperature data for cities.  \\n We will load the data files, fill missing values,  \\n and smooth time series.  \\n Finally,  \\n we will create a visualization of daily temperatures  \\n inspired by the New York Times weather plots.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2294382\",\"duration\":372,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Loading station and temperature data\",\"fileName\":\"2825705_05_02_XR30_loading\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"NumPy allows you to load data in many different formats without writing custom code. In this video, learn how to use Python to download files from the web and use NumPy to load a fixed-width table.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14833489,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this chapter,  \\n we download several data files from the web.  \\n However, all the files that we analyze  \\n are also included in your exercise files,  \\n in case they became unavailable online,  \\n or you're unable to download them for any other reason.  \\n And before we load the data itself,  \\n it's also a good idea  \\n to start by looking at it's documentation.  \\n Browsing through the file listing at the NOAA website,  \\n we see a README file, and we start there.  \\n Instead of clicking on that link in our browser,  \\n let's use Python to download the file.  \\n There are several Python modules we could use,  \\n but for a simple download,  \\n the standard library module, urllib,  \\n is quite appropriate.  \\n urllib.request.urlretrieve needs the URL  \\n and the name of a local file.  \\n It's done already.  \\n We can use the Jupyter Notebook  \\n to look at the README file, by clicking on it.  \\n We see that it describes the contents of the directory,  \\n the format of DLY data files,  \\n which contain data for a single station,  \\n and are formatted with fixed-width columns.  \\n And the format of a file, GHCND stations,  \\n thank you, says the location, elevation, and ID  \\n for each station in the network.  \\n That's where we need to start,  \\n so we download that file with urllib.  \\n I've copied the description of the format  \\n into a text field in this Notebook,  \\n for our reference.  \\n To load a fixed-width text file, such as this,  \\n we can use NumPy genfromtxt.  \\n It needs rather precise information.  \\n We specify the width of each field  \\n in the parameter delimiter.  \\n We can derive the widths from the table above,  \\n or we need to increase them  \\n to include the spaces between the columns.  \\n Next, we provide the name,  \\n a descriptive name for each column.  \\n And we specify the dtype,  \\n the NumPy data type for each column.  \\n For the first field, for instance,  \\n we need a string of 11 characters.  \\n Then, we have three floats,  \\n and a few more strings of various lengths.  \\n And last, we'll tell NumPy  \\n to remove all the leading and trailing spaces  \\n from all the strings it parses.  \\n There result is a NumPy record array  \\n with more than a hundred thousand entries.  \\n Thankfully, Jupyter chooses to show us only a few lines  \\n at the top and the bottom.  \\n By plotting longitude against latitude,  \\n we get an idea of the impressive global coverage  \\n of the database.  \\n We need to make the dots small,  \\n so that they're not too crowded.  \\n Even so, the US and Europe are basically  \\n just solid masses of ink.  \\n How about stations in California?  \\n We can use fancy indexing with a Boolean expression,  \\n stations state equals equals CA  \\n to downselect our dataset.  \\n Coverage is still impressive.  \\n What if we need a specific station?  \\n Fancy indexing again comes to the rescue.  \\n We select all stations,  \\n for which it is true that the name field equals 'Pasadena'.  \\n We find one.  \\n If we want all stations that start with a given string,  \\n the syntax would be more esoteric.  \\n What np.char.find does,  \\n is to return the index at which a certain substring,  \\n 'Pasadena' in this case,  \\n appears in the field,  \\n or minus one if the substring is not there.  \\n If we required the index to be zero,  \\n that's the same as saying  \\n that the station name begins with 'Pasadena'.  \\n So, we see that there are several stations,  \\n a few of them in Pasadena, Texas and Pasadena, Maryland.  \\n Only one, however,  \\n is in the quality-controlled HCN network.  \\n Let's get that one.  \\n I've built this URL by looking at the structure  \\n of the directories on the website.  \\n You see that it ends in Pasadena's ID, USC00046719.dly.  \\n Locally, we'll download to the file Pasadena.dly.  \\n Let's look at that file.  \\n Again, with Jupyter Notebook.  \\n It's quite messy, but we recognize the station ID  \\n in the beginning of each line,  \\n followed by year and month.  \\n Here, we are in 1918.  \\n The name of an element, such as TMAX,  \\n and 31 data points, one for each day of the month.  \\n Each data point itself,  \\n consists of the value and three flags.  \\n We could use genfromtxt again,  \\n but it's going to take us a while,  \\n so, I prepared a small module for you, getweather.py,  \\n that takes care of parsing the file  \\n and returning consecutive daily values for a year.  \\n The module uses Pandas to clean and reformat data,  \\n but returns it as a pure NumPy array.  \\n After you've learned about Pandas later in this course,  \\n I encourage you to go look at getweather.py  \\n and see what I did there.  \\n We import the module  \\n and then we look at the help, known as docstring.  \\n This function does what I described,  \\n returning data for one year, for one station.  \\n Let's try it out on Pasadena.  \\n We request both TMIN and TMAX for year 2000.  \\n Some measurements are missing,  \\n and they are here represented as 'nan', nan, not a number.  \\n This function would be a great foundation  \\n for our work in the next few videos.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2294383\",\"duration\":310,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Filling missing values\",\"fileName\":\"2825705_05_03_XR30_filling\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Data often needs to be cleaned or otherwise edited before analyzing it. In this video, learn how to integrate missing data in the weather files you have loaded using a simple interpolation technique.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9559456,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] We pick up where we left  \\n and load temperature data for Pasadena, California  \\n using our getweather module.  \\n This is a time series, a sequence of values,  \\n organized chronologically, usually with equal cadence,  \\n that is the same time interval  \\n between every two consecutive samples.  \\n To get a sense of the data, one of them begins  \\n by computing its average value  \\n and perhaps its extreme, the minimum and maximum.  \\n With NumPy, we can use mean, min, and max.  \\n But wait, in this case, we seem to get NaNs.  \\n What's going on?  \\n It shouldn't be surprising, if we remember  \\n that the data contains missing values,  \\n which are indeed represented as NaNs.  \\n And NaNs can't really participate  \\n in any mathematical operation, NaN plus one is still NaN.  \\n In fact, how many do we have?  \\n The NumPy function isnan,  \\n creates a Boolean array of NaN-ness, so to speak.  \\n We can then count the instances of true  \\n in this array by using a neat trick.  \\n If we do arithmetics with Booleans in Python,  \\n they are converted to integers  \\n with false counting as zero and true as one.  \\n So, for instance, false plus true plus true is two.  \\n It follows that we can count the trues  \\n in a Boolean array just by summing it up with NumPy sum.  \\n So what can we do?  \\n Missing values are so common, in fact,  \\n the NumPy offers versions of its functions  \\n that simply ignore them.  \\n For instance, NumPy nanmin and nanmax.  \\n If we do need an uninterrupted series of numbers,  \\n we could just set the NaNs to the average of the column.  \\n This is yet another application of fancy indexing  \\n because we want to modify only the NaN elements.  \\n So we write something like Pasadena to mean  \\n fancy indexed to the true NaN Boolean mask  \\n is set equal to the NaN mean of the same variable.  \\n This works fine.  \\n We can tell which elements we changed  \\n because they have more digits than all the others,  \\n which were encoded with limited precision  \\n in the GHCN data.  \\n The integrated dataset can now be plotted  \\n without discontinuities.  \\n A more powerful approach to restoring missing values  \\n is to interpolate.  \\n That is, we can use neighbor values  \\n to compute a plausible number  \\n for the values that are missing.  \\n Let me demonstrate in a tall problem.  \\n Let's say we measure a function y,  \\n defined at integers x between zero and eight,  \\n but we don't have some of the values.  \\n In this case, we don't have value set x  \\n of two, three, and six.  \\n Let me show you all of this in a plot.  \\n I will now define an array of the integers  \\n at which we do want new interpolated values.  \\n I use NumPy linspace, so if I sat down  \\n and I said zero and eight, it's nine elements total.  \\n The function of NumPy interp takes as arguments  \\n the desired location, my xnew, followed by the data we have,  \\n x and y.  \\n It returns values that are interpolated linearly  \\n by fitting segments between existing data points.  \\n Here I'm plotting interpolated points as orange squares.  \\n This seems to make sense and to be rather conservative.  \\n The newx sequence, if you needed to,  \\n could well be the answer.  \\n 30 points between zero and eight.  \\n So let's use interpolation to fill missing values  \\n in the Pasadena temperature data.  \\n I need to load it again, since I fixed it already  \\n by replacing NaNs with nints.  \\n Here, now it's broken again, so to interpolate,  \\n we select the good data points, those that are not NaNs,  \\n and a tilde in this expression denotes logical notation.  \\n Then we make an array of the x-values  \\n for which we want interpolation.  \\n All days from one through 365,  \\n and then we can apply NumPy interp.  \\n This seems to work well.  \\n We celebrate by generalizing our Pasadena-centric code,  \\n so that it can fill up holes in any array by interpolation.  \\n It's just a question of replacing Pasadena  \\n with a generic array argument, and 365  \\n with the length of that array.  \\n Finally, we can plot interpolated temperature series  \\n in all their glory using our new function.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2295363\",\"duration\":386,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Smoothing time series\",\"fileName\":\"2825705_05_04_XR30_smoothing\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Many interesting data sets are organized as time series: numerical sequences sorted by date and time. In this video, learn how to use NumPy to perform basic time-series analysis tasks: computing means and standard deviations and smoothing time series.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11602546,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Now we know how to load  \\n temperature data from any station,  \\n how to compute basic summaries  \\n such as mean, min and max,  \\n and how to integrate missing data points  \\n using interpolation.  \\n We'll continue with more data analysis in NumPy.  \\n I've copied your fill NaNs function here  \\n since we will need it.  \\n We looked at data for Pasadena in the last video,  \\n now let's move to even sunnier skies  \\n by looking at weather in the town of Hilo,  \\n big island Hawaii.  \\n We use our custom loader  \\n and again, I encourage you to go look under the hood.  \\n This is data in fact from Hilo International Airport  \\n we now fill the missing data for both T min and T max.  \\n Once more two pole unpacking is very useful.  \\n Let's look at some data summaries.  \\n The yearly average which gives us a sense  \\n of the typical value for T min, and it's min and max.  \\n Will span the range of variation of these measurements.  \\n We can plot the summaries together with the time series.  \\n The map load live function adds each line,  \\n plots a horizontal line that spans the entire graph.  \\n Also useful for reference values,  \\n and we'll make them dotted.  \\n Another common way to measure the range of variation  \\n of a time series is to compute the standard deviation  \\n defined at the square root of the values.  \\n If you don't know about this you can go to statistics,  \\n text book, or to Wikipedia.  \\n Mean and variance are computed in NumPy with NP Mean  \\n and NP Var, we can plot the time series again  \\n using the mean and the mean minus  \\n and plus the standard deviation as references.  \\n Most of the time the temperatures are included in this range  \\n and given that this is Hawaii it's also interesting  \\n to look at precipitation.  \\n We grab those values with Get Weather and just plot them.  \\n The rainy season starting in November is quite evident.  \\n Now looking at the data this way is very informative,  \\n but we also see lots of noise, rapid variations between  \\n one day and the next, which can obscure underlying trends.  \\n To remove the noise, we can smooth the data.  \\n So that we see the slower long-term behavior  \\n below those still issues and the simplest approach  \\n to smoothing is replacing each value with the average  \\n of a set of its neighbors.  \\n With NumPy we can do this with correlate.  \\n I will demonstrate first with a very simple  \\n and short data set consisting of two peaks  \\n over a background of zeroes.  \\n I then define a roughly triangular correlation mask  \\n which is highest in the middle and drops down to the sides  \\n and I have arranged the values so they sum to one,  \\n next I write the correlation.  \\n What it does is to multiply each element  \\n in the series with the mask  \\n and then sum up all the resulting short series.  \\n Sliding them so they are centered around the sample  \\n that we multiply, thus as you see in this figure,  \\n each peak generates a triangle centered  \\n on its location.  \\n I have not explained the keyword same that we gave  \\n to correlate, what it does is to request that  \\n the output of the correlation be the same length  \\n as the input.  \\n Even as this means that the points on the boundaries  \\n get sums from fewer masks.  \\n So we may see some anomaly there.  \\n To smooth our temperature series we will use  \\n an even simpler mask, uniform values normalized  \\n so that they sum to one.  \\n We're going to get that in NumPy with NumPy once,  \\n let's try this out.  \\n We plot the regional temperature series as dots,  \\n maybe a little smaller than usual,  \\n and the smoothed series has a continuous line.  \\n This works fine, we are reducing oscillations  \\n while emphasizing the underlying slower trend.  \\n We do see something strange happening  \\n at the beginning and end of the data set.  \\n The values are going way low.  \\n That has to do with the keyword same,  \\n and we detect that the values on the boundaries  \\n have fewer neighbors than everybody else.  \\n If we can do without a few points at the end  \\n and at the beginning we can change our request  \\n to amply correlate to valid and avoid this problem.  \\n We can now make a function to apply smoothing  \\n of any length to any array.  \\n The length of the smoothing mask is sometimes called window  \\n and by default we'll use valid mode.  \\n We then plot T min and T max together for Hilo.  \\n It's an interesting plot  \\n and I'd like to see it for other cities,  \\n so once again let's take a code  \\n and generalize it to arbitrary station and year.  \\n So we get the data, we fill in the NaNs  \\n and we plot both the original data and a smooth version.  \\n We need to offset the plot a bit since we lose points  \\n with valid mode, so the range if the smoothing is over  \\n 20 days would just be from day 10  \\n to day 356 of the year.  \\n Let me try this out, for instance over multiple years  \\n for Hilo to see if the Hawaii climate is stable.  \\n Quite so.  \\n How about comparing cities in different climates?  \\n Let me look over Pasadena, New York, San Diego,  \\n and Minneapolis and then create a sub-plot  \\n in two rows and two columns for each one of them.  \\n I'll focus on the year 2000.  \\n There's some downloading and then we see the plots  \\n come out together, if you can choose based on weather alone  \\n where would you live?  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2295364\",\"duration\":303,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Weather charts\",\"fileName\":\"2825705_05_05_XR30_charts\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Using NumPy and matplotlib together, you can create insightful visualizations with little effort. In this video, learn how to compute daily temperature records and plot weather charts in the style of the New York Times.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9494132,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] We're going to conclude a NumPy practice,  \\n by making a quick but impressive weather visualization,  \\n that showcases the power  \\n and flexibility of NumPy and Matplotlib.  \\n It's inspired by the New York Times  \\n weather chart shown here.  \\n And it shows daily minima and maxima,  \\n the proper band in the context of their normal range,  \\n dark gray and of their records, light gray.  \\n We will again use Pasadena as an example,  \\n but you can do your own city,  \\n if it's included in the NOAA data set.  \\n Remember, we want to show Records,  \\n which means that we need all the data we can get.  \\n The Gateway, the module,  \\n that's a query for one year of data at a time.  \\n So, we'll call it repeatedly collecting the results  \\n in a comprehension and feeding that to NumPy vstack,  \\n which makes a two-dimensional array formally  \\n so one-dimensional arrays.  \\n The result can be visualized with matchshow.  \\n We've added also a colored bar to provide a reference  \\n of the mapping of values to color.  \\n And we have specified the extent  \\n to get more informative labels on the axis.  \\n We see some missing data, the white patches,  \\n and we can observe winter and summer nights getting warmer  \\n towards the end of the century.  \\n For simplicity, we will forego filling a nance  \\n and use NaN robust functions.  \\n We want record temperatures for every day of the year.  \\n This means that we can use NumPy nanmin on the demand data  \\n and specify axis equals zero,  \\n so that the minimum will be taken  \\n across all rows for each column.  \\n We do the same for tmax using NumPy nanmax.  \\n Let's see the records.  \\n Now for the normals.  \\n In the New York Times plot the normal temperature range  \\n for each day is defined as the average  \\n of the low and high from 1981 to 2010.  \\n So we build another stacked array with a reduced year range.  \\n And again, we take nan robust means, across rows.  \\n So that's axis equals zero.  \\n Here's the normal range.  \\n We're ready to get the plot together.  \\n We do Pasadena in 2018, just like the Times,  \\n so I'd better get that data.  \\n To plot the band,  \\n we use Micro-LIBS fill between which needs an x-axis  \\n to coordinate the array.  \\n It needs also the lower  \\n and upper lines that delimit the band.  \\n So for the x-axis,  \\n I will use the day of the year for one to 365.  \\n Here it is, I will do similar bands  \\n for the records and normal ranges.  \\n I also want to show the average temperature for the year.  \\n I have to compute that though.  \\n That would be the mean of the minimum temperature  \\n plus the mean of the maximum  \\n all across the year, divided by two.  \\n In 2018,  \\n that was 19.46 degrees Celsius.  \\n I'm going to put this average temperature in the title,  \\n so I need to build up a string for that.  \\n For that, I will use the very convenient formatted string  \\n literals introduced in Python 3.6.  \\n If I start the string with an \\\"F\\\" before the quotes,  \\n I can include variable names in braces,  \\n which are then replaced by their values.  \\n I can also specify formatting instructions as I would,  \\n using the string format interface.  \\n For instance, two decimal digits  \\n for the average temperature.  \\n Let me put everything together.  \\n I plot the three bands for record normal and current year.  \\n I have also looked up the red green blue,  \\n the composition of the New York Times colors,  \\n which Matplotlib bonds as values between zero and one.  \\n The optional step equals myth makes the band look blocky  \\n so the individual days are in evidence.  \\n And last, the alpha setting makes  \\n the current year band partially transparent.  \\n I set reasonable access limit and finally add a title.  \\n Let's fire it off, very nice.  \\n In 2018, some values are missing,  \\n we see the purple band is interrupted  \\n and those are actually the hottest days.  \\n So you can play with other cities we'll turn this example  \\n into a function that can plot any year in any station.  \\n So here's New York in 2018.  \\n NumPy is extremely powerful and flexible,  \\n so you should learn about it in depth.  \\n Coupled with Matplotlib it offers a direct route  \\n to beautiful and informative visualizations.  \\n In my course, Python Statistics Essential Training,  \\n explores statistical plots in more detail  \\n and with more examples.  \\n You are now ready for your challenge.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2293502\",\"duration\":46,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Weather anomalies\",\"fileName\":\"2825705_05_06_XR30_CH30_challenge2\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In the challenge, you are asked to make a visualization of temperature anomalies over the historical record by computing yearly averages and comparing to midcentury averages. To do so, build on the code that you developed in this chapter.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1250849,\"solution\":false,\"welcomeContent\":null,\"challenge\":true,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat chime)  \\n - For your challenge,  \\n I'm asking you to plot the temperature anomaly for New York  \\n by computing yearly temperature averages for each year  \\n and comparing those with a mid-century average  \\n in the 1945-1955 decade.  \\n You can make yearly averages  \\n just as we did for the title  \\n of the New York Times inspired plot.  \\n TMIN plus TMAX over two  \\n averaged across the year.  \\n To make the mid-century average,  \\n just sum up results for 1945 through 1955  \\n and then take the difference.  \\n This challenge should take about 15 minutes.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2293503\",\"duration\":125,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Weather anomalies\",\"fileName\":\"2825705_05_07_XR30_SO30_solution2\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4073114,\"solution\":true,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] We start by importing getWeather,  \\n since we need temperature data.  \\n It will also be good to show smoothed plots,  \\n so we'll grab the smooth function that we made.  \\n We need all the available historical data for New York,  \\n say 1880 through 2019,  \\n which we collect in a stacked array.  \\n We sum TMIN and TMAX for this large array  \\n and then take the average across the columns.  \\n So x is equal one,  \\n so that we get a value for each year.  \\n The shape isn't yet what we expect.  \\n Next, the mid-century average.  \\n We need to figure out the index of 1945 and 1955,  \\n in this all avg array.  \\n For lists, we can use index  \\n to figure out the location of an element.  \\n But unfortunately,  \\n that doesn't work for NumPy arrays.  \\n However, we can turn array into lists quite easily  \\n by feeding them to the list constructor,  \\n and then index works.  \\n So now we can perform the mid-century average  \\n over the correct slice of the array.  \\n It's 12.8 degrees.  \\n And here's the plot.  \\n The anomaly is all avg  \\n minus the mid-century mean.  \\n It's quite noisy,  \\n so let's smooth it out.  \\n If we use valid mode to avoid artifacts at the edges,  \\n we need to remove some years from the x-coordinate.  \\n The solution is complete really,  \\n but we can collect this code in a more general function  \\n so we can compare a few cities.  \\n So here's the comparison,  \\n with some complaints from NumPy  \\n by way of Jupiter  \\n that there's really no data before 1910 for Pasadena,  \\n and 1940 for Minneapolis.  \\n Still, the upward trend for all these three locations  \\n is quite evident.  \\n \\n\\n\"}],\"name\":\"5. Use Case: Weather Data\",\"size\":52916907,\"urn\":\"urn:li:learningContentChapter:2295372\"},{\"duration\":1258,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2293504\",\"duration\":68,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"pandas overview\",\"fileName\":\"2825705_06_01_XR30_pandasoverview\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"pandas' powerful table objects, DataFrames, are extremely useful in the analysis of structured data that associates textual, date, and numerical information. In this video, you are introduced to the basic structure of pandas series and DataFrames, and compare them with NumPy arrays.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1758373,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Pandas has gained broad acceptance  \\n in the Python community  \\n as the data analysis tool for Python.  \\n As of January 2020,  \\n it should reach version 1.0 very soon,  \\n signaling the stability of its API,  \\n its programming interface.  \\n Pandas is built on top of NumPy so it's very fast.  \\n And it extends NumPy in ways that are extremely useful  \\n to data analysis.  \\n For instance it attaches labels to table columns and rows.  \\n It lets us access data using indexes  \\n built from any variable.  \\n It allows us to modify table structure  \\n by adding and dropping columns  \\n and by performing other transformations.  \\n It recognizes many common data formats.  \\n It handles missing data easily.  \\n It implements database operations such as joins  \\n and it can even make plots.  \\n So if you want to do data analysis  \\n or data science with Python,  \\n I really recommend that you learn Pandas.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2294384\",\"duration\":395,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"DataFrames and Series\",\"fileName\":\"2825705_06_02_XR30_dataframes\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"To be efficient in data analysis, you need to ingest data sets in many formats, reorganize them into usable tables, and select subsets of their rows or columns. In this video, learn how to create DataFrames from Python data structures or from files; to inspect DataFrames; to extract and modify their columns; and to select data based on conditions\u2014the pandas version of fancy indexing, and the query method.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12008064,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] The two key objects in pandas  \\n are the DataFrame and the Series.  \\n A DataFrame is basically a table of data.  \\n Each column has a name and an assigned data type as a NumPy.  \\n In addition though, the DataFrame has an index,  \\n which is not necessarily the ordinal number of the row.  \\n In this example, where the columns contain name,  \\n date of birth and city, the index could be  \\n the social security number or an alphanumerical employee ID.  \\n A Series is effectively a single column  \\n from a DataFrame with its own index.  \\n Having an index makes it more powerful  \\n than a simple NumPy array.  \\n For instance, if we have two time Series  \\n that have partially overlapping indices, times,  \\n we can still combine them and pandas will figure out  \\n which entries it can actually compute.  \\n Just as for NumPy arrays the easiest way  \\n to get a pandas DataFrame is to load it from a file.  \\n And pandas can read and write  \\n an even larger variety of formats than NumPy  \\n These include ASCII tables, json, Excel,  \\n the hierarchical data format HDF  \\n using many scientific application, SAS, SAS, Strata,  \\n Big data storage formats, such as Apache Feather and Parquet  \\n SQL and even HTML tables, which is great  \\n if you want to scrape data from a website.  \\n In this table, I show you the formats  \\n with the pandas functions that read and write them.  \\n Some of them may require the assistance of another package  \\n that you need to install separately.  \\n We start with a simple text file.  \\n Nobels.CSV, which contains a list of Nobel Laureates  \\n with a year in discipline  \\n in which they were awarded their prize.  \\n We can have a look.  \\n This is quite simple, the values are separated by commas.  \\n Pandas read CSV with this without breaking a sweat.  \\n We do need to provide names for the columns  \\n since the file itself doesn't have that information.  \\n Read CSV has many other options  \\n including specifying separators other than commerce,  \\n skipping columns or rows, converting dates and more.  \\n Let's look at the DataFrame.  \\n The method Info gives us basic information.  \\n And the method Head prints the first few rows,  \\n Tail the last.  \\n We have a total of 950 records.  \\n And we see that indeed the columns are named year  \\n discipline and the Nobelist.  \\n The data types are integer for the year  \\n and Python object for discipline and Nobelist.  \\n That's an important observation while in NumPy,  \\n we represent strings as fixed with runs of characters.  \\n In panda strings are effectively the immutable strings  \\n of Python, which are more versatile.  \\n As I mentioned, the index plays a very important role  \\n for data frames, but this one in particular,  \\n is a boring Numerical index.  \\n To grab the individual columns, which become Series,  \\n we use a dict like syntax with brackets,  \\n or a class like syntax with dot.  \\n Either way, the result is a pandas series,  \\n the object there represents a column.  \\n Consequently, the series has a name, ID type,  \\n and it carries with it the index of the original DataFrame.  \\n If we need a naked NumPy array of the values,  \\n we can still get it with dot values.  \\n Here's a small slice.  \\n Sometimes it's useful to get a list or really a NumPy array  \\n of all the unique values in a column.  \\n Other times, it's useful to have counts of the times  \\n each item appears.  \\n This accounting confirms that three scientists  \\n were awarded two prizes.  \\n To select records, we can use fancy indexing,  \\n building a Boolean expression that involves the columns.  \\n For instance, select the Nobels in Physics.  \\n Or you can use the convenient and fast Query Interface  \\n which takes a logical expression given as a string.  \\n We have to mind our quotes here making sure that we use  \\n single quotes for the query  \\n and double quotes for any values inside it.  \\n Sometimes it's not evident how to write  \\n a filtering operation.  \\n For instance, if we seek all the Nobelists  \\n whose name contain Curie, we like to write  \\n something like this, Nobel's fancy indexed by Curie  \\n in Nobel's Nobelist but that fails quite spectacularly.  \\n Instead we need to dig down in the string methods  \\n supported by the series to find Contains,  \\n which does what we need.  \\n The selection confers incredible winning streak  \\n of the Curie family.  \\n Marie her husband Pierre,  \\n her daughter Irine and her son in law Frederic  \\n Be sure how to create a data frame yourself.  \\n First from a NumPy record array.  \\n For instance, the David Bowie discography  \\n that we use as an example in chapter four.  \\n This is almost like cheating because NumPy record arrays  \\n are in fact the very back end of pandas.  \\n Nevertheless, the fixed length strings  \\n are converted to Python objects, when we get a DataFrame.  \\n Starting from scratch, we may build a DataFrame  \\n from a list of dictionaries, which means  \\n that we'll be repeating the column names over and over.  \\n Or from a list of tuples by providing names for the columns  \\n here title and top rank.  \\n Name tuples would also work great here.  \\n Otherwise, we can build the DataFrame in the other direction  \\n from a dictionary of vectors or lists.  \\n Here I'm just copying information  \\n from the NumPy record array to make actual lists.  \\n And then I feed them to DataFrame.  \\n We have DataFrames, let's move on to using them.  \\n Now that we know how to load or make DataFrames,  \\n let's do something interesting with them.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2293505\",\"duration\":349,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Indexing in pandas\",\"fileName\":\"2825705_06_03_XR30_indexing\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Once you have your data organized, you may need to find the specific records you want. In this video, learn how to index DataFrames with NumPy-like indexing, or by creating indexes. Also, explore powerful but confusing MultiIndexes.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10651500,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] We have seen how to load  \\n and create dataframes  \\n and how to select records based on boolean conditions  \\n both with fancy indexing  \\n and with a string expression-based query interface.  \\n Now we'll see how we can make selections even more directly  \\n and more efficiently using indices.  \\n So let's load up our Nobel list data set again.  \\n The index is currently the simplest possible  \\n just numbers from zero through 949.  \\n We elevate the years to serve as index.  \\n We do this with a set index method  \\n which does not work in place  \\n but other creates a new dataframe.  \\n Now the years appear as the index  \\n at the front of each row.  \\n And here's the index itself.  \\n This shows that in Pandas,  \\n indices do not need to have unique values.  \\n That's a feature, not a bug.  \\n It lets us select all records for a year, for instance,  \\n using the indexing notation  \\n and here things get a bit complicated.  \\n There are several ways to do indexing and slicing in Pandas,  \\n some equivalent to each other, some not.  \\n I'm going to show you the two interfaces  \\n that I think are the least confusing.  \\n To select all records for a given index,  \\n we use .loc followed by brackets, not parentheses,  \\n with the index value.  \\n For instance, 1901.  \\n We can also add a column name  \\n just as if we were selecting a cell in a numpy array.  \\n The result is a series.  \\n In addition to selecting individual index values  \\n .loc allows for slices.  \\n But in a break from Python new search,  \\n the range is inclusive of its end value.  \\n If we choose the years of the great war, 1914-1918,  \\n then 1918 appears in the selection.  \\n We are not limited to all micro-indices.  \\n We can set up a dataframe  \\n indexed by discipline, for instance.  \\n And it's always best to keep the index sorted  \\n which we do with sort_index.  \\n Then we can select not just individual index values,  \\n but also ranges, which again, are inclusive.  \\n So physics,  \\n or medicine through peace.  \\n Here there are 353 rows  \\n but Pandas is only showing us a few at the beginning  \\n and at the end.  \\n That's what the ellipses symbols mean in the middle.  \\n If you want numpy style indexing,  \\n looking only the progressive count of the records,  \\n you can have it with .iloc, again, brackets.  \\n Here we get the first 10 records, whatever they are,  \\n in the dataframe in its current order.  \\n But this is not the end of the story.  \\n Pandas supports multi indices.  \\n For instance, a dataframe indices nobelists by year first,  \\n and then discipline.  \\n It looks like this.  \\n The underlying index is a very complicated beast.  \\n But we can isolate the values set at two levels  \\n using, appropriately, get level values.  \\n Armed with this multi-index dataframe,  \\n we can select records by both year and discipline.  \\n Passing it two-pole to .loc.  \\n For instance, physics and 2017.  \\n Slicing however, can get complicated.  \\n Say we want chemistry prizes between 1901 and 1910.  \\n What we would like to do is to write a slice  \\n with a column in a two-pole.  \\n But Python doesn't allow that.  \\n How about we use the long-hand expression for the slice?  \\n Slice by indices start and end value.  \\n That's still confusing to Pandas  \\n because then it tries to use chemistry as a column name.  \\n What works is to request the set of columns  \\n explicitly in the .loc expression.  \\n In this case, all of them with a comma.  \\n Fancy indexing works also.  \\n Here, slice none means that we're taking all the years  \\n but we select a list of the disciplines.  \\n And again, all the fields.  \\n Finally, while multi-indexing is powerful,  \\n it can be confusing.  \\n And so you may have to resort to trial and error.  \\n Any selection that you make that way, however,  \\n you can also achieve without multi-indexing  \\n by using criteria on the values in the columns.  \\n For instance, chemistry prizes between 1901 and 1910.  \\n Where I have built a three boolean arrays,  \\n years are going to equal the 1901,  \\n years less or equal to 1910,  \\n and discipline equals equals chemistry  \\n and I have taken the logical end with the ampersand.  \\n Or again, with query,  \\n which lets me write the same selection  \\n in a more intuitive form.  \\n Note, however, that all these queries return new dataframes.  \\n So you cannot use them to modify values  \\n in the original table as you could do with the indices.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2293506\",\"duration\":446,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Plotting\",\"fileName\":\"2825705_06_04_XR30_plotting\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In data analysis, you often need to transform tables by applying operations to one or more columns. Visualizing the transformed datassets is also crucial to understanding them. In this video, learn about the basics of performing mathematical operations and of plotting with pandas.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14191300,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] We have seen how to load  \\n and create data frames, and how to select records  \\n or ranges of records, but we have not done much  \\n with the values in the tables.  \\n In many data analysis tasks, you're interested  \\n in running computations with the columns  \\n and then making plots.  \\n Let's try that with Pandas.  \\n We'll load the data set consisting  \\n of global population health and wealth statistics,  \\n from the amazing Gapminder website.  \\n Gapminder is a Swedish foundation created  \\n by the late Hans Rosling to promote a fact-based worldview  \\n and to fight misconceptions about global development.  \\n If you want to make plots in the Gapminder style  \\n internalize their data in some depth,  \\n you can try my LinkedIn learning course,  \\n Python Statistics Essential Training.  \\n In this video we'll do simpler things  \\n but still learn a lot.  \\n So we load the comma-separated file.  \\n And we see that the data set includes a number  \\n of global statistics.  \\n Each row identifies a country, a year,  \\n the corresponding region, and then they count us  \\n population, life expectancy, the percentage  \\n of children born alive who survive to age five,  \\n the average number of babies per fertile woman  \\n and the domestic product per person in 2011 dollars.  \\n We can ask Pandas to compute single statistics  \\n on all the fields.  \\n We do that with data frame describe.  \\n We see that there are almost 15,000 records  \\n in the data set for years ranging from 1800 to 2015.  \\n We can read off the minimum, maximum, mean,  \\n and standard deviation for all the fields  \\n as well as the 25th, 50th, and 75th percentiles.  \\n If you don't know about these don't worry.  \\n They give us an idea of the typical values  \\n and typical range of variation of a quantity.  \\n One of the points that Gapminder makes to great effect  \\n is that life expectancy improves with wealth,  \\n the Gross Domestic Product per person, GDP,  \\n and that the correlation is even clear  \\n if we look at the logarithm of the GDP per person per day.  \\n We don't have a column with that information,  \\n but it's very easy to compute it and to store  \\n it in a new data frame column.  \\n So we divide GDP per capita by the average number  \\n of days in a year, apply a non-pi log 10,  \\n and store that in a new column which then appears  \\n in the data frame.  \\n To see global trends as a function of time  \\n or to examine individual countries,  \\n it makes sense to index by year and then country.  \\n We will have two versions of the data frame  \\n with those two indices.  \\n Pandas has its own plotting interface  \\n which focuses on choosing the variables with which to plot.  \\n Say for instance we want to show life expectancy  \\n is a function of log GDP.  \\n So we select data for 1960 with dot log,  \\n and then generate a scatter plot of those two variables,  \\n it's sufficient to give the column names to Pandas.  \\n If we want to compare a more recent year in the same plot,  \\n we need to grab a map of lib axis object from the first plot  \\n and then pass it to the second.  \\n This is the kind of solution you find  \\n on http://www.stackoverflow.com.  \\n You should also change colors  \\n and label the two clouds of points.  \\n We see that in going between 1960  \\n and 2015, the dots flatten towards the top,  \\n towards higher life-expectancies.  \\n Thanks to significant progress  \\n in public health policy and practice.  \\n The trend is the same for other statistical indicators  \\n such as survival by age five.  \\n The data frame index by country lets us make easy plots  \\n of the chronological evolution of an indicator  \\n such as life expectancy for a country such as Italy.  \\n But the result is jumbled  \\n if we don't first sort the values by year.  \\n I'll do it here with sort values.  \\n The style of coding where I concatenate one Pandas method  \\n after the other is in fact quite idiomatic for Pandas,  \\n if not for Python.  \\n So when we say that Pandas speaks  \\n its own Python dialect.  \\n Here's a comparative plot for three countries.  \\n Again we grab the math.lib axis and pass them  \\n to the second or third plots.  \\n We see that Italy caught up with the United States  \\n in terms of life expectancy and China is coming close  \\n after the disastrous 1960 famine.  \\n Another interesting and important correlation  \\n was between fertility rate and survival to age five.  \\n To look at this question globally we can compute the average  \\n fertility rate over all records,  \\n that however doesn't mean much,  \\n since it misses data from many years.  \\n We can use the Panda's group by functionality  \\n to segment the data frame by year  \\n before computing the average.  \\n The result is a series indexed by year  \\n which shows the average fertility rate as a function of time  \\n and we plot it against age five survival,  \\n treated in the same fashion.  \\n The Pandas plot allows us to add a second axis  \\n on the right to show the range of the second variable  \\n that we plot.  \\n This shows forcefully that high natality  \\n is a consequence of infant mortality,  \\n and that women have many fewer children  \\n when they believe they will survive.  \\n On a small scale, we see that post 1950 baby boom.  \\n To gain even more insight we can create a pivot table.  \\n This segments the fertility means by both year  \\n and geographical region.  \\n Pandas is very happy to plot the resulting timelines.  \\n The drop in fertility came after the baby boom  \\n for Africa, America, and Asia.  \\n Europe was already low and decreasing  \\n since the beginning of the 20th century.  \\n And here's the corresponding plot for age five survival.  \\n Africa is now roughly where Europe was in 1950.  \\n Using Pandas plotting functions is the quickest way  \\n to make insightful statistical illustrations.  \\n But for maximum flexibility,  \\n you can always extract the values  \\n from the frames as non-pras  \\n and pass those to standard math.lib functions,  \\n which are fully customizable.  \\n There's a lot more you can do in Pandas,  \\n and there's a lot more to learn  \\n that I can cover in this course.  \\n I try instead to give you a feel  \\n for what is possible and practical.  \\n I encourage you to start working  \\n on a data set that interests you  \\n and to pick up more techniques and knowledge  \\n using the many resources available on the web.  \\n \\n\\n\"}],\"name\":\"6. pandas\",\"size\":38609237,\"urn\":\"urn:li:learningContentChapter:2293508\"},{\"duration\":1033,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2293507\",\"duration\":44,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Overview of use case\",\"fileName\":\"2825705_07_01_XR30_babyoverview\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this chapter, practice pandas by analyzing the popularity of baby names in the U.S. as recorded by the Social Security Administration. In this video, outline your analysis goals.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1315728,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] We are now going to apply pandas  \\n to an intriguing real world use case.  \\n We will analyze the U.S. Social Security Baby Name Catalog,  \\n which reports the names given to male and female newborns  \\n for every year since 1880.  \\n This is a very simple data set,  \\n but it's great fun to play with,  \\n and it has been mined, analyzed, and visualized  \\n in many publications and websites.  \\n Specifically, I will show you how to load it,  \\n how to track the popularity of a name across all years,  \\n and how to extract the 10 most popular names every year.  \\n Your challenge will be about finding  \\n the most common unisex names.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2295365\",\"duration\":196,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Loading data sets\",\"fileName\":\"2825705_07_02_XR30_loadingbaby\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Using Python and its libraries, you can gather and organize data very efficiently. In this video, download compressed archives from the web, use Python to open them, load their contents, concatenate them into DataFrames, and save the resulting well-organized data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5780382,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] You can download the Social Security  \\n name data set from their website.  \\n But I have also included the archive names.zip  \\n in your exercise files.  \\n We need to uncompress it,  \\n which we can do in Python using the ZipFile module.  \\n The interface is object oriented.  \\n You first create a ZipFile object,  \\n then called extract all in the current directory.  \\n That's the dot.  \\n Jupiter lets us browse the contents  \\n of the current directory with ls.  \\n We see that names.zip,  \\n unpacked into a directory with many text files,  \\n presumably one for every year.  \\n Let's have a look at one.  \\n We open it in read mode  \\n and print out the first few lines.  \\n It's a very simple comma separated format.  \\n Name, sex, presumably F or M,  \\n and then the number of babies born that year with that name.  \\n Pandas read CSV shouldn't have any problems.  \\n But we did do something wrong.  \\n The first record in the file, Sofia,  \\n was used to set the names of the columns.  \\n We need to provide the column names explicitly,  \\n since they're not in the file.  \\n We've already done this for the Nobel data set.  \\n Better.  \\n We will need to load all the tables  \\n and concatenate them in a single data frame.  \\n To avoid confusing data from different years,  \\n we can prepare the individual data frames  \\n by adding a new column that specifies the year.  \\n To do it on the fly, directly from the output of read CSV,  \\n by chaining a method,  \\n we can use data frame assign.  \\n In this case, a new column year appears with value 2011.  \\n Excellent.  \\n We managed to load the file in a one liner,  \\n so you can see that I'm going to use a comprehension  \\n to concatenate all the data frames.  \\n There are several things happening here.  \\n So let's look at this carefully.  \\n We loop over all the years between 1880 and 2018.  \\n We build up the file name using an f-string,  \\n and feed that to read CSV.  \\n We specify the column names,  \\n and we add the column that gives the correct year  \\n from the loop variable.  \\n Finally, we pass all the resulting data frames  \\n to pd concat, pandas concat.  \\n You see there are no brackets here,  \\n so this is in fact a generator expression,  \\n not the comprehension,  \\n which pd concat accepts quite happily.  \\n It's a very efficient way to build a data frame,  \\n and it's all there.  \\n Almost two million entries.  \\n The year range is what we expect.  \\n We can save the merged data frame using to CSV.  \\n We don't need to say the index  \\n and if we specify a file name that ends in .gz,  \\n the resulting file will be automatically compressed.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2295366\",\"duration\":260,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Comparing name popularity\",\"fileName\":\"2825705_07_03_XR30_popularity\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Organizing pandas DataFrames with the appropriate index is important to gain easy access to the records we seek. In this video, learn how to create a MultiIndex for your data set, sort it by index value, and compare the popularity of common and similar names by plotting the corresponding time series together. Parallel and stacked plots of timelines are often insightful visualizations.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7870708,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] We are ready to start analyzing this data.  \\n How we load and look  \\n at the combined data frame we just created.  \\n We want to examine the change in popularity of a name.  \\n So we need to reframe the data in a way,  \\n that will make this easier.  \\n We will use a multi-index.  \\n We will index the date on sex first,  \\n then name, and then year.  \\n And we will also sort the index.  \\n Getting the data for any given name  \\n is then a simple exercise  \\n of indexing with dot loc.  \\n For instance Mary,  \\n this series is ready to plot.  \\n Notice how Metro-lib automatically uses  \\n the index to set the x-axis.  \\n We see two peaks.  \\n At approximately 1920 and 1950.  \\n It probably makes sense also  \\n to consider the frequency of a name as a fraction  \\n of the number of babies born in a year.  \\n To get that, we can apply group by  \\n on the un-indexed data frame and take the sum.  \\n Then we can normalize Mary  \\n by all the newborns in every year.  \\n So as a percentage of all babies,  \\n Mary was actually more popular  \\n at the beginning of the 20th century.  \\n But there were altogether  \\n more Marys born in the 1920s and 50s.  \\n For simplicity, we continue with unnormalized counts.  \\n So let's make a generic function plot name  \\n to make a plot like this.  \\n In another function compare names  \\n to plot a few names together.  \\n You see that we pass a list of names through compare names.  \\n We look over the list called plot name for each of them  \\n and then add a legend, which is always good.  \\n Let's for instance compare Michael, John, David and Martin.  \\n Or for girls Emily, Anna, Claire and Elizabeth.  \\n It's already a popularity contest that we make in here.  \\n Another interesting investigations  \\n is to compare variance of the same name.  \\n For instance, there are two spellings of Claire.  \\n There's an older version Clara,  \\n and an Italian and Irish spelling  \\n for the pronunciation Ciara.  \\n Here's the plot.  \\n Notice how Metro-lib tries to put the legend out of the way.  \\n Claire is now dominant,  \\n but Clara is having a resurgance  \\n after having been the dominant variant  \\n at the beginning of the 20th century.  \\n We can make a slightly different cumulative  \\n or stacked plot that adds up the counts  \\n on top of each other.  \\n For that, we need a table of the frequencies  \\n for all the variance as a function of time.  \\n That would be an exercise in multi-indexing.  \\n We start by selecting all the Claires  \\n and then we can apply data frame and stack  \\n to move one of the index levels to a column name.  \\n In this case the year is level two of the multi-index.  \\n So the years become the columns.  \\n I must say that sometimes it's hard to make sense  \\n of what unstack does, so expect some trial and error.  \\n By contrast, unstacking level one  \\n would have made columns corresponding to the name variance.  \\n We can now try this stack plot.  \\n On the x-axis we will have the years  \\n just a simple range will suffice.  \\n And on the y-axis, the stacked values.  \\n This works but we see that the plot  \\n is truncated around 1974.  \\n That must be because of some of the Nans in the table,  \\n which do not lend themselves to be summed up.  \\n That's easy to fix  \\n with data frame fill and A.  \\n We'll replace the Nans with zeros.  \\n And here's the final plot.  \\n I have also added a legend,  \\n seeded by the labels given to stack plot.  \\n And I've restricted the x-axis,  \\n quite nice and informative.  \\n Except perhaps for the garish different colors  \\n chosen by map plot lib.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2295367\",\"duration\":263,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Yearly top ten names\",\"fileName\":\"2825705_07_04_XR30_toptennames\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In many cases, you need to sort and subset DataFrames based on values rather than indices. In this video, find the top ten names over a range of years, which involves several pandas operations that are broadly useful.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8221776,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this video,  \\n we will create yearly top 10s for male and female names.  \\n We load the data, and this time,  \\n we index it slightly differently, by sex and year only  \\n since we will need to compare all the names in the database.  \\n We now build up a query by chaining pandas methods.  \\n Getting males in 2018  \\n is a simple matter of a multiindex.log.  \\n Then we get the most popular names  \\n by sorting our number in descending order.  \\n Head gives us a top 10.  \\n In 2018, Liam was king, followed closely by Noah.  \\n How about girls?  \\n We index sex with f, and the year is still 2018.  \\n For girls, it was Emma and Olivia.  \\n If we to build a table of the top 10 names  \\n over multiple years,  \\n we should get rid of the index with Reset Index,  \\n and select the name, Column Only.  \\n I will make this into a generic function.  \\n As I was saying, in pandas,  \\n one sees this operator chaining style often.  \\n And we can make the code clear  \\n by giving each method it's own line.  \\n It's a good idea, however, to add comments  \\n to explain what you're doing,  \\n either for your collaborators or your future self  \\n since pandas can be obscure.  \\n To form a table,  \\n we collect the series for different years  \\n as the columns of the data frame, labeled by the year.  \\n So in effect,  \\n we're creating a data frame from a dict comprehension.  \\n For males, you can see Liam gaining popularity  \\n and Jacob dropping.  \\n For females, we see that Emma has been dominant  \\n for a few years while Sophia dropped.  \\n Olivia is there, waiting to take the crown.  \\n How about the popularity plot for the 2018 top 10?  \\n For a change, we'll use the query interface,  \\n which conveniently lets us use the values of variables  \\n with the Add Notation.  \\n So here, we're passing sex and name to the function.  \\n And we select in All The Records  \\n where the sex column equals the sex argument,  \\n and the name column equals the name argument.  \\n This is not very pythonic,  \\n but it's nice to have once you know about it.  \\n Once we have the records, we plot number against year.  \\n So let's loop over the 2018 female top 10 and plot each.  \\n It's interesting to see  \\n that all the top names seem to have surged quite recently  \\n except perhaps for Evelyn, which was popular in the 1920s.  \\n As for the male names, two are classics, William and James.  \\n Let's look at the others then.  \\n This is the list, and I will copy  \\n a subset of it to the loop over which I plot.  \\n So now, I'm excluding William and James.  \\n And I see that, yes, the other eight  \\n have also been recent discoveries.  \\n How about all-time favorites?  \\n We get there quickly by selecting females, say,  \\n grouping by name, summing the numbers,  \\n sorting by the values, and then taking the top 10.  \\n Mary is in fact the all-time favorite among girls.  \\n If we look at the popularity over time of these names,  \\n we see that they've gained their spots  \\n in the first half of the 20th century except for Jennifer.  \\n Now that given the structure of the all-time f data frame,  \\n I'm looping over the index rather than the value.  \\n Next, you'll be asked to use what you learned  \\n in a challenge about unisex names.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2295368\",\"duration\":63,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Unisex baby names\",\"fileName\":\"2825705_07_05_XR30_CH30_challenge3\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In the challenge, you are asked to identify unisex names and to plot their male/female distribution, by building on the code that you developed in this chapter.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1715937,\"solution\":false,\"welcomeContent\":null,\"challenge\":true,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (playful music)  \\n - [Narrator] For your challenge,  \\n I would like you to find a top ten unisex names  \\n and to plot their popularity throughout the years  \\n for both the male and female usage of the name.  \\n We will define unisex names as those for which  \\n the total number of boys and a total number of girls  \\n born across all years and within a factor of two.  \\n That means that you're going to compute  \\n the total number of boys,  \\n divide by the total number of girls,  \\n and verify that that's between 0.5 and two.  \\n Given the technical nature of some Pandas computations,  \\n I have some hints for you.  \\n Try using DataFrame.groupby().  \\n Take advantage of the fact that Pandas  \\n can execute a mathematical operation between two series  \\n even if they have different indexes.  \\n And finally, if you need to drop not a number,  \\n nan values, use DataFrame.dropna.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2295369\",\"duration\":207,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Unisex baby names\",\"fileName\":\"2825705_07_06_XR30_SO30_solution3\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6047873,\"solution\":true,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - As I mentioned already, in Pandas there are often  \\n several ways to get the same result.  \\n So if your results are similar to mine,  \\n but you get them in a different way,  \\n don't worry, your solution may even be better.  \\n We'll load our data set as usual.  \\n We need to compute the total number of boys and girls  \\n for a given name.  \\n This seems a good place to use group by,  \\n which lets us segment the data before applying  \\n an aggregation, in this case, the sum  \\n of the number of babies.  \\n So we use group by over sex and name,  \\n we select the number column and we take the sum.  \\n From this list with a multi-index,  \\n we can grab the males and females respectively,  \\n using dot lock.  \\n As you see, the two indices are going to be different.  \\n Epon doesn't appear in the females.  \\n Nevertheless, we can combine the two series  \\n and pandas will align the indices for us.  \\n The results would be none where either series  \\n doesn't have an element.  \\n For instance we check where the ratio between  \\n males and females is less than two.  \\n We can certainly get rid of those nones  \\n with drop in A.  \\n Now, remember the definition of unisex names  \\n as those with a ratio between .5 and two.  \\n This is a good expression for fancy indexing,  \\n and after we apply it,  \\n we see that 1660 names pass the test.  \\n Here, I've taken the index, because we don't actually need  \\n the ratio itself, but just the names.  \\n The next thing to do is to find the ten most common  \\n unisex names, so we sum the male and female counts  \\n using the unisex array to index our two totals.  \\n We sort the resulting series,  \\n and we cut it off at the top.  \\n Jessie seems to be the winner of this particular contest,  \\n followed by Reilly, Casey, and Jackie.  \\n Now to clock popularity, it's convenient to use  \\n a fully indexed data frame.  \\n We also remember to sort that index.  \\n Now we'll loop over the most common unisex names,  \\n which remember, are the index of the common series,  \\n and we plot by selecting male or female in the name.  \\n Matplotlib type layout helps improve the spacing  \\n of the subplots, while Jessie is the absolute winner,  \\n it appears to have fallen out of favor somewhat.  \\n Reilly is ascendant, but not for boys anymore,  \\n and Casey, which peaked around 1990,  \\n may be the most unisex name of all,  \\n with the male usage of the name tracking the  \\n female usage very closely.  \\n Great, we've been through a lot in this course,  \\n and I hope you've gained an understanding  \\n of what is possible with NumPy and Pandas and matplotlib,  \\n and more generally, with Python as a language  \\n for data analysis.  \\n I hope you'll go out to discover and learn  \\n even more and that you use Python happily  \\n in your every day work.  \\n \\n\\n\"}],\"name\":\"7. Use Case: Baby Names\",\"size\":30952404,\"urn\":\"urn:li:learningContentChapter:2295373\"},{\"duration\":58,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2294385\",\"duration\":58,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Next steps\",\"fileName\":\"2825705_08_01_LA30_NextSteps\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"There are many resources on the web that can help you master data analysis with Python and explore more advanced topics. This video suggests a few options.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12669699,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - Well done.  \\n Thank you for following along  \\n through some rather challenging material.  \\n You are at the start of your road to mastery.  \\n You can now move on to my more advanced courses  \\n on programming efficiently in Python  \\n and on statistics with Python.  \\n Back to the topic of this course,  \\n here are some resources that can help you learn more.  \\n You can find the answer to any question about Python  \\n or about NumPy, Matploylib and pandas  \\n on the official websites.  \\n These packages are also discussed in  \\n \\\"Python Data Science Handbook\\\" by Jake Vanderplas,  \\n a very insightful and no-nonsense textbook.  \\n If you get stuck, look for help  \\n in an internet forum such as Stack Overflow.  \\n You'll find that Python has a very helpful  \\n and supportive community.  \\n Many have said that the best thing about Python  \\n is that it's a language that makes you happy.  \\n I agree so have fun and do great things.  \\n \\n\\n\"}],\"name\":\"Conclusion\",\"size\":12669699,\"urn\":\"urn:li:learningContentChapter:2293509\"}],\"size\":310335127,\"duration\":9035,\"zeroBased\":false},{\"course_title\":\"pandas Essential Training\",\"course_admin_id\":4493047,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":4493047,\"Project ID\":null,\"Course Name\":\"pandas Essential Training\",\"Course Name EN\":\"pandas Essential Training\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"&lt;p&gt;pandas is an open-source data analysis library that provides high-performance, easy-to-use data structures, and data analysis tools for Python. In this intermediate-level, hands-on course, learn how to use the pandas library and tools for data analysis and data structuring with instructor Jonathan Fernandes. Take a deep dive into topics such as DataFrames, basic plotting, indexing, and groupby. To help you learn how to work with data more effectively, Jonathan guides you through a series of practical coding exercises that are based on the same large, public dataset.&lt;/p&gt;&lt;p&gt;Note: A basic working knowledge of Python is a prerequisite of this course.&lt;/p&gt;\",\"Course Short Description\":\"Discover how to work with the pandas library and tools for data analysis and data structuring.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":10223359,\"Instructor Name\":\"Jonathan Anand Fernandes\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Expert in Generative AI and Large Language Models\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2024-05-24T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"No\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/pandas-essential-training-24082178,https://www.linkedin.com/learning/pandas-essential-training-revision-fy-2024-q4\",\"Series\":\"Essential Training\",\"Limited Series\":null,\"Manager Level\":\"General\",\"LI Level\":\"Intermediate\",\"LI Level EN\":\"Intermediate\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Data Science\",\"Primary Software\":\"pandas\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"No\",\"Visible Duration\":11448.0,\"Visible Video Count\":42.0,\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":44,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5916304\",\"duration\":44,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Welcome to pandas\",\"fileName\":\"4493047_en_US_00_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Use live action take 1 3rd atttempt\\n\\n4:20 time\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":260,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2257025,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Two things I'm totally into, coding and Python\\nand anything Olympics-related.\\nThe Summer Olympics, I don't do snow.\\nI've come up with a great way\\nto share these two passions with you.\\nWe'll work with data using Pandas.\\nIt's this awesome data analysis library\\nbuilt on top of Python.\\nAnd get this, we'll be using an Olympic\\nmedal winners data set.\\nI'm super excited\\nto dive into Pandas using this Olympics data\\nand see what kind of insights we can uncover.\\n\\nI think this might be the closest I get to an Olympic goal.\\nIt's going to be a blast.\\nPut on your coding hats\\nand let's crunch some numbers.\\nOn your mark, get set, let's go.\\n\"}],\"name\":\"Introduction\",\"size\":2257025,\"urn\":\"urn:li:learningContentChapter:5915595\"},{\"duration\":1082,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2719581\",\"duration\":222,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Using Google Colab\",\"fileName\":\"4493047_en_US_01_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"exerciseFileUrl\":\"https://github.com/LinkedInLearning/pandas-essential-training-new-dataset-dupe-4493047\",\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":410,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"This course uses Google Colab. After watching this video, you will be able to use basic functions in Google Colab.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6854987,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] You're going to love this course,\\nwe'll have loads of fun.\\nAnd regardless of how experienced you are with Pandas,\\nyou'll learn something along the way.\\nWe'll be building on material from each section,\\nso you really want to work your way in order\\nfrom the start to the finish.\\nSo head over to the course page\\nand you should find a section\\nthat says exercise files on GitHub,\\nand a link that says get files.\\nYou'll end up on a page that looks something like this.\\nSo what we'll want to do is just go ahead\\nand copy this link.\\n\\nNow when you do any coding, you need an editor,\\nso that's somewhere you write your code.\\nNow I've chosen to use Google Colab\\nbecause you don't need to install anything\\nand you can just see your code and your comments\\nand any visualizations you might have,\\nall in the same place.\\nSo you'll need a Google account,\\nso this is the same one that you might use for Gmail.\\nSo let's head over to Google Colab.\\nSo that's colab.research.google.com.\\nAnd because we're getting our code from GitHub,\\nI'm just going to select GitHub\\nand paste in the link that I copied earlier.\\n\\nSelect search.\\nSo this is the notebook that we want,\\nPandas essential training,\\nand so go ahead and just open this in a new window.\\nAnd this is going to be the notebook\\nthat we'll be using for our time together.\\nNow what's really nice about Colab, as I said,\\nis it allows you to have code and commentary\\nand visualizations all in the same place.\\nSo let me go ahead and enter a little bit of code\\nup at the top.\\nNow each of these sections are called cells\\nand I can go ahead and just hover over here\\nand insert a code cell or a text cell.\\n\\nSo let me go ahead and enter a text cell\\nand I'm just going to type a little bit of text\\nand I'm going to say the following cell\\nhas the year for the first Olympics.\\nAnd then when I'm done with that,\\nand I can go ahead and enter a code cell\\nstraight after that.\\nAnd then I can say year = 1896,\\nwhich is the year of the first Olympics.\\nNow, in order for me to get any output from this,\\nI actually need to run the cell,\\nand so there are a couple of options.\\n\\nI can go ahead and select the play icon over on the left,\\nand you can see I'm getting a message,\\nthis notebook is being loaded from GitHub and so on.\\nSo I'm going to go ahead and select run anyway,\\nand it's going to go ahead and run this code cell for me.\\nNow you can see it didn't return anything to us\\nbecause I haven't said that I want to be able to see\\nwhat those results are.\\nSo I can say year = 1896, and then enter a new line\\nand just put the variable for year over there.\\n\\nAnd you can see I get the output 1896.\\nSo if I wanted to calculate the next Olympics,\\nI could go ahead and just say year + four,\\nand then run this code cell again.\\nAnd you can see I get the result, 1900.\\nNow if you want, you can go ahead and just run\\na single cell, or you can go ahead up to the menu,\\nselect runtime, and run all the cells.\\nAnd that's about all you need to know about Google Colab.\\n\\nThe most important thing to remember with Google Colab\\nis to run the cells in order.\\nSo sometimes things don't work\\nbecause you might have run a cell lower down in the notebook\\nbefore you've run something that's higher up.\\nSo all you need to do in that case\\nis to just select runtime and run all,\\nand this will run all of the cells in the notebook.\\nSo we've seen how we're going to use Google Colab\\nas our code editor for this course,\\nallowing you to write code and comments\\nin the same workspace.\\n\\nAnd we'll be using this for the rest of this course.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5912592\",\"duration\":100,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"What is pandas?\",\"fileName\":\"4493047_en_US_01_02_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":179,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"pandas is an excellent tool for data analysis. After watching this video, you will be able to describe the benefits of pandas.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2947014,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Presenter] Now, I'm assuming you're either\\na data scientist or a data analyst,\\nor you want to become one, and Pandas is definitely\\none of the tools you want to be good at.\\nYou can work with large data sets in a variety of formats.\\nSo, you've got spreadsheets and CSV files and databases\\nand all sorts of messy data sources\\ncoming at you from different directions.\\nOh, and Pandas got its name from panel data,\\nwhich is where you have data over multiple time periods.\\nNow, you can use Pandas to clean up, organize,\\nor manipulate data.\\n\\nSo you can focus on the fun stuff, like finding insights\\nand telling stories with your analysis.\\nAt its core, Pandas is all about working with tabular data,\\nso think of spreadsheets or SQL tables.\\nIt provides two main data structures,\\nthe series, like a single column,\\nand the data frame, like a table with rows and columns.\\nAnd Pandas can help you handle things\\nlike missing data or data in different formats,\\nbut that's just the beginning.\\nWith Pandas, you can slice and dice your data\\nin 100 different ways.\\n\\nSo you want to filter your data based on certain conditions?\\nEasy peasy.\\nDo you need to group and aggregate your data?\\nPandas lets you do that.\\nDealing with date and time data?\\nPandas is a pro at that too.\\nAnd the best part, Pandas integrates seamlessly\\nwith other Python libraries,\\nlike NumPy for numerical operations,\\nor Matplotlib for data visualizations,\\nand even machine learning libraries like Scikit-learn.\\nAlright, so we know that Pandas is great for data analysis.\\nYou can work with series, which is a column,\\nor a table, which is a data frame,\\nand it integrates with other Python libraries\\nfor visualizing data\\nor working with machine learning libraries.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5915594\",\"duration\":254,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Using pandas\",\"fileName\":\"4493047_en_US_01_03_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":364,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Like other Python packages, pandas comes with great documentation. After watching this video, you will be able to access pandas's documentation.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11445562,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] You've probably come across the saying,\\ngive a man a fish\\nteach a man to fish and you feed him for a lifetime, right?\\nWell, that's exactly what we're going to do in this video.\\nI want to show you how to help yourself\\nby looking up any documentation you might need\\nwhen working with Pandas.\\nAnd it's all about empowering you to find the answers\\nto your questions any time you need them.\\nNow, Pandas, like many other Python packages,\\nis under constant development and has great documentation.\\nNow, one of the best ways to learn Pandas effectively is\\nusing this built-in documentation as your starting point.\\n\\nSo let's head over to the\\nnotebook and let me show you what I mean.\\nSo the first thing we're going to do is go ahead\\nand install Pandas.\\nNow I'm going to be using the version here.\\nSo that's Pandas 2.02.\\nAnd the reason I've stuck with this version is\\nbecause there's always going to be changes\\nand development work to Pandas,\\nand I want to make sure that the features\\nand functionality that I demonstrate are ones\\nthat are available with this specific version of Pandas.\\nNow you can see that there's an error over here\\nand you can ignore it because all it means is\\nthat the version of Pandas\\nthat we are installing is different to the one\\nthat's currently available on Google CoLab.\\n\\nSo like with any other Python package,\\nall you need to do is import it.\\nSo we're going to import Pandas.\\nNow, what you'll often find is that people use aliases.\\nSo you use the alias PD for import Pandas,\\nand so you can say import Pandas as PD,\\nand then this means that any further time you use Pandas,\\nyou can just use the alias PD instead of having\\nto type Pandas.\\nNow, let's just go ahead and confirm that the version\\nof Pandas that we've installed is 2.02, which it is.\\n\\nNow when I do a DIR with PD, I get a list\\nof the methods\\nand the attributes that are available in the Pandas package.\\nSo you've got things like is now, JSON, normalize,\\nand so on.\\nAnd you can see that there's an awful lot of attributes\\nand methods that are currently available in\\nthe Pandas library.\\nNow, if you ever get stuck\\nand you want to get a little bit more details about a\\nspecific attribute\\nor a method, you can go ahead and do a help.\\n\\nAnd then within those brackets you can do PD dot\\nand the name of the method or the attribute.\\nSo in this case, what I'm looking at is trying\\nto get some help on the Read CSV method.\\nNow this is one way you can look up the\\ndocumentation for Pandas.\\nAnother great way\\nthat's available within Google CoLab is you do pd.read_csv.\\nSo that's exactly the same method,\\nand then you have a question mark at the end.\\n\\nNow, I really like this one\\nbecause what happens is I can get my documentation over at\\nthe right hand side of my screen\\nand then I can continue to go ahead and work,\\nand then I can reference the documentation when I need it.\\nSo what you have here is the PD.read CSV,\\nwhich is the method name.\\nAll of these over here are parameters.\\nNow if I want to get into the details of these,\\nI just need to scroll down.\\nAnd so for example, I've got file path or buffer,\\nand I can get the details of what is involved there.\\n\\nWhat's also really helpful is I have a doc string,\\nand a doc string is a one\\nor two liner that tells me what this method does.\\nSo in this case, you can see that\\nthe pd.read CSV reads a comma separated values\\nfile into a data frame,\\nand it also supports optionally iterating\\nor breaking of the file into chunks and so on.\\nNow, the other place where you might want to go\\nfor documentation is in the official\\nPandas documentation.\\n\\nSo I've provided a link over here\\nand you can see that we've got the API reference guide,\\nand you can look up all of the methods\\nor attributes as you need them over here.\\nSo let's head back to our notebook.\\nSo you've seen the documentation for Read CSV.\\nSo go ahead and find another couple of methods\\nand attributes and read the documentation and try\\nand figure out what they do.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5917325\",\"duration\":506,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Reading tabular data into pandas\",\"fileName\":\"4493047_en_US_01_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":654,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"This course uses the Olympics dataset. After this video, you will know how to download the dataset for this course.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":26218949,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] One of the reasons I started using Pandas\\nwas that as part of my job,\\nI received these massive spreadsheets\\nwith thousands of rows and columns\\nand I needed to analyze them.\\nBut every time I tried to open it in Excel,\\nmy computer froze.\\nThe game changer when dealing with large datasets\\nwas Pandas and the Read CSV Method.\\nSo let's head over to our Notebook\\nand let's download our dataset.\\nNow in Google Colab, I can go ahead\\nand run a couple of commands using the Unix or Linux Shell\\nby having a exclamation mark at the start.\\n\\nAnd so what this is going to do,\\nit's going to use the wget command\\nand that's going to pull this dataset down.\\nNow, let's say I wanted to upload my own dataset, right?\\nThen I can go ahead and run this command.\\nNow the reason that I have these hashes in front of this\\nis that these means that these are comments.\\nAnd so these cells don't normally need to be run.\\nSo for example, if I have my own dataset\\nand I want to upload a CSV file here,\\nI could run this command or this cell.\\n\\nSo I can go ahead here and select Run Cell.\\nAnd what I need to do here then is just choose a file\\nfrom my computer and I can go ahead\\nand upload this into Google Colab.\\nSo let me just go ahead and select Cancel Upload.\\nNow what I can do is I can go ahead\\nand see whether that file is in Google Colab.\\nSo in Linux, if you want to be able\\nto see your current directory, you can do an ls.\\nAnd you can see that the file that I've downloaded,\\nwhich is the Olympics CSV file from 1896 to 2004\\nis available over here.\\n\\nSo let's go ahead and take a look\\nat the Read CSV Method again.\\nSo you can see that I've got a couple\\nof different parameters over here.\\nAnd the first parameter here is filepath or the buffer.\\nAnd so what I can then do is I can provide the filepath\\nto the Olympics CSV file over here.\\nAnd that's exactly what I've done here.\\nAnd so this allows me to open the CSV file.\\n\\nAnd so let me just make a little bit more space for myself.\\nAnd you can see that this is the file that's been open.\\nNow you can see that there's a whole load of NaNs,\\nwhich means it's not a number and so on.\\nAnd this doesn't seem to be a very easy\\nto use file in the current format.\\nSo let's take a look at the original file\\nto try and figure out why it looks like the way it does.\\nSo you can see that we've got a line\\nthat says Summer Olympic Games 1896 to 2004 and so on.\\n\\nAnd then you've got a whole load of Unnamed-1,\\nUnnamed-2, and so on.\\nSo let's look at the original CSV file\\nto try and make sense of it.\\nAnd you can see that that first line corresponds\\nto the first line that we have over here.\\nNow all of those NaNs correspond to the spaces\\nthat you have over here in each of these different columns.\\nSo what we need to do is to try\\nand figure out a way to actually ignore\\nthe first five rows when reading\\nthat file into Read CSV.\\n\\nSo the other option that I have is instead of specifying\\nthe file name like I did,\\nI can actually specify the parameter name,\\nso filepath or buffer, and use that instead.\\nAnd you can see that we'll have exactly\\nthe same output as earlier.\\nNow, often what I'll want to do\\nis I'll want to actually store the information\\nfrom that file in a variable.\\nSo in this case, I've called it OO,\\nthat corresponds to Olympics.\\n\\nAnd what I'll often do when working with Pandas\\nis I want to pick a two or three letter code that corresponds\\nto a data frame that I'll be using.\\nAnd the reason I do this is because\\nI know that I'm going to be using that data frame a lot.\\nAnd so I want to make sure that the name\\nof the data frame isn't very long.\\nSo this is the CSV file and this has been stored\\nin something known as a data frame, which is a table.\\nNow you see that we've still got the problem\\nwhere we've got a whole load of these NaNs.\\n\\nAnd as I said earlier, what we'll want to do\\nis we'll want to get rid of the first five rows.\\nSo let's go ahead and take a look\\nat the Pandas documentation and let's try and see\\nif there's an obvious way\\nwhere we can ignore the first five rows.\\nSo I'm going to just go ahead and take a look\\nat the different parameters over here.\\nSo there's a little bit of a description over here.\\nSo it's filepath.\\nSo I know that I've already used filepath\\nand that's the path to the file that I'm uploading.\\nSEP seems to refer to a delimiter,\\nso that's not what I need.\\n\\nThe header corresponds to the row number\\nthat I'm using as a column name,\\nso that doesn't seem to be what I'm looking for.\\nSo if I scroll down a little bit\\nand just take a look at the different parameters\\nthat's available, I see something\\nthat's looks quite promising and that's a skiprows.\\nAnd you can see over here at skiprows it's saying,\\nthe \\\"Line numbers to skip or the number of lines\\nto skip at the start of the file.\\\"\\nAnd that's exactly what I want,\\n'cause I want to be able to skip the first five rows,\\nbecause if you remember from that CSV file,\\nI want to be able to skip these five rows,\\nbecause I'm not able to get any\\nuseful information from there.\\n\\nAnd this is not relevant to the dataset that I need.\\nSo I can use that as another parameter.\\nSo I've got the original parameter, which is the filepath,\\nand now I've got this additional parameter\\nand I say skiprows=5.\\nAnd this is stored in the variable OO.\\nSo if I then look at what's stored in the variable OO,\\nI've got a much better output here.\\nAnd I have the column names,\\nso that's Year, City, Sport, Discipline, and so on.\\n\\nAnd then I've got the different rows over here.\\nNow another thing I often like to do\\nis to provide the name of the file.\\nAnd instead of having to keep using that filename,\\nI just store that again in a variable\\nand I just call that filename.\\nAnd then the next time I want to open that file,\\nI just need to head over to pd.read CSV,\\nand then I just say filename over here,\\nand it'll automatically enter the name of that file.\\nAnd so I've stored that in the variable filename.\\n\\nAll right, so we've taken a glimpse at the Read CSV Method,\\nwhich is your go-to method for opening any CSV file.\\nNow this is a really powerful method\\nand we could easily spend a couple\\nof hours just looking at all\\nof the available options over here.\\nSo let me just show you what I mean.\\nRight, so within Read CSV,\\nyou've got all of these different parameters\\nthat give you a variety of options of what you want\\nand what you can do with that CSV file.\\nSo I've got two really quick exercises for you to do.\\n\\nSo go ahead and use pd.read CSV and try\\nand figure out what the parameter SEP does.\\nSo SEP or SEP is this one over here.\\nSo pause the video here,\\nand I'll let you know what SEP does in a few seconds.\\nSo what SEP is all about is what delimiter to use.\\nNow, CSV or the CSV file extension stands\\nfor Comma Separated Values,\\nbecause each of the components in the file\\nare separated by commas.\\n\\nNow there are other files that are separated by tabs\\nor semicolons and so on, and you can specify\\nthat you're not going to be using your default comma,\\nbut perhaps you're going to be using a semicolon and so on.\\nNow, for the second exercise,\\ngo ahead and upload one of your own CSV files\\nfrom your computer and see if you can read it\\nin using the Read CSV Method.\\nAnd try an experiment with some\\nof the other parameter options.\\nSo what you'll want to do\\nis you'll want to uncomment these two lines.\\n\\nAnd then go ahead and run this cell,\\nand upload a CSV file from your computer.\\n\"}],\"name\":\"1. Technical Setup\",\"size\":47466512,\"urn\":\"urn:li:learningContentChapter:5911598\"},{\"duration\":3224,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5910589\",\"duration\":503,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Get an overview of the data and displaying it\",\"fileName\":\"4493047_en_US_02_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":658,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Many times you want to get an overview of the data, including displaying all of the rows. After watching of this video, you will be able to display data in different ways.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":19596788,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Now one of the first things you'll notice\\nis that we've got this new cell over here.\\nAnd all I'm doing over at this cell is I am going\\nto go ahead and install Panda's version 2.02\\nif it's not already installed.\\nAnd then what I'm doing is I'm going to look for the file.\\nSo that's the Olympics 1896 to 2004 file.\\nAnd if it doesn't exist, I want to go ahead\\nand pull it down and then that's it.\\nAnd the reason I do this is because I don't expect you\\nto go through this entire course in one sitting.\\n\\nSo if you ever to come back to this course,\\nyou'll always be able to download the files that you need\\nby running this cell\\nbefore you go ahead and run that section.\\nSo let me go ahead and run this.\\nAnd let's go ahead and use our read_csv method\\nto store the values in oo.\\nAnd let's take a look at the first attribute\\nthat we're going to be looking at.\\n\\nSo it's oo.shape, right?\\nNow I use the word attribute\\nbecause it's very difficult to determine\\nwhether something is a method or an attribute, right?\\nSo if it doesn't have a bracket,\\nthen that means it's an attribute.\\nAnd so for example, shape is an attribute,\\nbut if it has an open and closed bracket,\\nthat means it's a method.\\nSo read_csv is a method, head is a method,\\ntail is a method, shape is an attribute.\\n\\nNow what's shape all about?\\nWell, let's go ahead and run the cell.\\nAnd you can see that the documentation tells us\\nin this doc string that this returns a tuple\\nrepresenting the dimensionality of the data frame.\\nSo what this means is that\\nthis specific dataset has 27,174 rows and 11 columns.\\nNow let's go ahead and take a look at the next method.\\nAnd so that's oo.head.\\n\\nAnd what the head method does\\nis it displays the first N rows,\\nand now by default you can see that it's five, right?\\nSo for example, if I wanted\\nto just display the first three rows,\\nI could just do oo.head and three,\\nand that's going to display the first three rows of my dataset.\\nAnd as you can imagine, we also have a tail, right?\\nAnd the tail will display the last five rows of my dataset.\\n\\nNow, head and tail are the most common ways of being able\\nto take a look at parts of your dataset.\\nBut what I prefer,\\nespecially if I'm working with a new dataset,\\nis to use oo.sample(5) or sample three,\\nwhatever the case might be.\\nNow, the reason I do this is especially if I'm working\\nwith a new data set,\\nI want to be able to try and see what I can expect\\nfor the different columns and the rows,\\njust so that I can get a better feel\\nof this specific new dataset that I'm working with.\\n\\nAnd so for example, you can see here\\nthat here I'm randomly given five rows from this dataset.\\nSo this is from row 14,595.\\nThis is row 14,842 and so on.\\nAnd so for example, if you want to be able to specify\\nand ensure that you're able\\nto get exactly the same rows every time\\nas part of your sample,\\nthen you can go ahead and set the random state.\\nSo if you could go ahead and look at the documentation\\nthat we have for random state, right?\\nAnd this, I can set a random state\\nand give it as a number like one,\\nand then every time I run the cell,\\nI'll get those same three or five rows.\\n\\nNow another method\\nthat you might be interested in is oo.info.\\nAnd this gives me information about how many entries\\nthat I have in my dataset.\\nSo I've got 27,174 entries or rows in my dataset.\\nAnd what's interesting about this\\nis that I've got a whole load of different columns, right?\\nAnd so I've got the column year,\\ncity, sport, discipline and so on.\\nAnd if you notice in the next column, non-null,\\nI don't have any missing information\\nbecause you can see that the information that I have here,\\nI've got 27,174 entries for the year,\\nfor the city, and so on.\\n\\nThe next column is the Dtype or the data type, right?\\nAnd so you can see\\nthat the year has a data type of integer,\\nwhich is what you'd expect, so int64,\\nand then you've got object\\nfor most of the other ones, right?\\nAnd that's because they correspond to text.\\nSo you've got city and sport and so on.\\nAnd then the final column,\\nwhich is position, has a data type of integer.\\n\\nLet's take a look at another entry, which is describe,\\nand you can see that describe\\nSo for example, you can get the mean values\\nfor the column, year, or position\\nand describe will give you these statistical values\\nfor the numeric columns.\\nNow, because we only have two numeric columns,\\nthat's why we're only able\\nto get information about the year and the position.\\nSo now that you've seen a couple of rows and columns,\\nI want you to pause this video\\nand in a couple of sentence,\\ntry and describe what this dataset is all about.\\n\\nAnd as soon as you're ready, I'll let you know\\nwhat I think this dataset is all about in a few sentences.\\nAll right, so this dataset is all about medal winners\\nin the summer Olympics from 1896 to the year 2004.\\nNow I know that there are both summer and winter Olympics,\\nbut because this dataset is all about the summer Olympics,\\nI'll just call this the Olympics dataset\\ninstead of stating it's the summer Olympics.\\nNow for each of the medal winners,\\nit lets you know which Olympics it is.\\n\\nSo the year, where it was held,\\nthe sport and the discipline\\nthat the Olympian took part in,\\nwhich country they represented\\nusing the Olympics three letter code,\\nwhat gender for the event.\\nSo was it only for males or females?\\nThe medal, so what medal the Olympian received.\\nWas it a gold, silver, or bronze?\\nAnd then finally, the position which corresponds\\nto the gold, silver or bronze medal.\\nNow before we wrap up this video,\\nlet me just show you really quickly how you can go ahead\\nand change the number of rows\\nand columns that you are able to view.\\n\\nSo I could use the get_option\\nand this will show me the maximum number of rows\\nthat is currently set.\\nSo by default it'll show you 60 rows.\\nBut if I wanted to be able\\nto see all of the rows for this dataset,\\nI can go ahead and set that to none.\\nAnd now I can view all of the rows of this dataset.\\nSo let me just go ahead and scroll all the way up.\\nAnd so you can see that I've got entries\\nfrom the 1896 Olympics in Athens\\nand I can scroll all the way down\\nand then make my way all the way down\\nto the entries from the 2004 game.\\n\\nSo I have the ability to take a look at\\nand to be able to view all of the rows\\nin my data frame if I want.\\nSo let me just go ahead\\nand set that back to the default of 60.\\nAnd I can also change the number of columns.\\nSo by default I can view 20 columns.\\nI'm going to leave it at that.\\nBy default, the width is 80 characters.\\nI'm going to increase that to 100\\nbecause you can see that some of the columns are quite wide\\nand I want to make sure\\nthat this is not going to overflow to the next row.\\n\\nAll right, so we've had a good overview\\nof what this dataset is all about\\nand how you can go ahead\\nand view different parts of this dataset as needed.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5910588\",\"duration\":291,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Select a Series (column)\",\"fileName\":\"4493047_en_US_02_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":365,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Another key data structure in pandas is Series. After this video, you will be able to describe a Series and select one from a DataFrame\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11322299,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] With pandas I have the option of being able\\nto select a single column or view a couple of columns.\\nSo let's head over to our notebook.\\nSo I'm just going to go ahead and run the first cell\\nand let's go ahead and get our data frame.\\nAnd let's view three random rows from this data dataset.\\nNow, if I take a look at the type of this variable, 00,\\nI can see that this is a data frame\\nand that corresponds to the entire table.\\n\\nNow, I can also view just one of the columns.\\nSo for example, if I just wanted\\nto view the column discipline,\\nI can just do an 00.Discipline within square brackets\\nand I can use a double quote,\\nor I could also use a single quote\\nand I'll get exactly the same response.\\nNow, what I can't do, and\\nso I'm just going to remove the comment over here,\\nis say discipline.\\nBut if the actual column name is a discipline\\nwith a capital D, use a lowercase discipline,\\nthat won't work.\\n\\nAnd you can see the key error is discipline,\\nbecause what we're expecting here\\nis a Discipline with a capital D.\\nNow if I take a look at the column discipline,\\nI can see that this is in fact a series.\\nNow, it just so happens that every time a series is added\\nto a data frame, the series\\nbecomes an attribute of that data frame.\\nAnd attributes in Python can be accessed by a dot notation.\\n\\nSo instead of having an 00\\nand in square brackets discipline,\\nI could just do 00.Discipline.\\nNow why would I want to do that?\\nIt just makes it easier and it just means\\nthat I need to type less.\\nSo I can also view all of the columns within this dataset.\\nAnd you can see that I've got all\\nof the column header names.\\nSo year, city, sport, discipline, and so on.\\nAnd let's take a look at athlete name.\\nSo I've got 00 and then in square brackets,\\nathlete name within double quotes.\\n\\nNow this is where the square brackets\\nand the dot notation vary.\\nIf my column name has a space in it,\\nthen I cannot use the dot notation.\\nSo if I was to go ahead and uncomment this and try\\nand view the column name, athlete name,\\nyou can see I get invalid syntax.\\nSo anytime a column name has a space, I'll need\\nto use the square bracket notation.\\nSo let's go ahead and take a look at a random three rows\\nand let's look at the method unique.\\n\\nAnd so what this is doing is for the column year,\\nit's giving me all of the unique values.\\nAnd of course this corresponds to all of the years\\nthat we had an Olympics game.\\nSo that's 1896, 1900\\nand so on, all the way up to 2004 in this dataset.\\nAnd I can go ahead\\nand check out the documentation for unique.\\nAs you can see that it returns unique values\\nof the series object.\\nAnd my series or my column name here is Year.\\n\\nI can do the same for Sport and this will give me all\\nof the unique sports within my dataset.\\nAnd then I also have something known as value counts.\\nNow, value counts for a series,\\nreturns a series containing counts of the unique values.\\nAnd so the resulting object will be in descending order.\\nAnd so you can see for example,\\nthat we have 2015 medals for the year 2000.\\n\\nWe have 1,998 medals for the year 2004 and so on.\\nNow, if I don't want to see the actual numbers,\\nbut I want to be able to normalize this, right?\\nAnd this is where I have the option\\nof having the relative frequencies,\\nI can provide the parameter normalize equals true,\\nand all of these values will add up to one.\\nAll right, so what we've been able to see here is to be able\\nto view or select a column from the data frame\\nand to be able to access it via a dot notation\\nor a square bracket.\\n\\nNow, if you're ever in doubt,\\nthen always use the square bracket\\nbecause you'll never run into any problems with that.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5912591\",\"duration\":55,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Fundamentals\",\"fileName\":\"4493047_en_US_02_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Needs Challenge Bumper\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":62,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1519100,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] I've put together some specific questions\\nabout this Olympics dataset.\\nNow this is an opportunity\\nfor you to get to know this dataset a little better.\\nNow you won't necessarily be able to answer\\nall of the questions just using pandas.\\nYou might need to use Wikipedia or a search engine too.\\nSo here are the questions,\\nso answer the following questions.\\nIndicate the pandas command where relevant.\\nSo what's the time range covered in this dataset?\\nThe Olympics takes place every four years.\\nWhy are there missing years,\\nand what are the types of medals awarded?\\nAcross all of the Olympic Games,\\nhow many gold, silver, and bronze medals have there been?\\nAnd why are there not an equal number of gold,\\nsilver, and bronze?\\nNow, you'll also notice\\nthat there are more gold medals than silver\\nand more silver than bronze.\\n\\nWhy might that be?\\nAnd what are the different NOCs\\nor National Olympic Committees?\\nAnd finally, what does the NOC ZZX represent?\\n\"},{\"urn\":\"urn:li:learningContentVideo:5911597\",\"duration\":376,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Fundamentals\",\"fileName\":\"4493047_en_US_02_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Needs Solution Bumper\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":612,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":17255572,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] Okay, let's head over to the notebook.\\nSo what's the time range covered in this dataset?\\nSo we can just use 00.Year.unique\\nand we can get all of the years.\\nThe second question is,\\n\\\"The Olympics takes place every 4 years.\\nWhy are there missing years?\\\"\\nNow, this is primarily because of the World War.\\nSo World War I broke out in 1914 and lasted until 1918.\\nAnd so that's why we don't have a 1916 Olympics\\nand the 1940 and the 1944 games.\\n\\nSo because of World War II, that lasted from September\\nof 1939 to September of 1945.\\nSo the next question is,\\n\\\"What are the types of medals awarded?\\\"\\nSo we can just use 00.Medal.unique\\nand you can see that we've got a Gold,\\na Silver, and a Bronze.\\nNow this is a really good data validation query here\\nbecause you want to make sure that the types\\nof metals are only going to be Gold, Silver,\\nand >Bronze, and nothing else.\\n\\nNext, \\\"Across all of the Olympics, how many Gold, Silver,\\nand Bronze medals have there been?\\\"\\nNow you'll want to use value counts here.\\nSo, 00.Metal.value_counts,\\nand you can see that there have been 9181 for Gold,\\n9014 for Silver and so on.\\nNow, \\\"Why are there not an equal number of Gold, Silver,\\nand Bronze medals?\\\"\\nNow, there are a couple of reasons for this.\\nSo there could be tide results in some events leading\\nto the award of multiple medals of the same type.\\nOr there could be team events,\\nso this is where all the team members receive medals,\\nand this can potentially skew the distribution based\\non the number of participants in these events.\\n\\nOr there could be disqualifications\\nand adjustments that need\\nto be made in the metal distribution.\\nSo this is after the events have concluded.\\nNow you'll notice that, \\\"here are also more Gold medals\\nand Silver and more Silver than Bronze.\\nWhy might that be?\\\"\\nWell, to answer this question,\\nlet me give you a couple of examples\\nand a couple of fun facts from some of the earlier Olympics.\\nSo if we take a look at the men's 100 kilometer cycling,\\nand we'll be able to see that over here\\nby just looking at the first 10 rows.\\nSo what I'm referring\\nto here is the 100 kilometer cycling event,\\nwhich is the first two entries over here.\\n\\nYou can see that we've got only a Gold\\nand a Silver, but no Bronze.\\nAnd if you wanted to get into some of the details of this,\\nyou can head over to Wikipedia\\nand you can see here that there were\\nin fact these two winners.\\nSo Leon Flameng and Georgios Kolettis of Greece,\\nand you can take a look at all of the participants.\\nNow, what's quite amusing is\\nfor this 100 kilometer cycle race,\\nthere were in fact only two winners\\nand the rest of the participants didn't finish.\\n\\nBut we also had a couple of people who helped\\nto act as pacemakers.\\nAnd for example, you have Paul Masson from France\\nwho was a pacemaker to Leon,\\nand we've got a couple of others who didn't finish.\\nNow these Olympics are full of fascinating facts.\\nSo if I take a look at Aristidis\\nand look at the entry in Wikipedia for his race,\\nthis is an absolutely fascinating view\\nof what actually happened.\\n\\nSo we can see there's an entry\\nfor his success in the 1896 race.\\nAnd what's really amusing is that the race was done\\nwith the help of pacemakers.\\nAnd some of the sources say\\nthat he finished the race on a pacemakers bicycle\\nbecause his bicycle had broken down.\\nAnd other sources say that he finished the race\\nwith a bicycle from a spectator.\\nSo there's obviously a whole lot of stuff\\nthat's taking place in these early Olympics\\nthat we would not see.\\nAnd as our final example,\\nlet's take a look at the men's 100 meter freestyle.\\nSo that's the Gold and Silver.\\n\\nYou can see that there is no Bronze metal winner here.\\nAnd let's take a look at what information there is\\non this particular race on Wikipedia.\\nSo this is really amusing.\\nSo for the Men's 100 meter freestyle Game,\\nso this is presumably a photo from the event.\\nThe men's 100 meter freestyle was one\\nof the four swimming events in the swimming\\nin the 1896 summer Olympic Games.\\nAnd the first and second winners were\\nfrom Austria, Hungary, which we know.\\n\\nSo that's Alfred Hajos and Otto Herschmann.\\nAnd what's really amusing about this race\\nis that the swimmers,\\nso this is again very different from the swimming\\nthat we have today.\\nThe swimmers were taken out by ship into the bay\\nwhere they'd swim towards the shore\\nand boys marked the starting line\\nand you were able to determine the end of the finish line\\nbecause there was a red flag there.\\nNow, what's also quite amusing is that we've got a couple\\nof swimmers who were entered into the race,\\nbut for whatever reason, we only have results\\nfor the Gold and the Silver.\\n\\nAnd so we don't really know what happened\\nto the rest of the swimmers.\\nSo the next question is about the different NOCs\\nor National Olympic Committees,\\nand we can get that easily by\\nusing the NOC column and unique.\\nAnd so for example, we have the three letter code\\nfor the National Olympic Committees.\\nSo we've got France, Greece, USA and so on.\\nNow what does the NOC \\\"ZZX\\\" represent?\\nNow, clearly we can't determine this from pandas.\\n\\nSo if we were to head over to Wikipedia,\\nwe can get some interesting historical facts\\nabout these Olympics.\\nSo the IOC grouped results under the mixed team designation,\\nso that's ZZZX, and this is where the early Olympic games\\nallowed for individuals in a team\\nto be from different nations.\\nAnd so we have an interesting scenario\\nwhere we've got teams from France\\nand England working together over here\\nand other examples in some of the other events.\\n\\nNow this data set is full\\nof other such interesting stories and history.\\nAs a final challenge,\\ngo ahead and explore this a little more.\\nIf you find some really interesting fact\\nabout this Olympics data set,\\ngo ahead and post it on LinkedIn and tag me.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5916303\",\"duration\":122,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Python lists and dictionaries\",\"fileName\":\"4493047_en_US_02_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":159,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3708291,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] If you want to take your pandas skills\\nto the next level, one of the things you'll want to do\\nis to make sure you've got the basics of Python covered.\\nSo medals here is a list,\\nand lists are ordered collections of things.\\nNow, we can create a new list by using square brackets,\\nand inside those square brackets,\\nwe put each of the items that we want our list to contain,\\nand they're all separated by commas.\\nSo you've got Gold, Silver, Bronze.\\nNow, I can confirm that this is a list\\nby using the method type.\\n\\nAnd you can see that I've got\\nconfirmation that this is a list.\\nNow, list can contain any type of object.\\nSo each item in a list doesn't need to be of the same type.\\nSo you could have a string and an integer, and so on.\\nBut in practice, lists usually have the same type of object.\\nNow, lists are also fine with duplicate items,\\nso it doesn't matter if we're adding a value\\nthat's already in our list.\\nAnd lists in Python are normally used\\nfor storing items in a particular order.\\nFor example, the column names of the table.\\n\\nThe other structure you'll want to know about\\nare dictionaries.\\nAnd dictionaries are all about mapping keys to values\\nor mapping one item to another.\\nSo in this instance, we have a dictionary called position,\\nand we're mapping First to Gold,\\nSecond to Silver, and Third to Bronze.\\nAnd I can confirm that this is a dictionary\\nby using type again, and you can see that I've got\\na response of dict.\\nAnd the real purpose of dictionaries\\nare to make it easy to look up the value\\nthat corresponds to a particular key.\\n\\nSo for example, I can take a look at First,\\nand if I want to try and determine what First maps to,\\nI can do position, which is the name of the dictionary,\\nand First, and I'll get the response back being Gold.\\nAll right, so lists in Python\\nare used for storing items in a particular order.\\nFor example, the column names of the table.\\nAnd dictionaries are great\\nif you want to map things together.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5911596\",\"duration\":356,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Rename a Series (or column)\",\"fileName\":\"4493047_en_US_02_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":429,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Sometimes you need to rename a column in a DataFrame. In this video, learn how to give columns new names.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15136661,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] I'm sure you've worked with spreadsheets\\nwhere the column headings were just really unhelpful things\\nlike col_1, col_2, and so on,\\nand it doesn't tell you anything about\\nwhat that heading or that column's all about.\\nNow if you just rename those column names\\nwith something that's more descriptive,\\nit becomes significantly easier\\nto work with this kind of spreadsheet.\\nSo let's take a look at a couple of ways\\nthat we can rename a series or a column in our dataset.\\n\\nSo what I want to do here is I want to be able\\nto map or rename athlete name\\nso that it has Athlete_Name\\nand event gender so that it becomes Event_Gender.\\nNow one of the reason I might want to do something like this\\nis because this will then allow me to use a dot notation.\\nSo I have my mapper, which is a dictionary,\\nand let's take a look at the rename method.\\nAnd you can see that the purpose of the rename method\\nis to rename columns or index labels.\\n\\nAnd one of the options available within rename is columns.\\nSo this is where you provide your mapping as a dictionary.\\nSo for example, I could say, oo.rename and then columns=,\\nand then provide my dictionary, which is mapper.\\nAnd now you can see that I've got the athlete\\nwith Athlete_Name and Event_Gender.\\nAnd let's go ahead and just take a look\\nat a random three values.\\n\\nAnd you'll notice that the athlete name\\nand the event gender have gone back to the previous values\\nand this is because we need to be able\\nto save those variables back to itself.\\nSo if you go back to the rename method,\\nyou can see that what's returned by this method,\\nwhich is under the section called returns,\\nis it returns either a dataframe or none.\\nSo it returns a dataframe with the renamed axis labels\\nor none if in place equals true.\\nSo what I'm going to go ahead and do here is I'm going to say\\nthat I want it to return a dataframe\\nand so I have oo = oo.rename\\nand I provide the dictionary over here\\nand you can see that I get the athlete name\\nand the event gender with the underscore values here.\\n\\nNow what I can also do\\nis I can chain these methods together.\\nSo what I'm doing over here is I first read in the CSV file\\nand then I change it by using a dot\\nand then the method rename.\\nAnd this is going to first read in the file\\nand then the next thing it's going to do\\nis to rename the columns\\nbased on what we have in the dictionary.\\nAnd this allows us to achieve exactly the same output\\nas we had previously.\\n\\nNow instead of specifying the dictionary\\nor the mapping as a separate variable\\nlike we've done here with mapper,\\nI could just specify a dictionary directly over here.\\nAnd so I say columns =,\\nand then provide the actual mapping that I want\\nand I'll get exactly the same response\\nand the same results as earlier.\\nNow the other thing you could do is also if you want,\\nyou can go ahead and take a look at all of the headings\\nand you can see that this is a list.\\n\\nSo we've got year, city, sport, discipline and so on.\\nAnd if I wanted to go ahead\\nand rename the column names,\\nI could go ahead and change this\\nby modifying the column names\\nso that I have Athlete_Name and Event_Gender.\\nAnd so now I say oo.columns\\nwhich corresponds to the headers\\nand I can say oo.columns = column_names\\nwith the changes that I have to my column names.\\nAnd you can see that this achieves exactly the same results\\nas using the rename of the columns earlier.\\n\\nAnd so this means that I can go ahead\\nand make those changes when I read in the file.\\nSo I'm reading in the CSV file\\nand I'm providing the changes to the column names\\nby providing the names parameter =column_names.\\nNow the problem here is that row zero is over here\\nand this is the old column name.\\nYou can see that this is the old column name\\nbecause we've got a space here.\\nSo Athlete Name and Event_Gender.\\n\\nAnd to get around this, we need to say header=0\\nbecause just providing a names=columns_names,\\nI'm going to have an additional row,\\nwhich is the old column names.\\nAnd so if I have both names=column_names,\\nwhich is my list of the updated columns, and header=0,\\nI get the results that I want.\\nSo just to recap,\\nif you want to rename the columns\\nwhile reading in the file names,\\nyou need to remember two things,\\nprovide the column names as a list to the parameter names,\\nand you must have header=0.\\n\\nAll right, so we've seen a couple of different ways\\nof renaming the column names.\\nSo how can you decide which one you should use?\\nWell, if you're just renaming one of the column names,\\nthen you're probably better off\\njust using a Python dictionary\\nand mapping the old column name to the new one.\\nIf you want to make changes to several of the columns,\\nthen you're probably better off\\njust specifying the new column names just as a list\\nand making the changes there.\\nWhen working with a list option,\\nI always display the current column names,\\nfor example, oo.columns,\\nand then use that as my starting point.\\n\\nSo what I mean by that is\\nif I head over to here, oo.columns,\\nI always display that over here.\\nAnd this way I'm less likely to make a mistake\\nwith forgetting a column or misspelling something.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5915593\",\"duration\":519,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Remove a Series (column) or row\",\"fileName\":\"4493047_en_US_02_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":682,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Deleting unnecessary columns is key to working with data. In this video, learn how to use pandas's drop() function.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":20046300,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Narrator] Imagine you're working on a project\\nwhere you've gathered data from multiple sources,\\nand now you have this massive data frame\\nwith columns upon columns of information.\\nAnd some of it's relevant,\\nbut some of it might just be redundant data.\\nThat's where the drop method comes into play.\\nNow, why drop and not delete or remove?\\nThat's because drop comes from database terminology.\\nNow, in our collab notebook,\\nlet's take a look at the drop method.\\nNow, the first thing to remember\\nis that in Pandas, we always refer to rules\\nor the index as having an axis of zero\\nand the columns as having an axis of one.\\n\\nNow, I think the position column is pretty useless\\nbecause I already have gold and silver and bronze\\nto know what position the medalists have.\\nSo let's go ahead and remove it.\\nNow, because position is a column,\\nI need to specify that the axis is one.\\nAnd so if I go ahead and do that,\\nyou can see that the position column\\nis missing from this data frame.\\nRight now, if I was to go ahead and uncomment this line,\\nand if I say axis equals zero,\\nso that means I'm specifying a row\\nand then I specify position,\\nyou can see I get an error\\nand it says key error, position is not found in axis,\\nso I need to make sure\\nthat I'm specifying the right direction.\\n\\nSo is it a column or a row?\\nBecause positions are related to columns,\\nI need to make sure that I always have an axis equal one.\\nSo let's go ahead and take a look at our data frame again.\\nNow you'll notice that position is back, right?\\nSo we obviously haven't dropped it.\\nAnd the reason we haven't dropped it\\nis because when we make these changes\\nor when we go ahead and drop a column from a data frame,\\nwe need to update our data frame.\\nAnd so we need to say, oh, drop\\nand then pass that to our data frame here.\\n\\nAnd so now you can see\\nthat the position column has been removed.\\nNow what I can also do is I can chain\\na couple of commands together, right?\\nSo what I could do is I could read the CSV file in\\nand then knowing that I'm going to get rid of the position,\\nI can chain the drop method\\nand it's going to go ahead and drop the position column.\\nSo let's take a look at that\\nand you can see that we've got rid of the position column.\\nNow what I can also do is I can do this chaining,\\nbut I can do it across multiple rows\\nby having all of the commands within these brackets.\\n\\nSo for example, I have pd.read_csv as my first line.\\nThe second line is drop,\\nand then I've got the sample as my final line.\\nNow, the reason why I'd want to be able\\nto order my chaining in this way,\\nis because I've got a whole load of steps\\nor instructions here.\\nAnd so that way I can troubleshoot very easily\\nand understand that each row corresponds\\nto a specific instructional step.\\nAnd so you can see here that I've got\\nexactly the same output as earlier,\\nwhere I'm getting rid of the position column.\\n\\nNow, let me just go ahead and demonstrate\\nhow you can actually troubleshoot that.\\nSo now, if I just wanted to go ahead\\nand troubleshoot these lines,\\nI could add a comment in the second line,\\nand this will show me\\nmy entire data frame with the position.\\nAnd then if I wanted to add that line again,\\nI could uncomment that line.\\nAnd then this will go ahead\\nand read the CSV as the first line.\\nAnd then the second line or that second step,\\ngo ahead and drop the position column.\\nNow, if you've used Pandas before,\\nyou're probably thinking\\nthat you can also use the in place parameter option instead\\nof having to pass my data frame back to the variable OO.\\n\\nAnd so I can do that here, and you're absolutely right,\\nbut I prefer not to use in place.\\nSo let's go ahead\\nand just take a quick look at the documentation for drop.\\nAnd so we've got exactly the same output as earlier.\\nSo we're displaying the CSV file,\\nwe're going to drop the column,\\nand then we have in place equals true,\\nand that gets rid of the position column.\\nNow if I take a look at oo.drop,\\nnow the reason that the in place works\\nis because what's returned is either the data frame,\\nand this is what I return to the variable oo\\nor the data frame oo,\\nor I can have a return of none,\\nand then in place equals true.\\n\\nNow, the reason I suggest you don't use in place\\nis because it doesn't allow for chaining.\\nSo when I go ahead and try and run this command,\\nso I've got these multiple steps across multiple rows,\\nyou see that I end up with an attribute error,\\nnone type, object has no attribute sample.\\nAnd the reason for this\\nis because this step is going to return a none.\\nAnd so it doesn't allow me\\nto do chaining when I use in place equals true.\\n\\nSo you're much better off just not using in place\\nand just returning the data frame.\\nAnd so you'll get a result like this.\\nNow, I can also go ahead and delete rows.\\nSo I specify that the axis equals zero,\\nand if I wanted to get rid of the row with index equals two,\\nso this one over here,\\nI specify that index number and axis equals zero.\\nAnd you can see that we've got rid of the row too.\\n\\nI can also get rid of a couple of rows.\\nAnd the way I do that is I provide the index numbers\\nfor all of the rows that I want to get rid of in a list.\\nSo I want to get rid of zero, one, and three.\\nI say axis equals zero.\\nAnd you'll be able to see\\nthat we now get rid of rows zero, one, and three.\\nNow what I can also do is I can go ahead\\nand delete a couple of the columns.\\nSo I can say that I want to get rid of city and sport,\\nfor example, and I need to specify that axis equals one.\\n\\nAnd in just the same way,\\nI need to provide that information in a list.\\nAnd let's go back to our original state\\nwhere we've gone and dropped the position column.\\nLet's talk a little bit more about why using\\nin place equals true is not a good idea.\\nSo Pandas encourages a method chaining approach,\\nso where you can add\\nand apply multiple operations, one after the other.\\n\\nSo using in place equals true\\nbreaks this chain as I've shown you earlier.\\nAnd that's because the in place method\\ndoesn't return a reference to the data frame.\\nAnd so it makes it impossible\\nto chain further operations directly.\\nNow, the second reason\\nwhy you probably don't want to use in place equals true\\nis because it's not available in several Pandas methods\\nlike merge, for example.\\nSo let's just take a look at the merge documentation.\\nNow, if you go ahead and take a look\\nat all of the parameters,\\nyou'll find that there is no in place here.\\nAnd it's exactly the same case for the pd.concat again,\\nwhich is another popular method.\\n\\nAnd if you scroll through,\\nyou'll find that there's no in place there or group by.\\nAnd these are three really popular methods in Pandas,\\nand none of them have the in place option.\\nNow, if you're still not convinced,\\nthe third reason I suggest you don't use\\nan in place equals true is because it makes a debugging code\\nthat much more difficult because it modifies data in place\\nand it can make it way more challenging.\\nSo if something goes wrong after an in place operation,\\nyou've lost the original data frame,\\nand it makes it harder to understand\\nwhat changed or what went wrong.\\n\\nAnd the final reason I suggest you don't use\\nin place equals true is for future proofing reasons.\\nThere's been some discussions in the Pandas community\\nabout deprecating, that means not supporting\\nthe in place parameter in future versions of Pandas.\\nSo while it's still available,\\nrelying on it might mean that you might have to make changes\\nto your code if Pandas decides to phase out this feature.\\nSo for these reasons,\\nmany Pandas users and the community at large\\nrecommend avoiding in place equals true,\\nand you can use the assignment operations instead,\\nwhich are clearer, they're more consistent,\\nthey're more flexible.\\n\\nAnd we'll do this for the rest of this course.\\nOkay, we've covered a lot in this video.\\nSo let's do a quick recap.\\nWe can use the drop method to remove series.\\nNow, if you want to remove a row,\\nthen we have axis equals zero.\\nAnd if you want to remove a column, then axis equals one.\\nAnd I hope I've convinced you\\nnot to use in place equals true.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5917324\",\"duration\":226,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Filtering rows for a single condition\",\"fileName\":\"4493047_en_US_02_08_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Add pause animation at 3:56\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":342,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Boolean indexing allows you to filter based on certain conditions. After watching this video, you will be able to demonstrate how to use boolean indexing.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8041955,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] One of the reasons I switched\\nto Pandas was because I was working\\non these really large data sets at work,\\nand I could do it efficiently.\\nNow, one of its most useful features is the ability\\nto filter row based on specific conditions.\\nThis way, you and I can focus on the data that matters most.\\nSo here's a whole load of operators\\nand the associated symbols.\\nTake a quick look at the operators\\nand the symbols here,\\nand make sure that you memorize them\\nso that you can use them quickly when you need to in Pandas.\\nAnd now let's take a look at an example in our notebook.\\n\\nSo this is our sample data frame with no position column.\\nNow, if I want to go ahead\\nand take a look at what the type is for true,\\nyou can see that it's a Boolean, or a Bool.\\nAnd in exactly the same way, the type\\nfor a false is also a Bool.\\nSo if I go ahead and take a look at the year,\\nand I want to try and determine what the type\\nis for the year, you can see that that's an integer.\\n\\nSo this means if I want to be able\\nto compare the year with a specific year,\\nbecause it's an integer, I need to just specify that number.\\nSo what I'm doing here is I'm saying oo.Year,\\nso that's filtering all of the years that are 1896,\\nand I'm assigning that to the variable first_olympics.\\nAnd what's happening under the hood\\nis that for that series, year,\\neach of the values are being assigned a value\\nof true or false based on whether\\nthat specific entry is 1896.\\n\\nSo I can go ahead and apply that condition in my data frame.\\nAnd because the condition is creating a series of Booleans,\\nwhich is exactly the same link as the data frame,\\nyou are able to then get all\\nof the corresponding components\\nin the data frame where the year is 1896.\\nAnd you can see that these are the first couple\\nof entries and these are the last entries,\\nand all of them have the year 1896.\\nSo instead of having to have this across multiple steps\\nwhere I'm having a first Olympics\\nand so on, I can just specify that condition within oo.\\n\\nAnd so I can say oo, and then the condition\\nthat I used, and I'll get exactly the same result.\\nNow, I can also go ahead\\nand try a couple of other conditions.\\nSo let's say I want to try\\nand see all of the years that are less than 1896.\\nSo I want you to pause the video here,\\nand explain why we have no information in our data frame.\\n(offbeat uplifting music)\\nSo the reason that we have no entries\\nin the data frame\\nis that the first year in our dataset is 1896.\\n\\nOur condition here is asking for any years\\nthat are less than 1896.\\nSince that doesn't exist,\\nthere are no entries in our data frame.\\nNow, instead, if we change the condition, so that's the year\\nand less than or equal to 1896, then we'll be able\\nto see all of the entries from the data frame\\nthat correspond to that first year, or 1896.\\nAnd we can confirm that that's the case.\\nNow, just a tip that I often will use\\nis that if I have a single condition,\\nwhat I'll often do is include the condition within brackets.\\n\\nAnd the reason I do that\\nis because when it comes to having multiple conditions,\\neach of these conditions need to be in these brackets,\\nand this just ensures\\nthat there won't be any problems if I decide\\nto include a couple of other conditions further down\\nthe road.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5917323\",\"duration\":246,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Filter rows for multiple conditions\",\"fileName\":\"4493047_en_US_02_09_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":311,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Boolean indexing allows you to filter based on more than one condition. After watching this video, you will be able to demonstrate how to use boolean indexing for multiple conditions.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9911320,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In the previous video,\\nwe looked at filtering a row based\\non a single condition.\\nNow let's take a look at multiple conditions.\\nSo we've got our data frame with no position column,\\nand let's take a look at the data types for all\\nof the series or all of the columns in our data frame.\\nAnd in particular, let's look at city.\\nNow because city has a type object,\\nthat means it's most likely going to be a string.\\nAnd so if I want to do a comparison\\nor I just want to capture all of the cities that are Athens,\\nI need to make sure that it is within\\nsingle or double quotes.\\n\\nAnd you can see that I've got a data frame here with\\nall of these city entries being Athens.\\nNow, just to show you,\\nso let me just go ahead and un-comment this.\\nIf I didn't use the single\\nor double quotes Athens, I'll get an error.\\nAnd I've got a name error because Athens is not defined.\\nSo Pandas is expecting you to have a string there for city\\nbecause it's of a type object.\\nNow I can combine a couple of conditions\\nand you'll notice that each\\nof the conditions are within these brackets.\\n\\nSo I want the year to be 1896\\nor the year to be 2004.\\nAnd again, you can confirm that the first couple\\nof years are 1896 and the last couple of years are 2004.\\nSo if I wanted to make sure that that's in fact the case,\\nI could always just chain a unique\\nand you'll be able to see that I've only got two entries,\\n1896 and 2004.\\nSo you can see that I've got an attribute area here\\nbecause the data frame object has no attribute unique.\\n\\nAnd so what I need to do is I need to specify that I want\\nto determine the unique values for the series here.\\nAnd so if I go ahead and just add here,\\nwhich is now a series,\\nI've got just the values for 1896 and 2004.\\nSo here I've got the city is Athens and the year is 2004.\\nNow the reason I have that is\\nbecause Athens hosted the Olympics twice.\\n\\nOnce in 2004 and once in 1896.\\nSo there are a couple of ways that I can specify\\nthat I want the city's Athens\\nand the years 2004, I can say Athens and year equals 2004\\nor city equals Athens,\\nand not the year 1896, which is the other year\\nthat Athens hosted the Olympics.\\nSo just a word of warning about these conditions.\\nMake sure that they are clear\\nand as future proof as possible.\\nSo this means that if they're going to be possible future\\nconditions such as Athens hosting the Olympics again,\\nthen we don't want any future data to break some\\nof the conditions that we have over here.\\n\\nLet's take a look at another example.\\nSo we want to get the year 1896.\\nSo what we have here is the a hundred meters\\nOlympic sprint for men.\\nAnd you can see that our data frame has captured that here.\\nAnd we've got two bronze medals.\\nSo there was a tie for the bronze medals\\nand then you've got a gold and a silver.\\nNow if I just want to be able to see a couple\\nof these columns, so just say I want to just see the year\\nand the athlete's name, the country they represented\\nand the event and the medal, I can do that\\nby including the columns that I want in a list.\\n\\nAnd so now I'm not displaying the entire data frame,\\nbut just the columns that are specified in the list.\\nAnd I can also do that by specifying\\nor by taking the original data frames\\nof first men 100 meters\\nand then specifying the columns that I want to display.\\nAlright, so we've seen that we have a lot of flexibility\\nwith matching multiple conditions in Pandas.\\nThat's really important to remember\\nthat if you have multiple conditions,\\nthat each of the conditions are enclosed in bracket.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2719580\",\"duration\":272,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Using String methods\",\"fileName\":\"4493047_en_US_02_10_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Add pause animation at 4:45\\nAdd pause animation at 6:08\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":430,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"str methods allow you to clean and transform your data easily. After watching this video, you will be able to use many of the methods available in str.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10714613,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] If you want to work with lots of text data,\\none of the skills you'll need to pick up really quickly\\nis cleaning and transforming text data.\\nThe pandas str methods are perfect for this.\\nIf you take a look at the athlete names in our dataset,\\nthe surname is in capitals followed by the first name.\\nSo I can use the str methods in pandas to change that.\\nAnd so I can go ahead and change all of the athlete names\\nto lowercase,\\nand you can see that it's done that for the entire column.\\n\\nAnd I can do exactly the same thing for the events.\\nSo I could go ahead and take all of the events\\nand you can see that some of the events\\nare lower case at the start and some of them have uppercase.\\nSo if I wanted it to be a little bit more consistent,\\nI can ensure that the first letter is capitalized\\nand I can use the str.capitalize for that.\\nAnd if I go ahead and take a look at all of the values now,\\nso if I go ahead and do a unique here,\\nyou can see that the first character\\nhas been capitalized for all of the events.\\n\\nNow what I'll need to do\\nif I want to make sure that this is saved,\\nis I need to take the oo event, for example, str.capitalize,\\nand apply that to oo.event,\\nso that's actually stored in the column event.\\nAnd then that way those changes are permanent.\\nNow one way to determine\\nwhat's possible with the str methods\\nis to do a dir of the str methods.\\n\\nSo the oo.event is the Pandas series.\\nAnd then I want to do a dir of all of the methods\\nthat are going to be available here.\\nAnd you can see that I've got a whole load\\nof different options available to me,\\nthings like casefold and cat and count and decode and so on.\\nNow, Larisa Latynina has the most medals\\nof any female athlete in Olympics history\\nwith a total of 18.\\nShe competed for the Soviet Union\\nin 1956, 1960, and 1964.\\n\\nSo if I was to go ahead and do\\na oo athlete name.str.contains, what's happening here is\\nthat it's doing a comparison for every single entry\\nin that series, athlete name,\\nand determining whether that name contains Latynina.\\nAnd if it does, you get a true and if it's not the case,\\nyou get a false.\\nNow I want you to pause the video here\\nand try and explain to me why we get no results back.\\n\\nEven though I've specified\\nthat within the athlete name column,\\nit should contain Latynina.\\n(upbeat music)\\n(gentle music)\\nNow her name is Larissa Latynina,\\nand if you remember from this dataset,\\nthe surname is always capitalized.\\nAnd because her surname was not capitalized here,\\nit doesn't match the conditions,\\nand that's why we have no entries here.\\nNow if I change her surname to all capitals,\\nI'll be able to see all of the medals that she has won.\\n\\nSo the str methods are your go-to\\nif you need to transform your text data.\\nSo now as we wrap things up,\\nSo dir.oo.event.str will give you all of the methods\\nthat are available,\\nand go ahead and make all of the city names\\nin capital letters.\\nSo go ahead and pause the video again\\nand I'll give you the answer in a few seconds.\\n(upbeat music)\\n(gentle music)\\nSo if I want to make those changes to the city\\nand I want to make those permanent,\\nI need to apply that to oo.city.\\n\\nAnd the str method I'm looking for is upper.\\nAnd if we take a look at the city names here,\\nyou can see that in our random sample of three,\\nwe've got all of them as upper.\\nAnd I can go ahead and confirm that for all of the cities.\\nSo, 00.city.unique.\\nAnd you can see that all of the city names\\nare now in uppercase.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2719578\",\"duration\":258,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Sorting a DataFrame or Series\",\"fileName\":\"4493047_en_US_02_11_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":429,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"sort_values() allows you to sort values along either axis. After watching this video, you will be able to demonstrate how to use this method.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12305256,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] If you've found yourself staring\\nat a jumbled mess of data,\\nwishing there was an easy way to bring order to the chaos,\\nthen the sort_values method is your friend.\\nYou can arrange your data alphabetically\\nor numerically to start with.\\nSo this is our DataFrame with three random indexes or rows,\\nand let's sort by the athlete name.\\nAnd you can see that we're sorting by the surname,\\nand so we've got Edgar and Arvo Ossian and so on.\\n\\nNow let's take a look at what the type is for this output.\\nAnd you can see visually that this is a series.\\nNow what you can also do is to sort by athlete name\\nfor the entire DataFrame.\\nSo you do oo.sort_values\\nand then you specify which column you're looking to sort by.\\nAnd here, you can see, again,\\nthat we've got the entire DataFrame here\\nand we're sorting by the athlete names that we saw earlier.\\n\\nSo that's Abbye, Edgar and Aaltonen, Arvo and so on.\\nAnd you can see that this is in fact a DataFrame\\nand you can confirm that visually\\nbecause this looks like a table.\\nSo let's take a look at the first couple\\nof entries in our DataFrame,\\nand what I could do is I can sort in reverse order,\\nso I say sort_values and I say ascending=False,\\nbecause by default, ascending=True.\\n\\nNow you might be a little bit surprised\\nbecause you're not seeing any results\\nor athlete names that start with a Z,\\nand this is because of Unicode.\\nNow Unicode is an encoding standard,\\nand all of the letters and numbers and symbols\\nand emojis are assigned a Unicode code point,\\nwhich is a number.\\nAnd so when you use sort_values, it uses these numbers,\\nand so this O with an umlaut, so Ostrand,\\nhas a higher value than the letter Z,\\nwhich is why it appears after.\\nSo I'll prove this to you\\nby displaying the first 25 entries,\\nand you can see that we've got a couple of other names\\nwith the umlaut, so we've got Ostmo and so on.\\n\\nSo if we take a look at the first 25 entries,\\nwe can see that we've got all of the names with the umlauts\\nwhich have the higher Unicode values,\\nand then we've got the Latin Z and so on.\\nNow, I can also sort first by one column,\\nso I can sort first by year, and then by athlete name.\\nAnd so we've got all of the entries from the first Olympics,\\nso that's 1896, and then sorted by the athlete name.\\nOr I could sort by year and athlete name\\nand then specify what order I want,\\nso I actually want descending for the year\\nand I want ascending for the athlete name.\\n\\nAnd so I specify a True or a False\\nfor each corresponding column by providing that in a list.\\nLet's take a look at the first seven entries.\\nSo what I can do is I can sort by the year first\\nand then the event,\\nand then I can sort by the medals.\\nBut I want to sort by the medals\\nin the reverse order, right?\\nBecause I want to be able to see the ones that are first gold\\nand then the ones that are silver,\\nand then the ones that are bronze.\\n\\nAnd so I'm sorting by year and then the event,\\nand then quite annoyingly,\\nI have silver before gold, followed by bronze.\\nAnd the reason for this is that it's sorting the medals\\nby alphabetical order.\\nBecause if you remember,\\nif we take a look at the data types,\\nMedal is a string.\\nSo it's sorting\\nby the alphabetical order rather than the order\\nof the medal, as in gold is higher than silver,\\nwhich is higher than bronze.\\n\\nAll right, so we've seen the power of sort_values\\nand it's very easy to have a variety\\nof different combinations,\\nand so go ahead and experiment with sort_values\\nand make sure that you are happy with ascending\\nand descending orders\\nfor different columns in your DataFrame.\\n\"}],\"name\":\"2. Fundamentals of Working with pandas\",\"size\":129558155,\"urn\":\"urn:li:learningContentChapter:5916305\"},{\"duration\":4707,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5914609\",\"duration\":308,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Working with data types (dtype)\",\"fileName\":\"4493047_en_US_03_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":404,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"pandas has a variety of different data types. After watching this video, you will be able to describe them and compare the benefits.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11152985,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] What really winds me up when working\\nwith data are errors about data types.\\nNumbers are treated like characters\\nor strings, years have a decimal point\\ninstead of just being an integer.\\nSo we're going to get to the bottom of this by understanding\\nhow dtypes or data types in pandas work.\\nSo we've got our data frame 00\\nand let's take a look at the different types\\nfor the columns.\\nAnd so you can see that year has a data type of integer 64,\\nand all of the others are of type object.\\n\\nAnd this normally means that they're strings.\\nNow categoricals are a pandas data type, corresponding\\nto categorical variables in statistics.\\nSo a categorical variable takes on a limited,\\nand it's usually a fixed number of possible values.\\nSo for example, you have things like medals and gender\\nor event gender.\\nLet's take a look at some of our columns.\\nYou can see that medal, for example,\\nhas gold, silver, or bronze.\\nAnd this is a great data validation technique\\nbecause you want to make sure\\nthat your medals are only going to be gold, silver, and bronze\\nand no other category.\\n\\nSo what we can do is we can assign the medal column\\nto the categorical or category type,\\nand we do that by using the astype method.\\nSo we say O0.Medal.astype category,\\nand then we assign that to medal.\\nNow when we look at the columns, you can see\\nthat now medal has a data type of category.\\nNow in the same way where we've got gender\\nand we've got two types, men and women,\\nand we've got event gender, so that's M and X and W.\\nSo that's men, women, and where there are mixed teams for X.\\n\\nSo let's go ahead and change all of these\\nto a category type.\\nAnd so now you can see we've got gender\\nand event gender and medal as categories.\\nAnd let's take a look at medal.\\nAnd if we were to take a look at the series, you can see\\nthat this is now off type category with bronze\\nand gold and silver.\\nNow, what if we wanted to actually order our categories\\nbecause we want to make sure\\nthat gold is considered better than silver,\\nwhich is considered better than bronze.\\n\\nBecause if we take a look at this category type over here,\\nit's all sorted alphabetically.\\nYou can see that bronze is first,\\nand then you've got gold and then silver.\\nBut we want to make sure that we've got gold which is better\\nthan silver, which is in turn better than bronze.\\nSo if we take a look at the categorical method,\\nand let's take a look at the definition.\\nSo you've got categoricals can only take on a limited\\nand usually fixed number of possible values.\\nAnd we know that these are the categories,\\nbut it seems to be that there's also an option\\nof ordering them.\\nSo order is defined by the order of the categories,\\nand if we scroll down, you can\\nprovide the different categories as a parameter,\\nand then you can specify that you want them to be ordered.\\n\\nSo by default it's false.\\nAnd so all we need to do is just flick that flag to true.\\nSo I can provide the medal order here,\\nand it's important that I specify the order correctly here.\\nSo I want bronze to be at the bottom\\nand then followed by silver and then gold.\\nAnd then I specify that order equals true.\\nAnd so now if I take a look at the column medal, right?\\nI've got all of the different types and let me just go ahead\\nand take a look at medal.\\nSo I'm just going to do 00.Medal.\\n\\nYou can see that I've got gold is greater than silver,\\nwhich in turn is greater than bronze.\\nSo in a previous video we were trying to sort the medal so\\nthat we first had the gold followed by the silver\\nand then the bronze, but we were not able to get this\\nto work because we were representing the medals as strings.\\nNow that we've got the medals as categories, this allows us\\nto order them in that way.\\nSo what we can have here is we have this first by year,\\nthen by event, and then we want to reverse the order\\nfor medals so that we first see the gold\\nand then the silver, and then the bronze.\\n\\nAnd you can see that that's the case here.\\nSo we've got the year 1896, all of these are year 1896.\\nThen sorted by event.\\nSo you've got the 100 km cycling event followed\\nby the 100 meters and so on.\\nAnd then you can see that we first got the gold,\\nthen the silver, and then the bronze.\\nNow before you think that categories are the next best thing\\nto sliced bread, they're also very fragile.\\nIt doesn't take much to move a column\\nor a series back to an object type\\nfrom a categorical variable.\\n\\nSo if you combine different columns\\nor have missing values, these all can result\\nin your column moving back to an object type.\\nOkay, so we've seen that categorical variables take\\non limited values, and previously we weren't able\\nto sort medals by their position,\\ngold being higher than silver, being higher than bronze\\nbecause they were represented as strings.\\nNow that they're represented as categories, we're able\\nto order them in the way that we want.\\nBut be careful with categorical variables,\\nthey're very fragile\\nand you can easily move back to an object type.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5916302\",\"duration\":276,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Memory usage of dtypes\",\"fileName\":\"4493047_en_US_03_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":379,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10296507,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Different data types\\nare represented differently,\\nand so some of them will take up less space\\nin memory than others.\\nLet's take a look at how things fare\\nwith the categorical variables.\\nNow this time I'm storing the Olympics dataset\\nin a data frame called DF, but I'm not doing any\\nof the processing that we've done so far.\\nSo if you'll notice we have the different data types.\\nSo most of them are of type object and then only year\\nand the column position are integers.\\n\\nNow if you take a look at the data frame\\nthat we've been working with all along, so that's OO,\\nlet's just try and compare the different data types here.\\nAnd you can see that we've got a couple\\nthat are category which we assigned in the previous video.\\nWe've got a couple of integers,\\nand then most of them are of type object.\\nNow if you remember, categoricals are a pandas data type\\ncorresponding to categorical variables and statistics.\\nSo they have a limited number of possible values,\\nbut let's take a look at what difference that makes in terms\\nof the memory usage.\\n\\nSo if you want to take a look at the documentation,\\nyou can go ahead and check that out here.\\nSo what I'm going to do is I'm going to compare the memory usage\\nof the data frame OO, which is the one\\nthat we've been using all along, versus the data frame DF\\nwhich is the one that doesn't do any\\nof the processing that we've done.\\nSo you can see for example that with DF,\\nthe metal type is an object.\\nWhile as with the OO data frame,\\nthe metal type is a category\\nand there's a significant difference in the amount\\nof memory that's used.\\n\\nSo this uses 27,000 bytes while this uses\\njust under 1.7 megabytes when representing it as an object.\\nAnd in fact, if you were to compare the two,\\nthe memory usage\\nof the metal data type when it's represented\\nas a category is only about 2% that\\nof when it is represented as an object.\\nNow if you were to go ahead\\nand compare the other categorical variables,\\nso things like gender and event agenda\\nwith these two different data types.\\n\\nSo OO being a category type and DF being an object type,\\nyou can see that we've got a similar amount\\nof memory savings.\\nSo it definitely makes sense to consider using categories\\nwhen working with larger data sets\\nbecause you'll get the memory saving\\nand the response time will be much quicker in terms\\nof querying datas.\\nNow another thing that's worth considering is in general,\\nwe know that object data types are typically string,\\nbut what if we could actually assign them\\nor be explicit about them being a string type?\\nAnd so we can do that by using the as type method.\\n\\nAnd so we can assign some of the object types as string.\\nSo for example, we do that to city\\nand you can see that city is of type over here.\\nAnd if you were to take a look at a sample\\nof the data frame, and let's go ahead now\\nand update all of the other object types\\nand make them of type string where they were of type object.\\nLet's just take a look at our current state of things\\nbefore I make that change.\\n\\nSo that's OO dot D types with an S.\\nAnd you can see that most of these are object type.\\nAnd so what we're going to do is we're going to change all\\nof these which have an associated type of object to string\\nand we assign that to the corresponding column\\nand you can see that now they're all string.\\nAnd then if you were to do a comparison of the amount\\nof savings that we have in terms of memory,\\nwhen we represent the column city\\nas a string type versus an object,\\nand you can see that there isn't any difference\\nwhen a column such as city is represented as a string\\nor as an object, but I still prefer\\nto keep it as a type string just\\nto be a little bit more explicit about things.\\n\\nAlright, so categoricals have the advantage\\nthat they take up less space in memory\\nand then all those strings don't seem to take up less space.\\nI prefer to define them as strings\\nand just to be more explicit\\nrather than having a type called object.\\nIt also reminds me that I can always manipulate them\\nusing the SDR methods.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5917322\",\"duration\":216,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Defining dtypes when you read in a file\",\"fileName\":\"4493047_en_US_03_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":337,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"You can also specify the dtypes in advance of reading in a file. After watching this video, you will be able to define dtypes when reading in a file.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7261736,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Imagine if someone handed you\\na toolbox for a job,\\nbut didn't tell you what each tool was for.\\nYou might figure it out eventually,\\nbut it'll be a lot of trial and error, right?\\nSo that's a bit like loading your data into Pandas\\nwithout specifying Dtypes.\\nToday we're going to crack open that toolbox\\nand show you exactly which tool\\nor data type is right for each job,\\nmaking your data analysis\\nthat much smoother and faster.\\nJust as a quick reminder, our current data types\\nare int 64 for the year,\\nand then everything else is an object.\\n\\nSo we're going to use a dictionary\\nand we're going to map each of the different columns\\nto the data type that we want.\\nSo we will have year as in 64, we'll have city\\nas string and so on.\\nNow, just for the sake of completeness, I've specified\\nthat a year should be in 64,\\neven though that's the case by default.\\nAnd so if we take a look at our dictionary,\\nwe can see we've got the mapping that we want.\\nNow we can go ahead and apply that.\\nNow, previously we used the categorical method.\\n\\nWe can go ahead and use the categorical Dtype method.\\nAnd this will allow us to specify the ordering\\nthat we would want when reading in the data as our file.\\nAnd so you can see that we've got the categoricalDtype,\\nwe've got them in the order that we want,\\nso bronze and then silver, and then followed by gold.\\nAnd we then specify that metal type is not only a category,\\nbut it's ordered metals\\nwhere we specify the order that we require.\\nAnd that's the only one of the categories\\nwhere we actually require an order.\\n\\nSo now we're ready to read our CSV file in.\\nSo we've got our CSV file, our file name,\\nand that's where we use the parameter Dtype\\nand we provide our mapping.\\nNow, if we take a look at our Dtypes,\\nhopefully we should see the mapping\\nthat we specified earlier.\\nAnd you can see that we've got integers followed\\nby strings and so on.\\nBut we do have one event gender,\\nwhich is still an object.\\n\\nNow remember I warned you earlier\\nthat these category types can be very fragile.\\nSo in this case, it's not obvious\\nto me why we have an event gender type,\\nwhich is still an object\\nbecause if I take a look at the unique values,\\nso that's oo event gender, and then unique.\\nI've only got these three categories, M, X, and W.\\nSo, what I'm going to do is I'm going to again read in\\nthat CSV file along with the Dtype mapper that I specified.\\n\\nBut because I have event agenda as of type object, I'm going\\nto hard code that as a category by specifying\\noo event gender as type category.\\nAnd this should allow me to then have the event gender\\nas type category.\\nSo let's just confirm that that's the case.\\nAnd we can see that by specifying\\nthat the event gender is of type category for a second time,\\nthat seemed to have fixed the problem.\\nAlright, so you can see that we can define the data types\\nwhen reading in the files,\\nbut I also warned you\\nabout how the categorical data types can be very fragile.\\n\\nAnd we saw an example of this in this video\\nand how we were able to fix that.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5910587\",\"duration\":290,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Python functions\",\"fileName\":\"4493047_en_US_03_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":395,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11154153,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] By now you're probably thinking\\nwe're having to do a fair bit of tweaking of our dataset\\nand I hope I don't forget any of the steps,\\nwe needed to remove the position column,\\nwe then needed to specify the different types of data types.\\nSo once I find that I'm having to reuse\\na whole load of code,\\nthe best solution is to define functions.\\nSo let's take a really quick look at Python functions.\\nSo functions allow you to make reusable code\\nand these can be called later.\\nSo you define a function by specifying the def keyword,\\nand then functions can have inputs,\\nso these are in the form of arguments,\\nand so that's whatever comes after this bracket.\\n\\nAnd then the output from the function\\nis in the form of a return value\\nand that's whatever's followed by the return statement.\\nSo in this case, we have the function called show_df,\\nso that's show data frame.\\nThe argument is the file name Olympics 1894,\\nand so on, CSV file.\\nWhat we're doing in the function is to read in that CSV file\\nand store it in this variable called df,\\nwhich is our data frame,\\nand we then go ahead and return that data frame.\\n\\nSo if we wanted to call this function,\\nwe do it by specifying the function name,\\nso you got show_df and then using these\\nopen and close brackets.\\nSo this will call the show_df function.\\nAnd the purpose of the show_df function\\nis to just return that data frame.\\nAnd you can see that you're able to view that data frame\\nbecause that's what's been returned by the function.\\nNow what type is returned by the function?\\nAnd you can see that that's a data frame\\nand that's what we were able to see earlier.\\n\\nSo if we take a look at all of the steps\\nthat we're trying to automate,\\nso specifying the different data types\\nfor the different columns,\\nso we've got that in dtype_mapper,\\nwe've got the ordered medals and so on.\\nThe other thing we were trying to do\\nwas also to drop the position column.\\nSo we can combine all of these steps in a function,\\nand I've called my function pre-process,\\nand my argument is the file name,\\nwhich is the CSV file that we're using.\\n\\nI could just provide file name without the actual file name.\\nAnd then I would always need to provide the file name\\nwhen I call that function.\\nBecause I know I'm only going to be using\\nthis one file for now,\\nI provide that as a variable over here,\\nso that that's going to be the default variable.\\nAnd that just means that I don't need\\nto keep specifying pre-process,\\nand then Olympics_1896_2004.CSV.\\nSo that's just a shortcut.\\nSo this is my pre-process function.\\nThis is the bits that we're trying to automate\\nor that we're doing a couple of times.\\n\\nSo it's the ordered medals, specifying the data types,\\nand then dropping the position column.\\nAnd then all we need to do\\nis to just return that data frame.\\nYou can see that we've got the output that we would expect.\\nWe'll obviously need to check the data types\\nto make sure that those changes have been done.\\nNow if we just call the function in this way,\\nit's returned that data frame.\\nBut now ideally we want to save the data frame\\nin a variable like we've done all along.\\n\\nSo if I was to go ahead and do oo,\\nso oo is going to save the data frame\\nthat is going to be returned by the pre-process function.\\nAnd we'll be able to see the sample\\nthat we've seen in our videos.\\nNow let's just confirm that the pre-process function\\nhas in fact gone and changed the data types\\nfor all of the columns as per the definitions\\nin the pre-process function.\\nAnd you can see that we've got int64 strings and so on.\\n\\nWe still have that problem with the event gender\\nand that's because I didn't specify that\\nor I didn't include that as part of the function.\\nSo all I need to do now is to go ahead\\nand hardcode that again\\njust like we did in the previous video,\\nand we will then be in a position to have our data types\\nin the format that we want.\\nSo it's really important that when you define a function\\nthat you include all of the steps.\\nNow in this case, I forgot to include this important step\\nwhy I need to actually respecify that the event gender\\nhas to be of type category\\nand that's the reason why I didn't see it earlier.\\n\\nSo what you'll want to do is when you define a function,\\nyou'll want to follow exactly the same steps that you did\\nand that you're trying to automate.\\nSo we've seen that functions are the way to go\\nif you have to do operations a couple of times.\\nNot only can it automate things for you,\\nbut it reduces the risk of you making errors\\nif you do things manually.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5914607\",\"duration\":375,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Working with indexes\",\"fileName\":\"4493047_en_US_03_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":574,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Indexes allow you to access a row or column in pandas. After watching this video, you will be able to describe what an index is.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13843633,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Have you ever tried to find\\nyour friend's phone number in a huge contacts list\\nwith no names or organization?\\nJust scrolling through endlessly is a real pain.\\nIndexes, I suppose I could say,\\nindices in pandas\\nare like having your contacts neatly sorted by name.\\nThey make your life so much easier\\nwhen working with data.\\nSo we've got a newly defined preprocess function,\\nwhich will return our preprocess DataFrame\\nand we store that in the variable oo.\\n\\nNow let's take a look at oo.index.\\nNow every DataFrame has an index,\\nso it's not optional.\\nAnd these correspond to the rows.\\nAnd you can see that we've got a RangeIndex object.\\nSo you have the start, which is zero,\\nall the way up to 27174 in our case.\\nNow if we take a look at oo.columns,\\nit also says Index.\\nBut this is just an Index object,\\nso it's not referring to the row index,\\nbut it just so happens that the columns\\nare also called an index type.\\n\\nNow in a table or a DataFrame,\\nyou have indexes to reference the rows and columns,\\nwell for the columns.\\nAnd by default, the rows and the columns\\nof a DataFrame are just integers.\\nSo let me show you that.\\nSo if I specify header=None in the read_csv method,\\nyou can see that we've got numbers for our rows\\nand numbers for our columns.\\nNow most of the time we leave the rows\\nor the index as integers\\nand we give the columns column names.\\n\\nNow why do we need indexes?\\nWell, it just allows us to identify a certain row.\\nSo for example,\\nif I wanted to access all of the events\\nthat the famous US Olympian Carl Lewis took part in,\\nI could get the indexes\\nand they'd stay with the rows\\nand those won't change.\\nSo for example, I specify here athlete name\\nand it contains Carl Lewis.\\nThese numbers here will never change\\nregardless of what transformation I might do.\\n\\nNow if I want to pick a certain event,\\nso for example, if I take row 22719,\\nand I'm looking for the last discipline\\nthat Carl Lewis took part in,\\nwhich was the long jump event in Atlanta.\\nSo row 222719\\nand the event, I should be able to find\\nthat the last event was long jump.\\nNow what I can also do is I can set the index\\nto something that's a little bit more useful\\nthan just a numeric value.\\n\\nSo I could set to the index, for example,\\nto the athlete names.\\nAnd so now let's take a look at the indexes.\\nSo you've got oo.index,\\nand then these are the names of all of the athletes.\\nSo now if I want to take a look at the events\\nthat Carl Lewis was involved in,\\nI could specify over as the row value his name, Carl Lewis.\\nAnd I can take a look at all of the events\\nhe took part in.\\n\\nAnd I can take a look at the first three rows\\nin the DataFrame.\\nYou can see that the athlete's name\\nis no longer in the columns here\\nbecause this is now the index.\\nAnd if I was to take a look\\nat the shape of the DataFrame,\\nyou'll notice that now\\nthis doesn't have 10 columns anymore,\\nit has nine columns\\nbecause I have moved the column name to an index.\\nNow I could also use oo.loc\\nto allow me to identify a portion of the DataFrame.\\n\\nSo I could specify again\\nthat I want information about Carl Lewis,\\nbut I could specify that I only want the Year,\\nthe Event, and the Medal columns.\\nAnd I do that by providing that information in a list.\\nAnd here I have the Year, the Event, and the Medal\\nfor all of the medals that Carl Lewis won\\nbetween 1984 and 1996.\\nNow I could get all of the information for Carl Lewis\\nby specifying the name Carl Lewis for the row\\nand by using a colon for the columns.\\n\\nAnd this is a shortcut for saying\\nthat I want to be able to display all of the columns.\\nNow another way you can do that\\nis to totally ignore the colon\\nand to just specify his name in this way.\\nBut again, I prefer to be more explicit\\nabout the fact that I'm having both an index\\nand a column component.\\nAnd so I would normally use\\nthis as opposed to just oo.loc and Carl Lewis.\\nNow because the athlete names is the index,\\nI can sort it by the athlete names\\nor I could go ahead and get the index\\nback to what it was earlier\\nby using the reset_index method.\\n\\nAnd this will allow me\\nback as one of the columns over here.\\nNow this is because under the hood,\\nthe DataFrame is just represented\\nby rows and columns, which are just integer values.\\nSo I could say iloc[0, 0].\\nAnd so zero in Python is the first component\\nor the first element,\\nand zero over on the column\\nis the first element here.\\nSo I've got the first row and the first element over here.\\nAnd so I should see AABYE, Edgar.\\nSo if I wanted to be able to see the fifth element here,\\nyou can see that it's 0, 1, 2, 3, 4, 5.\\n\\nAnd so I can see that this is the NOC country,\\nwhich is ZZX, which is when we had combined teams\\nin the early Olympics.\\nAnd similarly, I can view the entire row\\nby just specifying a colon.\\nAnd I have the shortcut\\nwhere I can not use the colon at all,\\nas we saw with loc.\\nAll right, so we've seen that by having indexes,\\nwe can gain access to that exact bit of data that we want.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5911595\",\"duration\":560,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Being productive in pandas: My best practices\",\"fileName\":\"4493047_en_US_03_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":764,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Over the years, the instructor has refined several best practices when working with pandas. After watching this video, you will be able to replicate some of them.\",\"captionsStatus\":\"IN_PROGRESS\",\"cdnStatus\":\"AVAILABLE\",\"size\":20281494,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":null},{\"urn\":\"urn:li:learningContentVideo:5915592\",\"duration\":132,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating Series and DataFrames\",\"fileName\":\"4493047_en_US_03_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":195,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Sometimes you want to create Series and DataFrames from scratch. After watching this video, you will be able to create Series and DataFrames from Python lists and dictionaries.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4213933,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] All along, we've looked\\nat creating DataFrames and Series\\nby reading in data from a CSV file.\\nLet's look at how we can create them from scratch.\\nNow, there are a couple of ways\\nand I'll show you the ways I use most often.\\nSo I've got two lists here.\\nOne with the city, so that's London, Rio, and Tokyo.\\nAnd the other is a list with the corresponding start_date.\\nSo the London Olympics started on the 27th of July, 2012.\\nThe Rio Olympic started on the 5th of August,\\n2016 and so on.\\n\\nSo I can create a Series by taking in that list,\\nso the list of city and passing it to pd.Series.\\nAnd you can see that I've got a Series here with London,\\nRio and Tokyo and the corresponding index.\\nNow, a DataFrame is made up of multiple Series,\\nand so I could take in a dictionary\\nand create a DataFrame by having a dictionary of Series.\\nSo then I've got a city\\nand the corresponding Series or column,\\nand then I've got a start_date\\nand the corresponding Series or column.\\n\\nAnd this is within a dictionary.\\nAnd you can see that I've now got a DataFrame,\\none column being city, or one Series being city,\\nand the other Series or column being the start_date.\\nNow, I don't even have to go ahead and create that Series.\\nI could just go ahead and say city\\nand provide the city as a list\\nor the start_date and the start_date as a list,\\nand I'll get exactly the same DataFrame.\\nThe other option that I have\\nis to go ahead and zip the two lists.\\nNow, what I do by zipping the list\\nis that I combine the corresponding components.\\n\\nAnd so what I'm doing over here\\nis I've got London with the corresponding start_date,\\nRio with the corresponding start_date and so on.\\nNow, you'll notice that I don't have a column name,\\nand so I can go ahead and specify a column name in this way.\\nAnd so I provide the column names as a list\\nand I go ahead and zip the two lists.\\nAnd this will allow me to create that DataFrame, okay?\\nSo we've seen that we can create a DataFrame from scratch\\nusing Series, lists, or dictionaries.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5914608\",\"duration\":241,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Working with dates\",\"fileName\":\"4493047_en_US_03_08_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":347,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"When working with data, sometimes you need to work with date and time values. After watching this video, you will be able to work with dates in pandas.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8582102,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Dates can be a real headache\\nwhen you're working with data.\\nThere's one format that's used in the US and Canada.\\nThe rest of the world pretty much uses another format.\\nPandas comes to the rescue again.\\nSo I've got three lists here.\\nCity has the Olympic cities, London, Rio, and Tokyo.\\nThe start date is the 2012, 2016 and 2020 Olympics.\\nAnd then we've got the corresponding end dates.\\nFor some of them you'll notice that we've got\\nthe month first, and then followed by the day.\\n\\nAnd for others we've got the day first,\\nand then followed by the month, and then the year.\\nSo let's see if Pandas can cope\\nwith these different formats of dates.\\nSo let's go ahead and create our DataFrame,\\nand we'll do that by zipping these three lists.\\nAnd adding the column names.\\nSo this is our DataFrame\\nwith the corresponding Olympic cities.\\nAnd their corresponding start and end dates.\\nAnd if we take a look at the data types,\\nyou can see that all of them are of type object.\\n\\nNow one of the things I'm sure that Pandas has\\nis some way to work with dates.\\nSo instead of going online, I'm going to go\\nand check to see if the string date\\nis in any of the methods or attributes for Pandas.\\nAnd if I take a look at the different methods\\nand attributes available here,\\nthe most relevant seems to be to_datetime,\\nso let me just go ahead and check out the doc string.\\nAnd you can see from here that it says\\n\\\"convert argument to datetime.\\\"\\nAnd this is exactly what we want to do.\\n\\nWe want to convert the object to a datetime type.\\nAnd this seems logical because if you're familiar\\nwith Python, Python also has a datetime library.\\nSo let's go ahead and convert the start date.\\nTo a datetime.\\nAnd you can see that we end up with a value error.\\nAnd the reason that we have a value error\\nis because there seems to be different formats\\nas we discussed earlier, so some of them\\nseems to be in the American and Canadian format.\\n\\nAnd others seem to be in the format\\nthat's used in the rest of the world.\\nSo let's go ahead and check our documentation again.\\nAnd we'll be able to see that there is\\nin fact a parameter called format\\nthat allows us to have a mixture of formats.\\nAnd in fact there's information here in the value error.\\nAnd it says \\\"the format will be inferred\\n\\\"for each element individually,\\\"\\nso this looks like what we need.\\nSo if I use the parameter format equals mixed,\\nI'm able to convert that to a datetime format\\nwhere I have the year first,\\nfollowed by the month, and then the day.\\n\\nSo let me go ahead and convert\\nall of the dates to a datetime format.\\nAnd we'll convert the Olympic cities to strings.\\nAnd here the games data type looks a lot more healthy.\\nSo let's take a look at the end date.\\nThe start date.\\nNow what would be really nice is to try\\nand figure out what the duration might be, right?\\nSo I want to subtract the end date from the start date.\\n\\nAnd quite interestingly, it looks like\\nall of these Olympic events had a duration of 16 days.\\nSo I can go ahead and assign this,\\nor create a new column called duration,\\nand I can use the assign method.\\nAnd that will allow me to create this new column\\ncalled duration which will subtract\\nthe end date from the start date.\\nLet me go ahead and just confirm\\nthe data types for all of the columns.\\nSo City is still string.\\n\\nWe've got a datetime for the start and end date.\\nAnd the duration has a timedelta data type.\\nOkay, so we've seen how to use\\nthe two datetime to convert objects to dates.\\nAnd then calculate the difference between these dates.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5911594\",\"duration\":360,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Combining DataFrames\",\"fileName\":\"4493047_en_US_03_09_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":495,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Sometimes you need to update DataFrames with new information for analysis. After watching this video, you will be able to combine DataFrames.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12406655,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] You know that feeling\\nwhen you're trying to merge data from different sources\\nand it's like mixing oil and water?\\nWell, Pandas has a couple of helpful methods\\nthat allow you to combine the DataFrames together\\nwithout the drama.\\nSo our first DataFrame is start,\\nwhich has the Olympic city,\\nso that's London, Rio, and Tokyo,\\nand their associated start dates.\\nThe other DataFrame end has the Olympic City,\\nso that's London, Tokyo, and Paris,\\nand their associated end dates.\\nNow anytime I'm looking to combine DataFrames,\\none of the methods I'm going to be thinking about is\\nthe Concat method.\\n\\nSo let's take a look at the documentation\\nand you can see that the purpose of the Concat method is\\nto concatenate pandas objects along a particular axes.\\nSo if I specify that the axis equals zero, so that means\\nthat's along the rows, you can see that it's combined,\\nthe start and the end DataFrame.\\nAnd we've got two separate columns, one for city,\\nand then oddly another one for city.\\nAnd that's because one of the cities is spelt\\nwith a capital C and the other one has a low case C.\\n\\nSo if you go ahead and make the changes so that we have\\nboth DataFrames with the column name with a low case city,\\nand let's go ahead and try and concatenate the start\\nand the end DataFrame.\\nSo you can see now that we have a single column called city\\nwith the start date and the end date.\\nNow Nan corresponds with the fact\\nthat there are missing values there,\\nbut it looks like there's no intelligence in the way that\\nthese two DataFrames have been concatenated.\\n\\nSo for example, you can see that we have London\\nwith the start date\\nand I was hoping to have a table where it's able\\nto understand that the fact that we've got London\\nand to have the associated end date\\nand in the same way we've got the start date for Tokyo,\\nbut no end date even though that information is available.\\nSo you can see that Concat does not take into account\\nsome of the columns when you want\\nto be intelligent in the way\\nthat you're combining the DataFrames.\\n\\nSo let's take a look at the Concat with an axis equals one\\nbefore we take a look at a couple of other options.\\nAnd you can see that Concat with an axis equals one\\nconcatenates, the two DataFrames along the columns axis.\\nAnd so we have a repeat of the city,\\nthe start date and the end date.\\nNow let's look at merging the DataFrames\\nin a more intelligent way.\\nSo we have a start DataFrame,\\nthe end DataFrame,\\nand let's look at how we might want to be able\\nto merge these two DataFrames.\\n\\nSo what we get as an output of this is merging DataFrames\\nor name series objects with a database style join.\\nSo what we'll want to provide are things like the left data\\nframe, the right DataFrame, how we want to merge them.\\nSo are we looking for an inner join,\\nan outer join, and so on.\\nAnd I'll talk through the differences\\nbetween them in a couple of seconds time.\\nAnd then what's the key\\nor the common column in both of the DataFrames\\nthat we want to be merging on?\\nSo here I'm going to say PD merge.\\n\\nThe left DataFrame's going to be start,\\nthe right DataFrame is going to be end.\\nI'm going to be merging on the common column city\\nand I want to do an inner merge.\\nNow an inner merge is an intersection of the keys.\\nSo that's information\\nthat's available in both the DataFrames.\\nAnd if we take a look at the DataFrames,\\nwhat's common to both the city columns is London and Tokyo.\\nAnd that's why we end up with the associated start date\\nand end date information.\\n\\nNow merging with an outer join is a\\nsimilar operation except\\nthat we are going to be combining the information\\nfrom the cities.\\nSo if it's available in either the start city\\nor the end city, that information is going\\nto be available in that merged DataFrame.\\nAnd so we can see we have information for all\\nof the four cities here.\\nNow the reason that we have a nan or a missing value here is\\nbecause we don't have any information about the end date\\nfor the Rio games.\\n\\nAnd similarly, we don't have information about the start\\ndate for the Paris games.\\nAnd so if I go ahead to the start DataFrame,\\nyou'll be able to confirm that there is no Paris\\nin our start DataFrame.\\nAnd similarly, there is no Rio in our end DataFrame.\\nNow I can also do a left join.\\nAnd so what we're doing over here is we're taking all\\nof the keys from the left DataFrame, so that's London, Rio,\\nand Tokyo, and then merging with the right one\\nwith the corresponding values if they exist.\\n\\nAnd you can see that we've got corresponding values\\nfor the end dates for London and Tokyo,\\nbut no information for Rio.\\nAnd in the same way with the right join,\\nmy starting point are the keys in the cities\\nfor the right hand DataFrame, which is London,\\nTokyo, and Paris.\\nAnd then I'm going to add the associated start dates\\nfor those cities.\\nAnd again, you can see that we have a missing value\\nfor Paris\\nbecause we don't have any start date information for Paris.\\n\\nAlright, so we've seen a couple of different ways\\nof combining DataFrames with Concat and with merge.\\nAnd depending on how you want to be able to use\\nor work with these DataFrames will determine whether you\\nuse the Concat method or the merge methods.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5912590\",\"duration\":308,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Combining datasets\",\"fileName\":\"4493047_en_US_03_10_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":396,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"There are several considerations to keep in mind when combining two different datasets. After watching this video, you will be able to perform some of the necessary checks.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12608209,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Okay, make sure you are sitting down for this\\nbecause I've got some thrilling news.\\nAfter some serious data hunting\\nand wrangling, I've managed\\nto get my hands on some 2008 Olympics data.\\nCan you believe it?\\nWe're now sitting on a gold mine\\nof Olympic data spanning a whopping 112 years\\nfrom 1896 all the way to 2008.\\nLet's take a look.\\nSo this is our original data set\\nand let's go ahead and grab the new file.\\n\\nNow I'm going to go ahead and open it\\nbecause I don't know what format the data is in\\nand it looks like there are no missing rows.\\nAnd so we've got City, Edition, Sport and so on.\\nNow let me go ahead and take a sample\\nof our original Olympics dataset.\\nSo that's from 1896 all the way to 2004.\\nAnd it looks like we have the same columns except they seem\\nto be in different orders.\\nSo let's try and combine these two data frames\\nand we'll use pd.concat and we're going to try\\nand combine them across the axis-0.\\n\\nAnd let's see how things pan out.\\nAnd you can see this is pretty messy.\\nSo we've got year, city, sport\\nand then there seems to be a little bit of a duplicate here.\\nNow what I was hoping for is that we would have a table\\nwith all of these nicely lined up.\\nSo it looks like we're going to have\\nSo if I take a look at the new dataset\\nand I look at the column names, I've got nw.columns\\nand I've got City, Edition, Sport and so on.\\nAnd if I take a look at the old\\nor my previous dataset, I've got Year, City and Sport.\\n\\nBut it's clear that there's a couple of differences, right?\\nIt looks like in the old dataset, all\\nof the column names have a capital first letter.\\nAnd in the new dataset, all\\nof the column names have a lowercase letter.\\nSo I can go ahead and manually update that\\nand I've done that over here.\\nAnd let me go ahead and read that in.\\nSo what I want to say is I've got names equals\\nand I'm going to give it the names of my updated list.\\n\\nSo let me just call this new names or new_columns.\\nAnd we've got the updated list in this way.\\nOr the other way to do that instead\\nof manually updating each column\\nand changing the first letter to uppercase is\\nto use a list comprehension.\\nAnd what we're doing here is making the first letter capital\\nby using the capitalize method.\\n\\nAnd this will achieve exactly the same thing.\\nSo if I now go ahead and try\\nand combine these two data frames,\\nyou can see that things are starting\\nto look a little bit better.\\nWe've now down to 15 columns\\nwhere we originally had 21 columns.\\nSo things are starting to sync up a little bit better.\\nNow let's compare the old data frame\\nand the columns with the new one.\\nSo we still need to make a couple\\nof modifications to the column names.\\n\\nSo if example, instead of country code, we have NOC\\nand instead of event_gender,\\nwe should be having a space between them.\\nSo I'm going to go ahead and make a couple of those changes\\nto my NW columns and let's go ahead\\nand combine these two data frames now.\\nOh, brilliant.\\nSo now we have a table with 11 columns\\nand they seem to have lined up very nicely.\\nNow one thing I'm not sure about\\nis this column called results.\\nSo let's take a look at the values in that column.\\n\\nAnd so you can see that I've got the values 3, 1, 2\\nand there are a couple of nans here.\\nAnd and nan means it's not a number.\\nNow it looks like result\\nis a lot like position in my previous dataset.\\nSo I'm going to go ahead and drop that.\\nAnd let's go ahead and combine the two data frames.\\nAnd so now we have 10 columns across the axis-0.\\nAnd let's confirm the data types of this.\\nAnd unfortunately all of the data types seem\\nto be of type object.\\n\\nSo we're going to have to do a bit of pre-processing,\\nbut let's combine all of the stuff that we've done\\nand put it into a function\\nand we'll call that pre-process 2008.\\nSo we are taking in our 2008 CSV file.\\nWe're going to go ahead and read it.\\nWe don't need to do any manipulation there.\\nThere are no missing rows.\\nWe want to change the column names so that it satisfies\\nand maps to the old data set.\\nWe want to go ahead and drop result and we want to go ahead\\nand return that data frame.\\n\\nAnd this is the sample rows\\nthat we get as a result of that.\\nAlright, so we've got the new data,\\nbut we've had to massage it a bit and move things around.\\nIt looks like we'll need to do a few more checks\\nbefore we can combine the data sets together.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5911593\",\"duration\":342,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Working with missing data\",\"fileName\":\"4493047_en_US_03_11_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":489,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Many real-world datasets contain missing data. After watching this video, you will be able to handle missing data in pandas.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12711070,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In the world of data analysis,\\nmissing values are like puzzle pieces\\nthat fell under the table.\\nYou can still see the big picture without them,\\nbut it's just not complete.\\nSo let's go ahead and reuse our preprocess\\nfile from earlier.\\nAnd let's take a look at the info method.\\nSo we can see that we've got 2059 entries in total,\\nbut we seem to have a couple of non values.\\nAnd the reason I know that\\nis because if I have 2059 entries in all of these columns,\\nI've got 2033, 2025 and so on.\\n\\nBut there certainly seems to be some missing values.\\nNow in previous videos we've seen\\nthat missing values are called NaN or sometimes NA.\\nSo what you could do is hazard a guess and try\\nand look for that search string NA,\\nor perhaps let's go ahead and try and look for missing\\nand see if there's a method called missing.\\nSo you can see that we've got an empty list.\\nSo that means there are no methods\\nwith the substring missing.\\nSo let's go ahead and try na.\\nNow because we're going to be looking for na,\\nI'm expecting there to be a whole lot of other strings\\nthat are captured, things like name and names and so on.\\n\\nSo let's make a change to our search string.\\nLet's make sure that na is right at the end.\\nSo if I go ahead and modify that pattern condition,\\nyou can see that I've now got a couple of methods\\nthat seem to be more logical.\\nSo you've got dropna, fillna, isna, and so on.\\nLet's take a look at isna.\\nSo if I was to take a look at the city column, so nw.City,\\nlet's take a look at the documentation for the isna,\\nlet me just give myself a little bit more space.\\n\\nAnd you can see that the doc string says that the purpose\\nof this isna method is to detect missing values,\\nwhich is exactly what we want to do here.\\nSo let's take a look and see if City has any missing values.\\nAnd we know that it does\\nbecause we've got 2033 non-null entries,\\nbut we've got a total of 2059 entries in our data frame.\\nSo again, this is going to return a false\\nor a true for every single entry within that series.\\n\\nNow what I can also do is\\nbecause each false is represented by a zero\\nand a true is represented by a one,\\nI can go ahead and sum all the isnas\\nfor that particular column.\\nAnd so this means I have a total of 26\\nmissing entries because I've got 26 entries that correspond\\nto a false for the nw.City.\\nNow I can go ahead and take that\\nand put that into a data frame.\\n\\nAnd these are the 26 entries\\nwhere we don't have any information for the city.\\nAnd you can see that these are all NaN.\\nAnd so there are missing values for the city information.\\nNow this seems a bit odd\\nbecause we know that in the 2008 Olympics\\nthat this was held in Beijing.\\nSo let's go ahead\\nand take a look at the values that we have for City.\\nAnd you can see that we've got Beijing and NaN.\\n\\nSo if I take a look at the number of rows\\nand the number of columns, I've got 2059 and 10.\\nSo what I'm going to want to do is to see\\nwhat methods might make sense for me to be able to deal\\nwith these NA values.\\nAnd if I take a look at the methods,\\nfillna seems to be a reasonable candidate.\\nSo let's check out the documentation for fillna.\\nAnd you can see that the doc string says fill NA\\nor NaN values using the specified method.\\nNow since we know that this is data for the 2008 Olympics\\nand we know that this was all held in Beijing,\\nwe can go ahead and ensure\\nthat the entire city column is Beijing.\\n\\nAnd so what we can do is we can use the fillna\\nand give it a value of Beijing.\\nAnd when we then go ahead\\nand take a look at the unique values in city,\\nyou can see that they've all been filled up.\\nWe don't have any more NAs, and we are good there.\\nLet's take a look at the year.\\nNow for some strange reason,\\nthe year seems to have a floating point representation,\\nso that's like a decimal value rather than an integer.\\nLet's take a look at the options within the year.\\nAnd you can see that we've got NaNs again.\\nSo these are not the numbers, and 2008.\\n\\nSo let's go ahead and update the year with the value 2008.\\nAnd so now we've got the city with 2059 entries\\nand we've got the year with 2059 entries.\\nNow the reason that I'm not going to go ahead\\nand update the year to a type of integer\\nis because I'm going to do that\\nfor all of the columns later on.\\nAnd so I'm going to treat that as a batch\\nand I don't want to just go ahead\\nand update just the year right yet.\\nSo let's make those changes\\nto our preprocess_2008 function.\\n\\nSo our additions are that we've gone ahead\\nand updated the city with a fillna to Beijing\\nand we've updated the year with a fillna to 2008.\\nAnd when I take a look at the data types,\\nyou can see that I've got a float of 64 for the year\\nand everything else seems to be an object.\\nSo what we've done here is to take a look at a couple\\nof the columns where there have been missing values\\nand we've gone ahead and updated the city and the year.\\n\\nAnd in the next few videos, we're going to go ahead\\nand deal with some of the other columns.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5917321\",\"duration\":257,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Removing missing data\",\"fileName\":\"4493047_en_US_03_12_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":321,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"There are many ways of dealing with missing data. After watching this video, you will be able to remove missing data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10637364,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] There are going to be some instances\\nwhere instead of filling our DataFrame with missing data,\\nwe might actually want to go ahead and remove\\neither some of the rows or the columns.\\nSo this is our preprocess_2008 function\\nthat is starting to work with making changes to that file.\\nLet's take a look at the overall status for the columns.\\nSo we've got 2059 entries\\nand we still have a couple of columns with missing values.\\n\\nSo given that we have 2048 entries for sport,\\nthat means we have some missing values,\\nso let's take a look at the values that are missing.\\nAnd so we've got a whole load of missing values\\nfor sport, but this also includes discipline, athlete name,\\nNOC and so on.\\nSo it looks like for whatever reason\\nthat 2008 file has a couple of rows\\nwhere we've just got the city and the year\\nand no entries in terms of the athlete or the medals,\\nand so we probably want to get rid of these.\\n\\nAnd there are a couple of ways that we can do that.\\nSo we can use the dropna method and we can specify all,\\nand this will remove all of the rows\\nif all of the columns are NaN.\\nThe other option is to use dropna\\nand where we have how equals any,\\nso if any of the columns are NaN,\\nthen we're going to go ahead and delete that row.\\n\\nSo if we take a look at the missing values again.\\nNow what we'll probably want to do\\nis to be a lot more prescriptive\\nabout the rows that we want to delete.\\nSo let's take a look at the options available within dropna.\\nNow we know that dropna will remove missing values,\\nbut if we scroll down,\\nyou can see that there's the subset parameter\\nthat allows us to drop rows based on a list of columns.\\n\\nAnd this seems to be a very prudent way of removing data\\nbecause you are being very prescriptive\\nabout which columns can be empty.\\nSo if I take a look at nw columns\\nand I've got city, year, sport and so on.\\nNow when I do a dropna, I can specify a subset\\nand then I want to make sure that all of these rows\\nare deleted because sport, discipline, athlete name\\nand so on are all NaN.\\n\\nAnd so I display all of the column names\\nand then I can just go ahead and copy them from here,\\nso I want sport, discipline, athlete name and so on,\\nand I'm going to provide that as a parameter to subset\\nand I say how equals all.\\nAnd so what's going to happen here\\nis that this is going to delete all of the rows\\nthat have missing values for the corresponding columns.\\nSo by that I mean sport discipline, athlete name,\\nand all the way up to medal.\\nNow these changes don't take place automatically,\\nso I actually need to provide the original DataFrame here.\\n\\nAnd so when I make those changes here,\\nwe'll be able to see that now with the updated info method,\\nwe have 2048 entries in total\\nfor city, year, sport and so on.\\nAnd it looks like all of the entries\\nhave now rectified themselves,\\nand so we now have no missing values.\\nSo if I go ahead and make those changes\\nto the preprocess_2008 functions file,\\nwe've now added this new entry to the function.\\nSo that's where we're going to go ahead\\nand drop all of the rows where these corresponding columns\\ncorrespond to an NaN value.\\n\\nSo we've seen that sometimes\\nwe don't just want to be filling in the values\\nin a DataFrame,\\nsometimes it makes really good sense\\nto actually drop some of the rows,\\nespecially when the majority of the columns are empty.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2719579\",\"duration\":190,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Working with duplicates\",\"fileName\":\"4493047_en_US_03_13_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":531,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Many real-world datasets have duplicates and you need to know how you want to deal with them. After watching this video, you will be able to remove duplicate data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7378900,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Have you ever been at a magic show\\nwhere the magician pulls the same rabbit\\nout of the hat multiple times?\\nWell, having duplicate data in your pandas DataFrame\\nis kind of like that,\\nexcept it's not nearly as funny or entertaining.\\nSo let's take a look at our DataFrame nw,\\nand you can see that we've got 10 columns and 2048 rows.\\nNow, if we look at the duplicated method,\\nand we sum them.\\n\\nActually, before we do that,\\nlet's just take a look at\\nwhat the duplicated method is all about.\\nAnd you can see that the duplicated method\\nreturns a boolean series denoting the duplicate rows.\\nAnd so, you can see that we've got six duplicates here.\\nAnd so if I want to go ahead and display them,\\nthese are the six rows that are duplicates.\\nYou can see we've got identical entries\\nfor every single row.\\n\\nSo we've got 2048 rows and 10 columns.\\nSo if we go ahead and drop the duplicates,\\nwe should find that we're now at 2042 rows,\\nwhich is exactly what we have here.\\nNow, let's say I want to use the duplicated method\\nto help me achieve something totally different.\\nLet's see if we can determine if there are any athletes\\nwho have won medals in multiple events.\\nSo if we take a look at the athlete_multiple_events,\\nand we do an nw.duplicated across just the athlete name,\\nthe NOC, and gender.\\n\\nAnd let's take a look at the results of that.\\nAnd because of the way that the duplicated works,\\nby default, it'll keep the first instance\\nand the rest are duplicates.\\nSo the reason that we are seeing mostly single entries here\\nis because the first is kept.\\nAnd so these are athletes who have won two medals\\nin these games.\\nNow, let me go ahead and sort by athlete name,\\nand this will allow us to see if there are any athletes\\nwho have won medals in multiple events.\\n\\nNow, if we take a look at Kai Zou, for example,\\nyou can see that he has won medals in the team competition\\nand the horizontal bar.\\nSo that means he's won three medals.\\nThe first being the instance that's kept,\\nand the rest being the duplicates.\\nAnd so if we take a look at Kai Zou,\\nso if we look up his name,\\nwe'll be able to see that we've got the three entries.\\nFloor exercises, horizontal bar, and team competition.\\n\\nSo, so much for pulling rabbits out of a hat,\\nwe've really gone down a rabbit hole.\\nSo let's head back to modify our pre-process function.\\nAnd what we'll want to do is we want to go ahead\\nand drop any duplicates.\\nAnd so we add that to our function.\\nAnd so now we have a DataFrame\\nwhere the duplicate entries have been removed.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5917320\",\"duration\":429,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Validating data\",\"fileName\":\"4493047_en_US_03_14_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":452,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"One important skill is to be able to validate data before incorporating it into your dataset. After watching this video, you will be able to use multiple tools to validate data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":17133780,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Data validation is about as exciting\\nas going through security at the airport.\\nIt might seem tedious, but it's absolutely crucial\\nto ensure a smooth journey ahead.\\nNow, based on your dataset, you'll want\\nto be thinking about edge cases,\\nand making sure your assumptions about your data are valid.\\nLet's go ahead and get our original functions\\nand the function for our new dataset.\\nNow, as part of data validation, we want to make sure\\nthat there are unique values\\nfor columns such as event gender, gender, and so on.\\n\\nSo we can see that for our new dataset,\\nwe've got event gender as men, women, and mixed.\\nFor gender, we've got men and women.\\nNow we want to check a few edge cases.\\nSo we want to make sure that if the event gender is for men,\\nthen we don't have any athlete who isn't a man taking part.\\nAnd so when we try and display this DataFrame,\\nwe should get no entries.\\nAnd we want to do exactly the same thing for women.\\nNow we want to check if the event gender is not for women\\nand not a mixed event,\\nthat means the event gender is for men,\\nthat we don't have women athletes taking part.\\n\\nAnd similarly, if the event gender is not for men\\nand not a mixed event,\\nthat means the event gender is for women,\\nthen we don't have men taking part.\\nSo our new dataset seems to be in good shape.\\nLet's go ahead and check our original dataset,\\nbecause we didn't have many of these skills\\nwhen we started off.\\nSo I'm going to work through this reasonably quickly,\\nbecause we've done these tests a couple of minutes ago.\\nSo for our old set,\\nwe've got the event gender,\\nwe've got the different unique values for the gender.\\n\\nAnd then I'm just going to pass\\nthrough the different edge cases,\\nand I'm expecting to get no entries\\nfor all of these edge cases.\\nNow, very surprisingly, we seem to have the case\\nwhere the event gender is not for men and not a mixed event.\\nSo this is an event gender for women,\\nbut we have a man taking part.\\nAnd if we take a look at the entry,\\nthis is Joyce Chepchumba taking part in a marathon.\\nThe event gender is for women, but for some strange reason,\\nwe have Joyce Chepchumba having a gender as a man.\\n\\nAnd if we were to go ahead\\nand look Joyce Chepchumba up on a search engine,\\nyou'll find that she is in fact a woman.\\nAnd so this seems to be a data entry error.\\nSo let's take a look at the Olympics 2000\\nwith the marathon event.\\nAnd you can see we've got all of the results for the men.\\nSo we've got a gold, silver, and bronze,\\nand we seem to have two bronze medals for the men.\\nAnd this is because we've got this incorrect entry\\nfor Joyce Chepchumba, which is the second row.\\n\\nSo let's go ahead and make a change to that.\\nSo if we take a look at index 24676, which is this row,\\nwith the gender, we're going to change that to women.\\nAnd let's re-look at our DataFrame.\\nAnd so we can see now that we have entries\\nfor gold, silver, and bronze,\\nand we have the correct genders\\nand the associated event gender.\\nNow let's go ahead and make that change\\nto our original function.\\n\\nSo we're going to add that change right\\nat the end of that function.\\nSo we have that as part of our pre-processing step,\\nbecause remember, one of the best practices is\\nto never make changes to the original CSV file.\\nInstead make changes to a pre-processing file.\\nAnd let's just confirm\\nthat it hasn't messed up our data types.\\nAnd you can see that we've still got the integers,\\nstrings, and the categories.\\nNow let's go ahead and try\\nand automate the comparison between the different values\\nfor the old dataset and the new dataset.\\n\\nSo the new dataset has a year of 2008.\\nWe have no missing values there.\\nWe have no missing values for the city.\\nIf we look at the event agenda for the new dataset,\\nwe have men, women, and mixed events.\\nLet's convert that to a list by using the tolist method.\\nSo we now have men, women, and the mixed gender events.\\nAnd let's do that for the previous dataset.\\nSo that's the oo dataset.\\n\\nSo we've got men, mixed, and women.\\nSo what we're trying to do is\\nto try and automate this task,\\nso that we can compare the columns from the old dataset\\nto the columns from the new dataset.\\nAnd so if I compare these two lists,\\nI surprisingly get the output that they aren't equal.\\nAnd the reason for that is,\\nbecause the second entry and the third entry don't match,\\nbecause they've been swapped around.\\nSo what we'll want to do is we'll want\\nto sort the entries in the list, and then compare them.\\nAnd so we can use the Python sorted command\\nto help us achieve that.\\n\\nSo I now have the event agenda sorted in both cases,\\nand now I expect there to be no problems.\\nSo this should print out, Passes all tests.\\nNow we're going to do exactly the same tests\\nfor the gender and the medal.\\nAnd this is what I mean\\nabout being able to automate this task,\\nwhere we want to do exactly the same tests\\nacross the multiple columns.\\nAnd we've ended up with a problem again.\\nNow this time we've got problems with the medal column.\\nIn our original dataset,\\nwe had medals as bronze, gold, and silver.\\n\\nAnd in our new dataset, we have bronze, gold, and silver.\\nNow you'll notice that the first letter is not capitalized\\nfor our new dataset.\\nSo what we'll want to do is we'll want to capitalize it\\nand make those changes to our column name.\\nSo once I've made those changes to my column,\\nI should be able to see that all the tests have passed,\\nand we can confirm that that is in fact the case.\\nLet's do the same thing for the sport.\\n\\nSo the new dataset has all\\nof the different sports in lower case.\\nIn our original dataset, it was capitalized.\\nAnd we're going to have to change the format,\\nso both of these datasets match up.\\nIf I take a look at the countries participating, you can see\\nthat they're all in uppercase for the new dataset,\\nand that matches what we saw in the previous dataset.\\nSo I'm going to combine all\\nof the changes that I need to make over here,\\nand let's go ahead and add this\\nto our preprocess_2008 function.\\n\\nAnd so I'm going to add them all at the end of this function.\\nAll right, so we've taken a look\\nat a couple of the columns in these datasets,\\nand we've been able to confirm\\nthat we have the same sort of values\\nand we have the same format\\nfor both the old and the new dataset.\\nIn the next video,\\nwe'll take a look at the remaining columns.\\n\"},{\"urn\":\"urn:li:learningContentVideo:2719577\",\"duration\":287,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Updating the dtypes\",\"fileName\":\"4493047_en_US_03_15_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":202,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Many times the data format isn't in the format you require. After watching this video, you will be able to change the data format for some of the columns.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9645242,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Okay, so we are still working our way\\nthrough airport security.\\nImagine you're at the stations\\nwhere you have to do a bit more work,\\nremove your laptop from your backpack, remove your shoes.\\nWell, it's all good, but we've got the equivalent,\\nslightly harder work coming up for our dataset.\\nNow, earlier, remember how we said\\nthat lists have square brackets,\\nand each item is separated by a comma.\\nSo we've got a list here of medals with five entries,\\ngold, silver, bronze, bronze, and gold.\\nWell, it turns out that if I want to access the first entry,\\nI need to just provide the name of the list\\nand provide the square brackets,\\nand zero is my first entry.\\n\\nAnd so I should be able to see gold.\\nMy second entry is given by an index of one,\\nand so I should get an output of silver.\\nAnd if I were to determine the length of this list,\\nthen I can use the len method.\\nAnd I've got five entries here,\\ngold, silver, bronze, bronze, and gold.\\nNow what we're going to be working on here is the athlete name.\\nSo let's take a look at what this looks like.\\nAnd you can see that we have the family,\\nor the surname first in capitals, followed by a comma,\\nand then the first name.\\n\\nSo what we'll want to check in terms of formatting\\nis if our new dataset has the same format,\\nat least for the family name as the older dataset.\\nWe're not going to worry about\\nthe rest of the names for now.\\nNow, if you're familiar with Python,\\nyou'll know that if you want to separate\\nout different strings, you can use the split command.\\nSo let's check if there's an equivalent\\nsplit command in Pandas.\\nAnd you can see that there is a split method.\\nAnd let's take a look at what the split method does.\\n\\nSo you can see that it returns a list\\nof the substrings in the string\\nusing the separator for the string.\\nSo what we're going to use as our separator\\nis the comma and the space.\\nAnd then this means that we will then have each of the names\\nreturned as a list.\\nThe first entry being the family name or the surname,\\nand the second entry being the first name.\\nSo you can see that we've got a list for each entry\\nwith the family name and the first name.\\n\\nNow let's go ahead and convert list to a list\\nby using the tolist method.\\nAnd I'm going to store this in a variable called athlete_names.\\nNow what I'm after is to be able to confirm\\nif the family or the surname is in capitals.\\nNow, from my knowledge of basic Python,\\nI know that if I want to confirm\\nif something is all capitals,\\nI need to use the upper method.\\nSo I'm going to go ahead and check\\nif there's a upper method in Pandas,\\nand you can see that there is an isupper,\\nand that's exactly what we'll want.\\n\\nSo let's take a look at how we might go ahead and do this.\\nNow, what we'll want to check is only\\nthat first entry in the list is in capitals.\\nSo this list comprehension will allow me to get\\nall of the athlete names,\\nand I then want to go ahead\\nand just pick up the first entry.\\nAnd so I can do that by providing an index of zero.\\nAnd now I've only got the surnames.\\nNow what I want to then do is to confirm\\nthat these are in uppercase\\nfor all of the entries in my column.\\n\\nAnd I can do that by using the isupper method.\\nAnd what I have is a list of Booleans.\\nNow I want you to pause the video here\\nand try and figure out how you will be able to confirm\\nif a list of Booleans\\nhas all of the family names in uppercase.\\nNow, one of the ways you can do this\\nis to sum up the length of that list.\\nAnd so in this case, we get 2,042 entries\\nwhere the first name, or the family name, is in uppercase.\\n\\nNow, if this number is the same as the number\\nof entries in the athlete name,\\nthen we know that all of the names are in uppercase\\nand we don't need to make any changes to our function.\\nAnd you can see that we've got 2,042 entries in total,\\nwhich correspond to the number of athlete names.\\nThis means that the new dataset has the same format\\nfor the family names as our previous Olympics dataset.\\n\\nAlright, so we did a lot of work here,\\nbut it turns out that all the new data is in good shape\\nand we don't need to make any changes\\nfor the athlete names after all.\\nIt's a lot like passing through airport security\\nfor a smoother journey ahead.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5911591\",\"duration\":136,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Combine the datasets\",\"fileName\":\"4493047_en_US_03_16_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":172,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"In order to combine datasets accurately\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5151081,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- So far, for the old dataset and the new dataset,\\nwe've confirmed that the column names\\nmatch in terms of the formatting.\\nSo we need to update our functions to reflect that.\\nAnd then we also need to go ahead and update the data types.\\nSo in our original pre-processing function\\nfor our older Olympic dataset,\\nwe make changes to the sport discipline and event\\nensuring that these are all lowercase\\nand that the Olympic countries is uppercase.\\n\\nAnd similarly, we make those changes to\\nthe corresponding fields in our new dataset.\\nSo let's go ahead and run that cell.\\nSo if we look at our data types for our new dataset,\\nwe can see that we have primarily objects and floats.\\nSo we need to ensure that they match the original dataset,\\nwhich correctly has integers, strings and categories.\\nNow, unfortunately, unlike our original dataset\\nwhere we could provide the dataset\\nas part of reading in the CSV file,\\nbecause our new dataset had missing values,\\nwe can't read in the new data types\\nas part of reading in that file,\\nand so we need to specify that after reading in the file.\\n\\nAnd this is what we do in our function.\\nSo after providing the necessary formatting\\nin our new pre-process 2008,\\nwe're going to go ahead and specify the correct data types\\nand you can see that we are doing that for\\nthe city, the year, the sport, and so on.\\nNow, go ahead and pause the video here\\nand make sure that you're comfortable\\nwith all of the different columns and the data types\\nthat we've assigned them to.\\n\\nSo if I now go ahead and take a look at the new data types\\nafter the pre-processing function,\\nyou can see that they match with the data types\\nthat we had for the original Olympics dataset.\\nSo now both the formatting for the columns\\nand the data types match\\nand we are ready to combine these two datasets.\\n\"}],\"name\":\"3. Intermediate pandas Techniques\",\"size\":174458844,\"urn\":\"urn:li:learningContentChapter:5911599\"},{\"duration\":1816,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5917319\",\"duration\":258,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Plotting data\",\"fileName\":\"4493047_en_US_04_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":324,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Matplotlib integrates with pandas seamlessly. After watching this video, you will be able to demonstrate using matplotlib with pandas.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9892871,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's be real,\\nstaring at rows and columns of data\\nis about as exciting as watching paint dry,\\nbut using matplotlib,\\nwhich is a Python visualization library,\\nbrings your data to life.\\nSo I'm going to go ahead and pre-process both the data frames\\nand we're going to import the library,\\nso that's import matplotlib.pyplot\\nand we're going to give it the alias, PLT.\\nAnd so we're going to learn about plotting\\nby answering a question.\\n\\nSo for the first Olympics, how many events were there\\nfor each of the different sports?\\nPlot them using different graphs.\\nSo if I want to go ahead and get the first Olympics,\\nthen I have the updated data frame or my updated dataset.\\nSo, up.year.\\nAnd that equals 1896.\\nI can go ahead and determine all of the different sports\\nthat were involved.\\n\\nNow I want to be able to plot the different sports involved.\\nNow, matplotlib is well integrated with pandas\\nand so let's take a look\\nand see if there is a plotting library available here.\\nAnd you can see we end up with a object plot not found.\\nSo, let's try and understand\\nwhat type the first_games.Sport.value_counts is.\\nAnd we know that this is a series,\\nand so sometimes if you want to go ahead\\nand try out some of the documentation,\\nyou might have to go back to the basic building blocks.\\n\\nAnd so we know that this is a series.\\nSo if I take a look at the methods available\\nwithin the series,\\nyou can see I then get a couple of options\\nand you can see now that we're able to find the plot method.\\nSo let's go ahead and find the documentation for plot.\\nAnd you can see this makes plots of series or data frames.\\nAnd so now what I'm going to do is I'm going to go ahead\\nand plot a line.\\nAnd so I say first_games.Sport.value_counts\\nand I want to go ahead and plot this line.\\n\\nYou can see it does a reasonably good job.\\nWe've got a line plot with the values\\nor the numbers of sports in the y axes\\nand the actual sports and the x axes.\\nNow that's difficult to read,\\nbut if I go ahead and check out the documentation\\nand try and figure out how I might be able\\nto make this image or this figure larger,\\nyou can see that there's a parameter called figsize,\\nwhich is a tuple of the width and the height,\\nand I can use that to make this a larger image.\\n\\nSo let me just go ahead and close the documentation\\nand let's use figsize of 10,3.\\nAnd you can see that we get a much bigger plot\\nthat allows us to see what the x axis is about more clearly.\\nNow, what I can also do is I can go ahead\\nand separate that instruction across several lines\\nand I'll get exactly the same result.\\nAs you said before,\\nthe reason you'll do that is because it allows you to\\ntroubleshoot this.\\nSo for example, if I don't want to be able to see the plot\\nand I just want to see the original series or the value count,\\nI can go ahead and get the different components\\nand the numbers for the sport.\\n\\nI can also go ahead and use different graphs.\\nI can use a bar graph.\\nAnd there are two ways to get a bar graph.\\nOne is to use the plot method and say kind = bar,\\nor I could use plot\\nand then there is a sub method called bar\\nand I'll get exactly the same output here.\\nSo you can use whichever one seems more logical to you.\\nI can also go ahead and plot different kinds of graphs.\\nSo I can use a horizontal bar.\\n\\nI can go ahead and change the colors for the bars,\\nso instead of them being a default of blue,\\nI can change that to red.\\nAnd if I want, I can go ahead and alternate the colors.\\nSo I specify blue and red in a list,\\nand I'm able to do this.\\nSo you can see that we can bring our data to life\\nby using library search as matplotlib.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5911592\",\"duration\":287,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Working with colormaps and seaborn\",\"fileName\":\"4493047_en_US_04_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":623,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Colormaps allow you to adopt a more intuitive color scheme for your dataset. After watching this video, you will be able to choose and use your own colormap.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11617781,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Seaborn is a visualization library\\nbased on Matplotlib.\\nSeaborn has an excellent examples gallery\\nthat provides plots on the website.\\nAnd if you click on any of the plots,\\nit provides the code to generate that plot.\\nNow, one of the reasons to use Seaborn\\nis that it produces beautiful statistical plots.\\nIt's very important to realize\\nthat Seaborn is a compliment\\nand not a substitute to Matplotlib.\\nNow, one of the advantages, again, with using Seaborn\\nis that it works very well with pandas.\\n\\nSeaborn, as with Matplotlib, has methods\\nfor bar plots, histograms, and pie charts.\\nLet's take a look at an example using Seaborn.\\nSo we've got our original dataset\\nand the associated pre-processing functions.\\nLet's use the example that we used in the previous video.\\nSo this is just the games for the 2008 Olympics.\\nAnd we're going to import Seaborn and give it an alias sns.\\n\\nNow, the reason we do this\\nis because if you were to go online,\\nyou'll find that this is the common nomenclature for Seaborn\\nand plt for Matplotlib.\\nNow let's take a look at an example\\nof one of the methods called count plot.\\nNow, count plot has very similar parameters to Matplotlib.\\nThe data parameter for count plot\\nis where you provide the data frame\\nor the source for the data,\\nand then you can plot different columns.\\nNow because Matplotlib is built on top of Seaborn,\\nI can also give the plot a title using plt.title.\\n\\nAnd so you can see the title over here,\\nMedals from the 2008 Games.\\nNow, I can also reorder the bar chart\\nso that the gold shows first\\nand then the silver, and then the bronze.\\nAnd so I do that by using the order parameter.\\nSo now that we've got the plot in the correct order,\\nlet's switch gears and talk a little bit about color maps.\\nColor maps allow you\\nto find a good representation for your data.\\nSo is there an intuitive color scheme for the data?\\nFor example, gold and silver and bronze for medal winners,\\nor blue for male and pink for female.\\n\\nThere are a couple of different classes of color maps.\\nThe sequential should be used\\nfor representing information that has ordering.\\nSo there's a change in lightness, often over a single hue.\\nDiverging is to be used when the information being plotted\\ndeviates around some middle value.\\nSo here there are often two different colors being used.\\nCyclic or cyclic color maps are good for representing things\\nthat take place in cycles like the time of the day.\\nAnd finally, the qualitative class is used\\nto represent information\\nwhich doesn't have any ordering or relationship,\\nand is often miscellaneous colors.\\n\\nNow it's important to remember\\nthat the most common form of colorblindness\\ninvolves differentiating between red and green.\\nSo avoid color maps with both red and green\\nwill avoid many problems in general.\\nLet's head back to the notebook.\\nNow Seaborn is excellent for categorical data.\\nNow, categorical variables are the ones\\nthat can take a fixed number of values.\\nSo in the Olympics dataset, we've had a couple of examples\\nlike the medals are gold, silver, or bronze, and so on.\\nNow the hue is for the categorical variables.\\n\\nSo the hue allows you to specify a categorical variable\\nin a different color.\\nSo let's go ahead and take a look at what this looks like.\\nThe order parameter allows you\\nto determine the sequence of the categorical variables.\\nAnd finally, I also have the palette option.\\nThe palette parameter allows you to specify colors\\nfor the different levels of the hue variable.\\nSo what I've done here is to pick a color map\\nthat allows me to have a blue and a pink.\\n\\nSo here's one with a palette that's called seismic.\\nI've tried a couple of other examples.\\nSo if I try coolwarm,\\nI can then again get the blue for the men\\nand the orange or pink for the women.\\nAnd finally, let's try a pallet called BWR.\\nNow, I think this one works the best.\\nAnd if I wanted to reverse the color maps,\\nall I need to do is add an _r suffix to swap them around.\\n\\nSo when would you use Seaborn versus Matplotlib?\\nNow if you're running a couple of scripts,\\nthen Matplotlib with pie plot is easy to use\\nand great for bar charts and pie charts and so on.\\nIf you're looking to write short scripts\\nto deal with things like categorical data\\nor more advanced statistical plots,\\nor creating other kinds of plots like heat maps,\\nthen Seaborn is going to be your tool of choice.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5915590\",\"duration\":400,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Working with groupby\",\"fileName\":\"4493047_en_US_04_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":669,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"A groupby operation involves some combination of splitting the object, applying a function, and combining the results. After watching this video, you will be able to demonstrate how to use groupby.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13889126,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Have you ever found yourself\\nstaring at a spreadsheet filled with data\\nwishing you could make sense of it all\\nwith just a few lines of code?\\nWell, that's where you get the power of Pandas' groupby.\\nNow groupby is one\\nof the most important functionalities available in Pandas,\\nand it does three things.\\nIt splits a DataFrame into a group based on some criteria.\\nIt applies a function to each group.\\nAnd then it combines the results into a DataFrame.\\nLet's take a look at this.\\nNow, one of the events I love in the Olympics\\nare the sprints,\\nand I'm talking about the 100-meters and 200 sprints.\\n\\nSo let's take a look at the 100-meters\\nand 200 sprints for the 2008 Olympics.\\nAnd so we've got the results over here.\\nAnd we've got the results for both men and women.\\nAnd one of the things I can try and do is\\nto group by the country, the gender, and the event.\\nAnd you can see the result of this is a groupby object.\\nNow let's go ahead and try and visualize this,\\nand we're going to head over to pandastutor.com\\nto help with that.\\n\\nPandas Tutor is great for visualizing many\\nof the methods available in Pandas.\\nAnd so you can see\\nAnd so we've got our 12 rows here, which are the entries\\nfor the 100-meters and 200-meters sprints\\nfor both men and women.\\nNow if I want to go ahead and visualize the groupby operation,\\nI'm just going to uncomment this line,\\nand I'm going to groupby and then perform the count operation.\\n\\nAnd you can see what's happening here\\nwith the groupby operation.\\nSo we're splitting this DataFrame,\\nand so that's the sprints DataFrame, into groups.\\nAnd our group here is the NOC,\\nso that's the country they represent,\\nthe agenda, and the event, and that's given\\nby the columns that we have over here.\\nWe then apply a function,\\nand in this case it's the count function,\\nand it combines all of the results in this DataFrame.\\nSo let's see how Pandas comes up with this DataFrame.\\n\\nSo if you take a look at Walter Dix, you can see\\nthat his 100-meter bronze is responsible\\nfor this entry over on the right.\\nIf I was to scroll down to the next one,\\nso that's Usain Bolt and his 100-meter gold,\\nyou can see he's responsible for the 100-meter gold.\\nIf I was to head down a little bit further,\\nbecause we've got three medal winners from Jamaica,\\nthey're all responsible for that entry,\\nand that's why we have three entries\\nfor the 100-meters that you can see in this DataFrame.\\n\\nAnd if we were to work our way down, so you can see\\nthat Walter Dix and Sean Crawford are responsible\\nfor the count of two,\\nbecause they're both from the US,\\nand they were both in the 200-meter race\\nwhere they won the bronze and the silver medals.\\nLet's head back to our notebook.\\nSo now let's go back to our updated DataFrame\\nand let's group by year.\\nAnd you can see\\nthat the object that we have is a groupby object,\\nand we can get some more details on that type we wanted.\\n\\nNow we can go ahead and perform an operation\\non that groupby object.\\nAnd so we can perform a count, and we get this DataFrame.\\nNow there are two components to the groupby object\\nand we can go and pick them apart.\\nOne is the groupby key,\\nand the other one is the group value.\\nSo let's go ahead and take a look\\nat these separate components.\\nNow, because we're grouping by year,\\nthe groupby key is the year, so we've got the year 2004.\\nAnd then these are all of the entries\\nthat correspond to the 2004 Olympic Games.\\n\\nSimilarly, we've got the 2008 key,\\nand then these are all of the entries\\nthat correspond to the 2008 games.\\nNow let's take a look at what type that group value is.\\nAnd you can see that this is a DataFrame,\\nso we could go ahead and group by year,\\nand the operation we'd perform there is a count operation.\\nAlternatively, you could also use the size method.\\nNow size is similar to count,\\nbut it also includes null values in its count.\\n\\nAnd so you can see\\nthat we get a very neat looking series.\\nSo I could group by the year\\nand the country that the athletes represent,\\nand I could get a count.\\nNow if I just want to be able to see the medal column,\\nI can filter by the medal column by just providing that\\nas a list here, and then applying the count.\\nAnd you can see that we then get a series as an output.\\nI can get exactly the same information\\nby using the size method.\\n\\nAnd similarly, I can go ahead and group by the year,\\nthe country that's represented, and a medal.\\nNow I want you to pause the video here,\\nand try and figure out what this instruction will do.\\n(upbeat music)\\nSo what this does is it's grouping by the country\\nand then it's filtering by the year,\\nand then the function you're performing is the minimum.\\nSo what this is giving you is the first time each\\nof the countries received their Olympic medal.\\n\\nSo for example, you can see that AFG, which corresponds\\nto Afghanistan, won their first Olympic medal in 2008.\\nVIE, which corresponds to Vietnam,\\nwon their first Olympic medal in 2000.\\nZZX, which corresponds to the mixed group,\\nreceived their first medal in 1896.\\nNow correspondingly the max is the most recent year\\nthat each of these countries have received a medal.\\nNow, if I want to be able to combine a couple\\nof these statistical operations, so that's min, max,\\nand count together, I can use the aggregate function\\nand then combine them in a list.\\n\\nAnd so you can see that for each of the countries,\\nI filter by year, and then I aggregate the minimum,\\nthe maximum, and the count.\\nAnd so what this is saying, for example, is\\nthat Afghanistan won its first Olympic medal in 2008,\\nand the last time that it won an Olympic medal\\nwas in 2008 also.\\nAnd this is just based\\non the 1896 to 2008 dataset that we're using.\\nAll right, so we've seen that groupby is one\\nof the most important functionalities available in Pandas,\\nand it allows you to make sense of your data.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5912589\",\"duration\":379,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Reshaping data: Stacking, unstacking, and MultiIndex\",\"fileName\":\"4493047_en_US_04_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":735,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"stack() takes levels from columns to the index. unstack() returns a DataFrame with a new level of column labels. After watching this video, you will be able to demonstrate how both of these work.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12357911,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Imagine you're trying\\nto build a house of cards,\\nbut the cards are all scattered in a messy pile.\\nThat's kind of what it's like working with data\\nthat's not structured in the way you want.\\nNow, one of the ways you can order your data in Pandas is\\nto use the stack and unstack.\\nLet's head over to our notebook.\\nSo we're going to be working with the sprints data again.\\nSo, that's the 100 and 200 meters sprints for the men\\nand women in the 2008 Olympics.\\nNow, one of the best ways to understand\\nwhat stack and unstack does is\\nto see it at work visually first,\\nand then we'll come back to this notebook.\\n\\nSo let's head over to pandastutor.com.\\nSo we've got our data over here\\nand the first thing we'll do is we will count\\nthe values using size.\\nNow based on this group by operation,\\nso, that's the NOC gender and event.\\nWe have these corresponding counts.\\nLet's perform our first unstack operation.\\nSo, what's happening with an unstack is we are taking items\\nfrom the column over on the left\\nand moving them to the items over on the right.\\n\\nSo if you were to take a look at the series over in blue,\\nwe are taking these items\\nand we are distributing them among the men\\nand women columns over on the right.\\nNow, the reason that we have an NAN or a naughty number is\\nbecause we have no women runners in the a hundred meters\\nfrom either Trinidad or the USA who won a medal.\\nNow I can go ahead and perform the second unstack operation,\\nbut let's take a look\\nat what the unstack operation is doing.\\n\\nWe are taking the associated levels from the rows\\nor the index, and we are moving them over to the column.\\nSo when we perform successive unstack operations,\\nso this time we're going to be doing an unstack on the NOC\\nand let's go ahead and visualize that.\\nYou can see that the NOC column over on the left is\\nmoved over to the right.\\nAnd so if I was to just take a look at the entries in blue,\\nyou can see how they've been redistributed in the data frame\\nover on the right.\\nNow, as you can imagine, stacking is the reverse operation.\\n\\nSo we are taking the associated levels from the column\\nand we are moving them to the index of the row.\\nOr you can think of it this way with stacking,\\nwe are making the resulting data frame taller,\\nand with unstacking we are making\\nthe resulting data frame wider.\\nSo now that we have a good visual representation\\nof the unstack operation, let's head back to our notebook\\nand we'll review much of what we saw in the notebook.\\nSo we're doing a group by operation on the NOC gender\\nand event, and we're doing a count of that\\nusing the size function.\\n\\nLet's take a look at the documentation for unstack\\nso you can unstack with a couple of parameters.\\nNow, every time we have a naughty number value, we can fill\\nthat with a number that makes sense for that data set.\\nSo for us here with the Olympic medals, it would make sense\\nfor us to have a fill value of zero.\\nSo let's go ahead and check this out on our unstack.\\nSo we've got fill values of zero\\nand we are unstacking based on gender.\\n\\nNow, if we perform the unstack on both gender\\nand event, we get the resulting data frame.\\nNow alternatively, we could do an unstack on both gender\\nand event and we'll end up with the same result.\\nSo if we take a look at our original group by,\\nso we've got grouping by NOC gender and event\\nand the operation we're performing there is a size.\\nSo we end up with this series.\\nNow we can also unstack by specifying a level.\\n\\nSo instead of saying gender, we can say level one\\nbecause in Python counting, the first column corresponds\\nto zero, the second one to one.\\nSo an SP unstack level one will give us exactly the same\\noutput as unstacking by the gender.\\nNow let's go ahead and create a table called sprints table.\\nAnd what we're going to want to do here is we want to be able\\nto stack this sprints table,\\nand we're going to store this in the variable sprints NOC.\\n\\nSo this is our corresponding data frame.\\nNow you'll notice that we have an index that seems\\nto have these two components.\\nOne is Jamaica men, and the second one is Jamaica women.\\nSo if you want to be able to identify the Jamaican men\\nwho took part in the 100 and 200 meters, you'd want\\nto specify this section here\\nand you'll be able to get these results over here.\\nAnd so the way we do that is by using what's known\\nas a multi-index.\\n\\nAnd so this representation\\nfor this index over here is what's known as a topple\\nbecause they're in brackets.\\nAnd so if I want to get all of the results\\nfor the Jamaican men, I can do a sprints, NOC LOC,\\nJamaica comma men, and then all of the results over here.\\nAnd so we should expect to see the 100 meters\\nand the 200 meters.\\nNow, similarly if I just want to be able to get the results\\nfor the a hundred meters over here\\nand so that's just this result over here,\\nI can specify the row which corresponds to this multi-index,\\nJamaica and men and the column entry is a hundred meters.\\n\\nAnd this is where I'll get to the count of one.\\nNow, I can also identify each of the components\\nby using an eye look.\\nAnd so I can specify that I want the first row\\nand all of the entries in the first row,\\nor I want the first row and the first column,\\nand I end up with that count of one.\\nNow if you want to take a look at further documentation,\\nthen you can do so with the stack and the unstack.\\nAlright, so just a quick summary of that.\\nWith stacking, we're making our resulting data frame taller\\nbecause we're taking the associated levels from the column\\nand moving them to the index of the rows.\\n\\nAnd with unstacking,\\nwe are making our resulting data frame wider\\nbecause we are doing the reverse,\\nwe're taking the associated levels from the rows or index\\nand moving them to the column.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5914606\",\"duration\":42,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Visualizations\",\"fileName\":\"4493047_en_US_04_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Needs Challenge Bumper\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":57,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1273943,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat electronic music)\\n- [Instructor] Time for some challenge questions.\\nSo the first one is using a line graph\\nplot the number of gold medals won by the USA male\\nand female Olympians throughout the history of the Olympics.\\nDistinguish between the male\\nand female Olympians in the line graph using blue and pink.\\nSo blue for male and pink for female.\\nAnd the second question is using a bar chart\\nplot the five Olympians\\nwho have won the most gold medals from the dataset.\\n\\nSo that's 1896 to 2008.\\nand when there is a tie,\\nconsidered the number of silver medals\\nand then bronze medals.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5914605\",\"duration\":347,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Visualizations\",\"fileName\":\"4493047_en_US_04_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Needs Solution Bumper\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":609,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12515139,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(techno music)\\n- [Instructor] Now I'm going to show you just one way\\nof solving this challenge problem.\\nSo the challenge question is using a line graph,\\nplot the number of gold medals won by the USA male\\nand female Olympians throughout the history of the Olympics.\\nDistinguish between the male and female Olympians\\nin the line graph using blue for male\\nand pink for female.\\nSo we've run our pre-process functions.\\nNow because we want USA Olympians who've won gold medals,\\nwe filter by NOC of USA and medal being gold,\\nand we store that in the variable usa_gold.\\n\\nThe next thing we'll want to do is to group by the year\\nand the gender and get a count of that.\\nNow let's take a look at the index that we have here.\\nAnd you can see not surprisingly, this is a multi-index.\\nNow we're looking to plot a graph.\\nIt'll be quite interesting to see\\nwhat a multi-index graph looks like.\\nSo let's plot our line graph\\nand see what we get when we have a multi-index\\non the X axis.\\n\\nAnd you can see that we've got these tuples\\nwith 1896 and men, 1920 for men and so on.\\nSo clearly having a multi-index is not going to allow us\\nto get the line graph that we want.\\nSo let's go back to our original groupby,\\nand let's now go ahead and unstack this series\\nand we unstack it by gender.\\nSo we're moving the men and the women across to the columns.\\nNow the reason we do this is\\nbecause then this way we will be able\\nto plot separate line graphs for the men and the women.\\n\\nSo now we have in the index or on the rows, the year,\\nand over on the columns we have the men and the women.\\nAnd so you can imagine visually\\nthat over on the X axis we'll have the year.\\nAnd over on the Y axis we'll have\\nthe count for the men and the women.\\nSo let's go ahead and plot this line graph.\\nAnd this looks a lot more like what we're looking for.\\nSo you've got the men represented by blue\\nand the women represented by orange.\\n\\nNow if you remember, when we tried to get the documentation\\nfor the plot in this way, we always ended up\\nwith this message \\\"Object 'plot' not found.\\\"\\nSo we can go back to first principles and try\\nand determine what the objects we're working with are.\\nAnd so since we're working with a series object here,\\nwe can go ahead and try\\nand get the documentation for the plot in that way.\\nNow, if we want to try and change the colors\\nfrom orange and blue to blue and pink,\\nlet's see if there's a corresponding\\nparameter over here.\\n\\nAnd you'll find that if you look through the documentation\\nthat there's a parameter color.\\nAnd so we can experiment and if we try a color\\nand we provide a list, so blue and pink in a list,\\nwe'll be able to get the output we want.\\nSo what we'll have to do is experiment a little bit here\\nbecause you can see that here I've got both the men\\nand the women with,\\nand I've got it right the first time round.\\nSo I've got men in blue and women in pink.\\nI could have got it wrong the first time\\nand so I might have had to have swapped them around.\\nAnd so for example, what I could have had here is pink\\nand then blue and I could swap them around\\nand get the colors that I want in this way.\\n\\nLet's head over to the second question.\\nSo using a bar chart plot the five Olympians\\nwho've won the most gold medals from the data set.\\nAnd that's from 1896 to 2008.\\nWhen there's a tie, consider the number of silver medals\\nand then the bronze medals.\\nSo we're going to use our updated dataset here\\nand we're going to group by the athlete name and their medal.\\nIf you do a groupby\\nand they do a count of this, we get this groupby object.\\n\\nNow let's go ahead and separate this out.\\nAnd so we will have a size on one line\\nand then let's go ahead and unstack this.\\nAnd we remember to have the fill values so that we have\\nthat for any end values.\\nSo this data frame is starting to look quite promising.\\nThe next thing we'll want to do is we'll want\\nto sort the values by gold and then silver and then bronze.\\nAnd so we can do that using the sort_values method.\\nBut because we want the numbers with the highest number\\nof golds and then the highest number of silver\\nand then the highest number of bronze,\\nwe'll want to sort in reverse order.\\n\\nAnd so we'll say ascending equals false.\\nAnd so we have the highest number\\nof golds over here followed by silver and bronze.\\nNow, it would be helpful if we could actually get this data\\nframe so that we had first the gold\\nfollowed by the silver and then the bronze.\\nAnd so we can change the order of the columns\\nby providing them as a list over at the end here.\\nAnd so we now have the athlete names,\\nthe corresponding gold, silver, and then bronze.\\n\\nNow because we're looking for the top five athletes,\\nI'm going to use the head method\\nwith the default parameter of five.\\nAnd we're going to be explicit and specify five.\\nAnd so we have these five athletes\\nwho've won the most number of medals.\\nNow because we want to plot it as a bar plot,\\nI'm going to go ahead and chain the plot method.\\nAnd there we have it.\\nWe've got Michael Phelps leading the way,\\nfollowed by the most decorated woman,\\nwhich is Larissa Latynina and so on.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5911590\",\"duration\":103,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Creating your own colormaps\",\"fileName\":\"4493047_en_US_04_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":163,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Sometimes the default colormaps do not meet your requirements. After watching this video, you will be able to define your own colormaps.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3841785,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's go ahead and create our own color maps.\\nNow, our starting point is going to be\\nthe solution from the previous challenge.\\nSo if you haven't reviewed that,\\nyou probably want to take a look at that solution.\\nSo the result that we got for that was this bar graph\\nwhere we have the gold, silver, and bronze\\nfor the Olympians.\\nAnd you can see that we've got gold represented in blue,\\nsilver in orange, and bronze in green.\\nNow, this would be so much more effective\\nif you could actually use the colors\\ngold, silver, and bronze.\\n\\nSo let's go ahead and try and do that\\nand define our own color map.\\nNow, one of the classes available in Matplotlib\\nis the listed color map class.\\nLet's take a look at the documentation for this.\\nAnd you can see that one of the parameters is colors\\nwhere you can provide a list\\nof Matplotlib color specifications.\\nAnd so these can be RGB values or red, green, and blue.\\nSo let's go ahead and do exactly that.\\nSo I've gone over to the internet\\nand looked up the corresponding values\\nfor gold, silver, and bronze.\\n\\nAnd these are the corresponding Hexadecimal values\\nbecause these are the values that you will need.\\nAnd I've stored them in a list.\\nSo I'm going to pass this to the listed color map class,\\nand the result is going to be our color map\\nthat we're going to be using with our gold, silver, and bronze.\\nNow all we need to do is to update the last line\\nwhere we specify that we want to be using\\nour corresponding color map and we're able to get\\na far more compelling visual representation\\nusing gold, silver, and bronze.\\n\\n\"}],\"name\":\"4. Visualizations\",\"size\":65388556,\"urn\":\"urn:li:learningContentChapter:5911600\"},{\"duration\":536,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5910586\",\"duration\":49,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Final challenge: Recap\",\"fileName\":\"4493047_en_US_05_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Needs Challenge Bumper\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":51,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1507252,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Narrator] All right, we're onto our final challenge.\\nSo for each Olympic year present in the dataset,\\nshow the US Olympian\\nand their sport, who's won the highest number\\nof medals in that particular year.\\nNow, in the case of a tie, a gold is better than a silver,\\nwhich is in turn, better than a bronze medal.\\nInclude only one Olympian for each Olympic year.\\nSo this means if there are two Olympians in one year\\nwho've won exactly the same number and type of medals,\\nthen show only the first one, based on sorting the names\\nin reverse alphabetical order by surname.\\n\\nYou should show the following columns\\nfor each of the Olympic years.\\nSo that's the athlete name, the sport, and the total.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5915591\",\"duration\":487,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Recap\",\"fileName\":\"4493047_en_US_05_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Needs Solution Bumper\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":752,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":20816266,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] Now, this is just going to be one of the ways\\nto go about solving this problem.\\nThere are going to be multiple ways you can go about it.\\nSo we ensure that we have the data that we need\\nand we process our files.\\nSince we're just looking at USA Olympians,\\nwe want to be able to filter by the NOC USA.\\nAnd so we go ahead and do that.\\nAnd you can see that we've got about 4,300 rules.\\nNow, I then want to be able to capture information\\nabout the year, the athlete name, the medal, and the sport.\\n\\nSo let me go ahead and do a group by\\nand a size operation for that.\\nSo that will give me an effective count.\\nAnd now let's go ahead and move the medals\\nfrom this side over here in the index over to the columns.\\nAnd so we'll want to do an unstack operation.\\nAnd you can see as a result\\nthat we've gone from around 4,300 rows here\\nto more than two and a half million rows over here.\\n\\nAnd this is because we now have a row for each athlete.\\nSo you can see that we've got a multi-index,\\nbut we've got a row for each athlete.\\nAnd the vast majority of these rows are going to have\\na total number of medals that are zero\\nbecause the athletes haven't received medals for that sport.\\nIn fact, they didn't even compete in those sports.\\nBut that's okay because what we'll do later on\\nis to just sort them and filter these entries out.\\nSo let me go ahead and save this\\nas a data frame called table because we're going to reuse this.\\n\\nNow, the next thing I want to do\\nis I want to sum up the bronze, silver, and gold medals\\nand I want to store that in a column name called total.\\nAnd so I'm going to use the assigned method\\nand this will give me the total number of medals\\nfor each of these athletes.\\nNow, what I'd really like to do is to be able\\nto move out of these multi-indexes\\nand go back to where we have indexes\\nor indices that are just numeric values.\\n\\nAnd one way to achieve this is to just reset this index.\\nSo I'm just going to go ahead and reset that multi-index.\\nAnd let's take a look at what table looks like now.\\nAnd you can see that this is a much cleaner form\\nand you can see again that most of the rows have\\na total number of medals of zero\\nbecause we don't have these athletes\\ntaking part in these sports.\\nNow, if you remember, we can group by year,\\nand there are two pieces of information that we get back.\\n\\nOne is the key. So for example, that is year.\\nAnd the second component is the group,\\nwhich is a data frame.\\nSo let's go ahead and take a look\\nat what information we have in that group\\nor in that data frame.\\nAnd so for example, this is the grouping for the year 1996.\\nThis is the grouping or the data frame for the year 2000\\nand so on.\\nNow, what I'd like to do is to be able to sort\\nthis data frame by the total number of medals,\\nbut I want to be able to sort it in reverse order\\nso I have the highest number of medals over at the top.\\n\\nAnd so again, you can see that we've got the groups by 1996,\\n2000, and so on,\\nwith the highest number of medals over at the top.\\nSo we've got a total of four medals over here\\nand then heading down to three and then zero.\\nNow, what I'd like to do next\\nis because I want to also be considering the athlete's name\\nas part of the criteria,\\nI don't only want to just use the total\\ngold, silver, and bronze, but I need to include\\nthe athlete name in the event that there is a tie.\\n\\nSo I'm going to go ahead and do a sort values\\nand add the athlete's name.\\nAnd then I want to be able to capture only one entry\\nfrom each of those groups.\\nAnd so I can do that by using the head method\\nand providing a value of one.\\nNow, you can see that I've got something\\nthat is starting to take shape\\nand anytime that I can iterate through items like this\\nusing a for loop means that I can start thinking about\\ntrying to put this into a list.\\n\\nAnd the reason that I'd want to think about a list\\nis because if I have a list of data frames,\\nthen I know that I'm done because all I need to do\\nis to combine all of the items\\nand I can combine them into a single data frame\\nand that will be the answer that I want.\\nSo what I want to do is to just use this for loop\\nand convert this using a list comprehension\\nso instead, I have this as a list of these items.\\nAnd the way that I can do that,\\nand we've seen this a couple of times,\\nis to use a list comprehension.\\n\\nSo let me just call that winners\\nwhere I take that statement.\\nSo that group sort values all the way up to the first item\\nfor each year, and I'm adding that to my list\\nfor each of the years in the table.\\nAnd so let's take a look at the associated list\\nand let's make sure that we understand what this is.\\nSo let me just go ahead\\nand view the first item in this list.\\n\\nAnd you can see that the first item\\ncertainly looks visually like a data frame.\\nLet's just go ahead and confirm that\\nthat's in fact the case.\\nSo now I have a list of data frames.\\nSo I can easily combine them together\\nand I don't have to do any pre-processing on them\\nbecause they're all in the same format.\\nNow, when we had to update our original data set\\nwith the data from the 2008 games,\\nwe just concatenated our list of data frames together.\\nSo what's to stop me from concatenating\\nall of these games together to get a complete data frame?\\nAnd so let's go ahead and provide the winners list\\nto the concat method.\\n\\nAnd we have our data frame with the results that we want.\\nNow, there's just a little bit more work that we need to do\\nto just format this in the way that we need it.\\nSo let me go ahead\\nand ensure that I'm displaying the year,\\nthe athlete name, the sport, and the total.\\nNow, you'll notice that we've got\\na couple of random values over on the left,\\nand they're not random values.\\nThese are the actual index values\\nfrom when we did the unstack operation.\\n\\nSo let's just tidy this up a little bit\\nand let's get rid of these numbers and let's reset this.\\nAnd so we can do that by using the reset index method.\\nAnd that's great, except it seems to have added\\nan additional column called index.\\nSo what we'd like to do is to do a reset index\\nand drop this additional index.\\nAnd so let's take a look at the documentation\\nto try and figure out how we can do that.\\nAnd so if I go ahead and just do a reset index,\\nquestion mark, you can see that we end up with\\na reset index not found.\\n\\nSo let's go back to the first principles.\\nWe know that this is going to be a data frame.\\nSo if you do a PD data frame.reset index,\\nthat will allow us to get access to the documentation\\nfor reset index.\\nAnd now if we take a look at the parameters,\\nso this doc stream will reset the index or a level of it,\\nand we can go ahead and use the drop parameter\\nand that'll allow us to go ahead and drop that index column.\\nSo let's say drop equals true,\\nand let's go ahead and take a look at these values.\\n\\nAnd there we have it,\\na list of US Olympians who've won the most number of medals\\nfor that particular Olympic year\\nacross all of the Olympic games from 1896 to 2008.\\n\"}],\"name\":\"5. Learning Recap\",\"size\":22323518,\"urn\":\"urn:li:learningContentChapter:5917326\"},{\"duration\":39,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5910585\",\"duration\":39,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Your next steps in pandas\",\"fileName\":\"4493047_en_US_06_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":true,\"rawDurationSeconds\":48,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":779048,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Every four years\\nwhen the Summer Olympics are on,\\nI drop whatever I'm doing\\nand enjoy a fortnight of fabulous sport.\\nI'll be running a whole load of data analysis\\nand trivia competitions about the Olympics\\nright here on LinkedIn.\\nYou're welcome to connect with me on LinkedIn if you want to\\nstay in the loop and take part.\\nNow, my main job isn't coding,\\nbut do you know why I still code?\\nIt's the immediate feedback\\nthat I get on my problem solving skills.\\nThe skill to learn how to think clearly step by step\\nand keep improving is one of the biggest benefits of coding.\\n\\nSo learn how to code and keep coding.\\nIt doesn't matter whether it's Python and Pandas\\nor something else.\\n\"}],\"name\":\"Conclusion\",\"size\":779048,\"urn\":\"urn:li:learningContentChapter:5912593\"}],\"size\":442231658,\"duration\":11448,\"zeroBased\":false},{\"course_title\":\"Python for Data Visualization\",\"course_admin_id\":4499006,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":4499006,\"Project ID\":null,\"Course Name\":\"Python for Data Visualization\",\"Course Name EN\":\"Python for Data Visualization\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"&lt;p&gt;Data visualization is incredibly important for data scientists, as it helps them communicate their insights to nontechnical peers. But you don\u00e2\u20ac\u2122t need to be a design pro. Python is a popular, easy-to-use programming language that offers a number of libraries specifically built for data visualization. In this course from the experts at Madecraft, you can learn how to build accurate, engaging, and easy-to-generate charts and graphs using Python. Explore the pandas and Matplotlib libraries, and then discover how to load and clean data sets and create simple and advanced plots, including heatmaps, histograms, and subplots. Instructor Michael Galarnyk provides all the instruction you need to create professional data visualizations through programming.&lt;/p&gt;&lt;p&gt;This course was created by &lt;a href=http://www.onlymadecraft.com target=_blank&gt;Madecraft&lt;/a&gt;. We are pleased to host this content in our library.&lt;br&gt;&lt;br&gt;&lt;img src=https://media.licdn.com/media/AAYAAgCwAAoAAQAAAAAAAHppnBQxgeyWS2CsU3aDDPcMgw.jpg height=25% width=25%&gt;&lt;/p&gt;\",\"Course Short Description\":\"Build accurate, engaging, and easy-to-generate data visualizations using the popular programming language Python.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":\"21401000, 20517019\",\"Instructor Name\":\"Madecraft Licensor, Michael Galarnyk\",\"Instructor Transliterated Name\":\",\",\"Instructor Short Bio\":\"Full-Service Learning Content Company|Python Instructor and Blogger\",\"Author Payment Category\":\"LICENSED, NONE\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2024-01-16T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"No\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/python-for-data-visualization-2023\",\"Series\":\"One-Off\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Intermediate\",\"LI Level EN\":\"Intermediate\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Data Science\",\"Primary Software\":\"Python\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"Yes\",\"Visible Duration\":4883.0,\"Visible Video Count\":30.0,\"Contract Type\":\"LICENSED, NO_CONTRACT\"},\"sections\":[{\"duration\":133,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3371043\",\"duration\":73,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Effectively present data with Python\",\"fileName\":\"4499006_en_US_00_01_WL24\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":73,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"It is one thing to capture data, but to communicate and understand your data, you need to use tools that will help you visualize your data. After watching this video, you'll be able to see the value of data visualization using Python.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3130200,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - In this vast ocean of data, there hides many treasures.  \\n How do you find, let alone unlock, these treasure chests?  \\n Everyone is on the same quest, wondering how  \\n to leverage this data to gain understanding,  \\n see trends, and gain competitive advantages.  \\n Data science is the key to solving  \\n what businesses need today.  \\n Finding the right tool is critical.  \\n This is where Python comes in.  \\n You can create compelling and publishable visualizations  \\n using Python's powerful data science libraries.  \\n By creating your own visualizations, you'll be able  \\n to explore the data in a new way.  \\n My name is Michael Galarnyk.  \\n I'm a Data Scientist, Python Instructor,  \\n YouTuber, and Blogger.  \\n In this course, you will be able  \\n to create compelling data visualizations using Python.  \\n We'll take a look at some of the various tools available,  \\n and you'll learn how to manipulate data using Pandas.  \\n Also, you get a chance  \\n to create your own visualizations using Matplotlib.  \\n You'll review how to create box plots, heat maps,  \\n histograms, and more.  \\n By the end of this course, you'll feel confident and ready  \\n to go build your own powerful visualizations using Python.  \\n So, if you're ready to dive in, then let's go.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3374061\",\"duration\":33,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Before you start\",\"fileName\":\"4499006_en_US_00_02_MM24\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":33,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Creating data visualizations in Python requires some basic knowledge of lists, tuples, and dictionaries. After watching this video, you'll be able to determine if you know enough to take this course without reviewing additional material.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1329041,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - There are a few things I'd like you  \\n to be aware of as we get started.  \\n First, you should be able  \\n to understand basic Python data structures,  \\n such as lists, tuples, and dictionaries.  \\n Also, you get more out of this course if you have a basic  \\n understanding of Jupyter Notebooks.  \\n I'll be programming using Jupyter in this course.  \\n Having said that, don't worry if you're feeling a little  \\n shaky and unsure where to begin.  \\n As you proceed through this course, it'll provide you  \\n with a number of tips and resources.  \\n As you follow along, you'll quickly see how powerful  \\n Python visualizations can be.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3369055\",\"duration\":27,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Using the exercise files\",\"fileName\":\"4499006_en_US_00_03_MM24\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":27,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"The course includes a folder called Exercise_Files. After watching this video, you'll be able to utilize the course files and code, along with the videos.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":934594,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - A folder of exercise files has been included  \\n to help you make the most of this course.  \\n You will be using them throughout the course.  \\n Please download those now so that you're prepared  \\n for when you need them.  \\n Check out your operating system's download folder,  \\n find the downloaded course exercise folder,  \\n and unzip these files.  \\n You'll see this folder here,  \\n which contains the contents of this course.  \\n Once you have done this, you'll be ready to code along.  \\n \\n\\n\"}],\"name\":\"Introduction\",\"size\":5393835,\"urn\":\"urn:li:learningContentChapter:3369060\"},{\"duration\":337,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3372047\",\"duration\":58,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Value of data visualization\",\"fileName\":\"4499006_en_US_01_01_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":58,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Data visualization is all about helping you and your stakeholders understand data on a more intuitive level. After watching this video, you'll be able to decide how data visualization can help your goals.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1371334,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] At the core of it,  \\n data visualization is all about  \\n communicating results from data.  \\n It's about simplifying data,  \\n cleaning data with the end goal of helping you,  \\n and your stakeholders understand data  \\n on a more intuitive level.  \\n To bring this to life,  \\n I want to give you an example.  \\n Suppose you take out a loan,  \\n and you're given a payment table like this.  \\n When you look at this,  \\n what you really want to do is find out  \\n how much you're paying interest over time.  \\n Now, this table is not really  \\n a great way to see this data.  \\n Instead, we might look to create  \\n a visualization like this.  \\n And as you can see,  \\n you actually pay more interest  \\n at the beginning of a loan,  \\n and this wouldn't have been clear  \\n if we didn't see this graph.  \\n And just like this example,  \\n visualizations can help change our perspectives  \\n or behavior.  \\n Another way to look at this is by using  \\n a visualization like we see here.  \\n It's easy to see.  \\n The longer the term the loan,  \\n the more you end up paying an interest overall.  \\n And just like that,  \\n data visualizations can be really powerful,  \\n and that's a reason why you should use  \\n visualizations in your day-to-day work.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3372048\",\"duration\":178,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Leverage programming languages\",\"fileName\":\"4499006_en_US_01_02_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":178,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Data visualization is an iterative process, and the ability to quickly iterate on a visualization can save a lot of time. After watching this video, you'll be able to explain why you might want to use a programming language to visualize data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4348777,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] There are a lot of great reasons  \\n to use a programming language like Python  \\n to build data visualizations.  \\n First, there are a lot of different types of visualizations  \\n you can make,  \\n including boxplots, bar graphs, line graphs,  \\n histograms, et cetera.  \\n Second, using a programming language  \\n to build the data visualization  \\n allows you to quickly iterate  \\n and improve on a visualization,  \\n as well as try out many different visualizations.  \\n To get you started, I want you to first understand  \\n a data visualization process.  \\n You first have to have data, where you clean it,  \\n you simplify it, and sometimes even augment your data source  \\n if your data source is not rich enough.  \\n From there, you can use that understanding  \\n from your visualization  \\n to improve your data quality  \\n or to explore other aspects of your data,  \\n and this process can keep on going  \\n until you have a visualization  \\n that tells a compelling story.  \\n So there are a few common tools and libraries in Python  \\n to build data visualizations.  \\n In this course,  \\n we're going to utilize the Anaconda Python distribution,  \\n and second, I encourage you to use Jupyter Notebooks.  \\n That's the program I'll be using throughout this course.  \\n Next, there are two major plotting libraries in Python,  \\n Matplotlib and Seaborn.  \\n And lastly, we'll go over the NumPy and Pandas libraries,  \\n as before you can visualize data, you have to get it,  \\n you have to clean it, and sometimes even augment it,  \\n and NumPy and Pandas help you do it.  \\n So as far as why we use the Anaconda Python distribution,  \\n Python has a solid claim  \\n to be the fastest-growing major programming language,  \\n and Anaconda is the strongly recommended way  \\n of installing Jupyter Notebooks.  \\n So what Anaconda is really is a package manager,  \\n an environment manager, and Python distribution.  \\n Think of it as kind of like an app store for Python.  \\n Data visualization projects  \\n require many different packages and libraries.  \\n What's great is that Anaconda comes pre-installed  \\n with many of them.  \\n So what about Jupyter?  \\n Jupyter Notebooks contain both code and rich text elements,  \\n such as visualizations, links, and equations.  \\n So right here is a Jupyter Notebook in action,  \\n and you'll see a lot of that throughout this course.  \\n So I want to quickly tell you about NumPy and Pandas.  \\n Before we can plot data,  \\n we first need data in a portable form.  \\n NumPy and Pandas can be used to efficiently load,  \\n store, manipulate, and export in memory data.  \\n Pandas can also be used as a wrapper for Matplotlib,  \\n which brings us to Matplotlib and Seaborn.  \\n They're very popular Python plotting libraries.  \\n Matplotlib's API is relatively low-level.  \\n This allows for a lot of customization,  \\n but also a lot of code.  \\n A way around this is to utilize a Seaborn wrapper,  \\n which offers high-level graphics  \\n and integrates well with Pandas' library.  \\n It's also important to keep in mind  \\n that with every Matplotlib wrapper like Seaborn  \\n is still often useful to dive into Matplotlib's syntax  \\n to adjust the final plot output.  \\n And that's it.  \\n These are a couple common tools you can use  \\n to build data visualizations.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3373064\",\"duration\":101,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Overview of Jupyter Notebooks\",\"fileName\":\"4499006_en_US_01_03_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":101,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Jupyter Notebooks provide a powerful way to write and iterate on your Python code for data cleaning and visualization. After watching this video, you'll be able to open Jupyter Notebooks and execute some basic commands.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2729351,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] To get started with the Jupyter Notebook,  \\n you first have to open it.  \\n I'm working on a Mac, and on a Windows  \\n and a Mac, you can open up a Jupyter Notebook  \\n through Anaconda Navigator.  \\n I'll click on Anaconda Navigator.  \\n If you're using a PC,  \\n it should look pretty similar.  \\n To open it, I go ahead and click Launch.  \\n As you can see, Anaconda run a process,  \\n and it's open at Jupyter Notebook here in Chrome.  \\n Depending on what your default browser is,  \\n it could open up in a different browser such as Firefox,  \\n Safari, or Microsoft Edge.  \\n I'll go ahead and make this full screen.  \\n I'll click on Desktop.  \\n I'm going to open up a Jupyter Notebook  \\n by clicking New, Python 3,  \\n and traditionally, like you always do, I'll go ahead  \\n and print Hello World.  \\n I'll press Shift + Enter to run the cell,  \\n and there we go.  \\n Jupyter also allows you  \\n to do markdown cells, which are very useful  \\n for annotating your code.  \\n So to annotate your code, go to Sell, Sell Type,  \\n and click on Markdown.  \\n Using Markdown,  \\n I made a secondary header, and I said loading data.  \\n And the reason why I did that is that oftentimes,  \\n it's good to use markdown to organize your code  \\n and to communicate with feature users  \\n of your Jupyter notebook.  \\n In this case, it'll allow others to know  \\n that the cells below are for loading data.  \\n You can also add new cells by doing Insert Above  \\n or Insert Below.  \\n So really, that's all you can know right now.  \\n If you're new to Jupyter Notebooks, don't worry,  \\n you're going to learn a lot more as we go through this course.  \\n \\n\\n\"}],\"name\":\"1. Data Visualization Overview\",\"size\":8449462,\"urn\":\"urn:li:learningContentChapter:3372052\"},{\"duration\":2207,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3372049\",\"duration\":90,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Introduction to pandas\",\"fileName\":\"4499006_en_US_02_01_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":90,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"The pandas library is meant for data manipulation and analysis. After watching this video, you'll be able to understand how the Python pandas library stores data in a tabular format with row and columns labels.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2641337,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In order to make a visualization,  \\n we need data,  \\n and we usually need it in organized tabular form  \\n suitable for plotting.  \\n The Pandas library provides easy to use data structures  \\n and data analysis tools you can use  \\n to make your data easier to plot.  \\n Pandas data frames are multidimensional,  \\n labeled data structures similar to tables and spreadsheets  \\n with rows and columns.  \\n With it, you can store and manipulate data  \\n in a table format,  \\n providing a convenient way to analyze and visualize it.  \\n An important data structure of the Panda's library  \\n is a fast and efficient object for data manipulation  \\n called a data frame.  \\n You can manipulate and transform data frames easily  \\n with various Pandas methods.  \\n You can perform operations such as filter rows  \\n based on certain conditions, sort the data,  \\n pick specific columns, merge or join multiple data frames,  \\n and aggregate data.  \\n Another useful feature of Pandas  \\n is its ability to deal with missing data.  \\n With Pandas, you can detect missing values,  \\n fill them with appropriate values,  \\n or remove them all together.  \\n It's crucial to have this capability  \\n when dealing with incomplete or messy datasets.  \\n The image below is a Pandas data frame.  \\n A row represents an observation,  \\n and a column represents a feature.  \\n Rows have labels called indexes.  \\n So for example, this row is the row with index zero.  \\n This row is a row with index one.  \\n Columns also have column names.  \\n So we have the month column.  \\n We have the starting balance column,  \\n the repayment column, and so on.  \\n Once you create a Pandas data frame,  \\n you will more easily be able  \\n to manipulate and clean your data.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3373065\",\"duration\":230,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Create sample data\",\"fileName\":\"4499006_en_US_02_02_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":230,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Sometimes you have create your own data, so it is a good idea to know how to initialize your own pandas DataFrame. After watching this video, you'll be able to create sample data and put it into pandas DataFrames.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10506659,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In order to create visualizations,  \\n you first need to have data to work with.  \\n So in this video I'll teach you how  \\n to create sample data in the form of a car loan table.  \\n So what we have here is a car loan of $34,690  \\n with a 7.02% interest rate over 60 months.  \\n There are multiple approaches to structure your data  \\n and to a form acceptable by Pandas DataFrame.  \\n One approach is by using a nested list  \\n where in the first row,  \\n we have the first month of our car payment.  \\n So we have one for the month, we have our starting balance,  \\n we have a repayment, which is $687.23.  \\n We have how much of that repayment is going toward interest.  \\n We have how much of  \\n that payment is going toward the principal of the loan.  \\n And then after a payment we have  \\n how much is our new balance?  \\n We have the terminal loan,  \\n which is how many months we're going to be paying this loan  \\n for in total.  \\n We have the interest rate for the loan  \\n and we also have the car type.  \\n In this case it's a Toyota Sienna.  \\n And then we also have the column names, which are  \\n what I just told you before in the form of a Python list  \\n where we have the month, the starting balance,  \\n the repayment, et cetera.  \\n And then once we have the data in a nice clean format,  \\n we're going to press shift enter to run the cell.  \\n From here we're going to use the PD.DataFrame method  \\n and we're going to insert our carLoans variable  \\n into the data parameter and our colNames variable  \\n into the columns parameter.  \\n And then we're going to press shift enter  \\n to initialize our DataFrame.  \\n And from here we're going to press shift enter again.  \\n And we have our data in a nice clean format.  \\n And obviously this is more readable than a nested list,  \\n but there's also other ways to do this as well.  \\n We could have also structured our data  \\n in the form of a NumPy array.  \\n And this looks very similar to a nested list, except  \\n for a NumPy array is more efficient than a Python list.  \\n And similarly, we have the same colNames as before.  \\n So now that we have the data in a format that we like,  \\n we're going to press shift enter.  \\n And just as before, we're going to have our carLoans variable  \\n assigned to the data parameter  \\n and our colNames variable assigned  \\n to our columns parameter.  \\n We're then going to press shift plus enter  \\n to initialize our dataframe.  \\n And as before, we have our data in a nice clean format,  \\n which is obviously easier to look at than an NumPy array.  \\n The third approach is to use a Python dictionary.  \\n And while this may not seem like as clean for a format  \\n to structure your data, there are times when you're already  \\n going to have your data in a Python dictionary.  \\n It'll just be easier to use that Python dictionary  \\n to then initialize the panda's DataFrame.  \\n So we have our carLoans data, and like before,  \\n we're going to press shift plus enter  \\n to initialize our carLoans variable.  \\n And from here we're initialize our Pandas DataFrame  \\n by having our carLoans variable assigned  \\n to our data parameter.  \\n And our colNames variable assigned to our columns parameter.  \\n And then we're going to press shift plus enter  \\n to initialize our dataframe.  \\n And like before you can see that we have our payment table,  \\n which is clearly an easier  \\n to look at format than our Python dictionary.  \\n So as you can see,  \\n building tables like these aren't all that difficult.  \\n However, there are some limitations of these approaches  \\n that I'd like you to be aware of.  \\n For example, if you have a larger data set,  \\n like the entire payment table for a particular car loan,  \\n for example, like what I'm showing here,  \\n it would be painfully slow to type this out  \\n and get it correctly, for one,  \\n and two, it'd be very memory intensive.  \\n And now that I've painfully typed out this payment table,  \\n we're going to run this cell by doing shift plus enter.  \\n And as before, we're going to initialize our DataFrame  \\n and voila, we have our entire payment table.  \\n This is a very tedious approach and I do not recommend it.  \\n I should note that if you don't know what a command does,  \\n for example PD.dataframe,  \\n you can always use the inbuilt Python function help  \\n to find out what the parameters accept  \\n as valid, and that's it.  \\n In order to create data visualizations,  \\n you need to have data and now you know how to create it.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3364070\",\"duration\":137,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Load sample data\",\"fileName\":\"4499006_en_US_02_03_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":137,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"The ability to load datasets is an essential skill as before you can manipulate and visualize data, you need to load it. After watching this video, you'll be able to load data into a pandas DataFrame from .csv and Excel files.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5846326,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In my line of work as a Python instructor,  \\n one question I often get from students is,  \\n \\\"How do you load your own data?\\\"  \\n In this video, I'm going to show you how to load CSV  \\n and Excel files using the Pandas library.  \\n So the first thing you have to do  \\n is you have to find out where your files are located.  \\n I've provided some exercise files included with the course.  \\n So inside the folder Exercise Files,  \\n inside the Pandas folder, under Data, I have two files,  \\n car_financing.csv and car_financing.xlsx.  \\n In this cell, we have a path to the file car_financing.csv.  \\n And what this here is is a relative path.  \\n So relative to the notebook load_data.ipynb  \\n I have the Data folder  \\n and inside the Data folder is the CSV file  \\n that we want to load.  \\n From here, we're going to use the pd.read_csv method  \\n to load that file into a Pandas DateFrame.  \\n And I'm going to press Shift + Enter.  \\n And then from here, we can look at the data frame.  \\n You can also load files from Excel.  \\n Inside the Exercise Files folder,  \\n we have a folder called Data,  \\n and inside that folder is a file called car_financing.xlsx.  \\n And what we're going to do now  \\n is we're going to read the Excel file  \\n using the method pd_read_excel into a Pandas DateFrame.  \\n And like before, we've loaded the file  \\n into a Pandas DateFrame.  \\n I should note that if you need help  \\n or if you don't understand what a method does,  \\n you can always use the in-built Help command  \\n to understand the documentation for a given method.  \\n And then I'll press Shift + Enter  \\n and I can see what the read_excel method takes in  \\n as parameters because sometimes your file may not load  \\n as perfectly as you may wish.  \\n So that's how you load data into Pandas DateFrames.  \\n And one more note,  \\n there are many different file types you can load  \\n than just CSV and Excel files into Pandas DateFrames.  \\n If you're curious about what other kind of files  \\n you can load, I encourage you to do pd.read_  \\n to see what other kind of files you can load.  \\n And there you go.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3366071\",\"duration\":117,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Basic operations\",\"fileName\":\"4499006_en_US_02_04_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":117,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Once you've loaded data into your pandas DataFrame, it's essential to explore and confirm accurate loading, understand the data types, and validate the dataset. After watching this video, you'll be able to check that your files have loaded properly and, most importantly, see if you have missing data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4371551,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Narrator] After reading the contents  \\n of a file into your Pandas DataFrame,  \\n it's important to examine your data for a couple of reasons.  \\n First, you need to ensure  \\n that you've correctly loaded the data.  \\n Second, you have to see what kind of data you have.  \\n And third, you have to check the validity of your dataset,  \\n and I'll go through a couple ways we can do this.  \\n So one of the first things you do  \\n after loading your data is look at the head  \\n and the tail of your dataset.  \\n The method head selects the top N number  \\n of records from your dataset.  \\n The method tail selects the bottom N number  \\n of records from your dataset.  \\n This is really important to do  \\n as oftentimes your data format  \\n could change throughout your dataset.  \\n Another important thing to do  \\n is to check your column data types.  \\n You can do this by using the dtypes attribute.  \\n One thing we'll notice is that certain columns are ints,  \\n certain columns are floats, whereas others can be objects  \\n and you can think of objects as strings.  \\n Another important thing to do is to find out how many rows  \\n and columns you have in your dataset.  \\n To do this, you can use the shape attribute,  \\n and you see that we have 408 rows and 9 columns.  \\n A really important thing to do that is often forgotten  \\n is to use the info method.  \\n And the reason why this is very valuable is that you can see  \\n how many non-null values you have in your dataset  \\n as oftentimes, data analysis tasks  \\n and data visualizations will not work  \\n if you have null values in your dataset.  \\n As you can see in the dataset over here  \\n for the interest paid column, we have one null value,  \\n because we have 407 non-null values  \\n versus every other column has 408.  \\n What this tells me is that I'll either have  \\n to remove the row or fill in the missing data  \\n with some sort of amputation technique.  \\n In the end, it is really important to remember  \\n to verify your data.  \\n Use the techniques I just showed you  \\n to make sure that everything looks good.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3366072\",\"duration\":252,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Simplify with slicing\",\"fileName\":\"4499006_en_US_02_05_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":252,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Before you can visualize data, it often helps to simplify data. After watching this video, you'll be able to select columns in pandas using brackets and the loc attribute, as well as understand how to execute slicing operations.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9036411,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] When working with large data sets,  \\n oftentimes you're only interested  \\n in a smaller subset of your data,  \\n and that's why slicing is so important.  \\n And in this video,  \\n I'll teach you how to select columns in pandas,  \\n because oftentimes, you're only interested  \\n in a smaller subset of columns in your data,  \\n and I'll also show you  \\n how to use slicing operations in pandas.  \\n I'm working with a car loans dataset,  \\n where I have the DataFrame df,  \\n and I'm looking at the first five rows.  \\n Let's say you're only interested  \\n in looking at a few columns of your dataset.  \\n So let's go over how to use brackets  \\n to select just a few columns.  \\n What the code here does  \\n is we're using double square brackets  \\n to only output one column of our data set.  \\n And as you see, I've only pulled out the car_type column.  \\n Now, we can also select multiple columns  \\n using double brackets.  \\n And let me show you how that's done.  \\n So right here,  \\n you notice that we have a list within these brackets.  \\n So I'm looking at the car_type column,  \\n and I'm looking at the Principal Paid column.  \\n So I run this, and now I have the car_type column  \\n and the Principal Paid column.  \\n And notice that when I use the inbuilt type function,  \\n that this is still a pandas DataFrame.  \\n One thing a lot of beginners often have difficulties with  \\n when working with pandas  \\n is if they just have single brackets,  \\n they end up with something that looks like this.  \\n This is called a pandas series,  \\n and what this is is a one-dimensional array,  \\n which can be labeled.  \\n In this case, our labels are zero or one or two  \\n or three and a four.  \\n These are called indexes.  \\n And notice when I use the inbuilt type function,  \\n I have a pandas series.  \\n Keep in mind that when you use pandas series,  \\n you cannot select multiple columns.  \\n This will result in a key error as you can see here.  \\n And this is a really common error  \\n that a lot of beginners run into.  \\n And this usually results  \\n from people wanting to select multiple columns.  \\n And the simple solution to this  \\n is simply to use a pandas DataFrame.  \\n In other words, use double brackets.  \\n So I have my DataFrame, I'm selecting my car_type column,  \\n and I'm selecting my Principal Paid column.  \\n One reason why you might use a pandas series  \\n as opposed to a DataFrame is that with a pandas series,  \\n you can select rows using slicing,  \\n where you have the series,  \\n the start index of what you want to select,  \\n the end index of what you want to select.  \\n And keep in mind, the end index is not inclusive,  \\n and this behavior is very similar to Python lists.  \\n So I have a pandas series here,  \\n where I'm looking at the car_type column.  \\n And this is the entire car_type column.  \\n Say I'm only interested in,  \\n let's say index zero up until but not including index 10,  \\n in other words, from here to here.  \\n I can use a slicing operation.  \\n So over here, I have my car_type column,  \\n and this is a pandas series, and here's my slice.  \\n And I'm just selecting from index zero  \\n up until but not including index 10.  \\n So from zero to nine.  \\n Keep in mind you can also select columns using dot notation.  \\n However, this is not the recommended syntax.  \\n And as you'll see in this cell over here,  \\n this can result in an error,  \\n as there's a space in this column name.  \\n Keep in mind that this also fails  \\n if your column name is the same as the pandas DataFrame's  \\n attributes, or methods.  \\n So a safer syntax is just to use single brackets.  \\n And lastly, I wanted to show you the preferred syntax  \\n for selecting columns.  \\n And this is by using the loc attribute.  \\n And this allows you to select columns, index,  \\n as well as slice your data.  \\n So over here,  \\n I'm selecting all the rows of my pandas DataFrame.  \\n I'm specifically saying I just want the car_type column,  \\n and then I want the first five rows.  \\n Similarly, if you just want a pandas series,  \\n you just take out the square brackets  \\n around your column name.  \\n So that's it.  \\n If in the future you're presented with a big data set  \\n and you want to look at a subset of it, consider slicing.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3373066\",\"duration\":339,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Filter and clean data\",\"fileName\":\"4499006_en_US_02_06_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":339,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Before you can visualize data, it often helps to clean data. After watching this video, you'll be able to filter pandas DataFrames based on multiple columns of data using various Python comparison operators.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11844319,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] When working with a dataset,  \\n oftentimes you're only interested  \\n in a smaller subset of your data.  \\n For example, say I have a car loan dataset  \\n and I want to filter out the data to only have  \\n a car type of Toyota Sienna with an interest rate of 7.02%.  \\n So the first thing I'm going to do  \\n is I'm look at the first five rows in my dataset,  \\n and while it appears that the first five rows  \\n are only Toyota Siennas,  \\n that doesn't mean the rest of my dataset  \\n is all of car type Toyota Sienna.  \\n The first thing I'm going to do  \\n is I'll use the value counts method on the car type column  \\n to see what other kind of cars I have my dataset.  \\n I have my data frame, I have square brackets,  \\n I have the column I'm interested in.  \\n I'm going to close those brackets,  \\n and then I have the value counts method.  \\n When I press shift enter,  \\n you'll see that I have 120 Toyota Siennas.  \\n Say for example,  \\n I was interested in Toyota Corollas instead,  \\n I would have to fix the data entry errors  \\n because I have 111 Toyota Carollas, instead of Corollas.  \\n This is a really important thing to take note of,  \\n as oftentimes you'll have misspellings in your dataset,  \\n you'll have errors,  \\n you'll have things you don't quite understand,  \\n but that's all part of the data exploration process.  \\n And what I'll do now is I'll create a car filter,  \\n and the way this works is I have a data frame,  \\n I have square brackets,  \\n I have the column that I'm interested in.  \\n I close those single brackets.  \\n I have two equal signs because this is equality,  \\n and I have the type of car I'm interested in,  \\n in this case, Toyota Sienna.  \\n And what this produces is a pandas series  \\n of true and false values.  \\n There's a couple different ways  \\n to utilize this pandas filter of true and false values  \\n to get a data frame of just Toyota Siennas.  \\n One way is to have your data frame, square brackets,  \\n your car filter, which is your pandas series,  \\n you close those brackets,  \\n and then I'm just going to look at the first five rows.  \\n The second way is to use a lock attribute,  \\n and the way this works is I have a data frame,  \\n I have .loc, I have single brackets,  \\n I have my pandas series of true and false values,  \\n and this colon just means that I want  \\n to look at all the columns and I'll press shift plus enter,  \\n and these two approaches are equivalent  \\n in the result they produce,  \\n but oftentimes the second approach is more legible.  \\n As you can see, I have identical outputs  \\n for the two different approaches.  \\n One thing to keep in mind is that if I try to use  \\n the value counts method again on the pandas series,  \\n for the car type column, it'll seem like nothing changed.  \\n And the reason why it looks like nothing changed  \\n is because we didn't assign the filtered data frame  \\n back to the original data frame.  \\n And the way to fix this is by assigning  \\n the filtered data frame back to the original data frame.  \\n And now if you look at the value counts,  \\n it looks like we have a filtered data frame.  \\n Now that we've taken care of the car type filter,  \\n we also have to make an interest rate filter.  \\n And if I look at the pandas series  \\n for the interest rate column, and the value counts for it,  \\n you'll see that we have 60 rows with a 7.02% interest rate,  \\n and 60 rows with a 3.59% interest rate.  \\n And what I want to do in this section is filter the data frame  \\n to only have the 7.02% interest rate.  \\n The code here is a filter that produces a pandas series  \\n of true and false values, where the rows that are true  \\n are the ones with the 7.02% interest rate,  \\n and the false ones will be the rows  \\n with the 3.59% interest rate.  \\n So I have the data frame, I have single brackets,  \\n I have a string of the column that I'm interested in.  \\n I close those brackets.  \\n I have two equal signs, and then the 7.02% interest rate,  \\n and this produces a pandas series of true and false values.  \\n What I'm going to do next is I'm going to assign  \\n that pandas series of true and false values  \\n to the variable interest filter.  \\n To utilize my interest filter,  \\n I'm going to use the lock attribute,  \\n followed by single brackets.  \\n I'll have my pandas series of true and false values,  \\n and then I'm going to select all the columns,  \\n and I'm going to take this filtered data frame  \\n and assign it back to the original data frame.  \\n I'm going to do shift plus enter  \\n to create the filtered data frame.  \\n To check that my interest filter worked as intended,  \\n I'm going to look at the pandas series  \\n of the interest rate column.  \\n I'll use the value counts method,  \\n and I'm going to press shift plus enter,  \\n and you'll see that I have 60 rows  \\n with the 7.02% interest rate.  \\n In the previous sections, we created a car filter  \\n and an interest filter and used a lock command  \\n to filter the data by first applying the car filter  \\n and then the interest filter.  \\n A more concise way to do this is shown below.  \\n I have my data frame, I have the lock attribute,  \\n I have single brackets, I have my car filter,  \\n and then I use the and bitwise logic operator,  \\n along with my interest filter,  \\n and then I say, I want all the columns.  \\n And this would've worked just as well  \\n as applying each of the filters individually.  \\n As you can see, by using filtering, you can get the data  \\n that you're just interested in looking at.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3366073\",\"duration\":196,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Rename and delete columns\",\"fileName\":\"4499006_en_US_02_07_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":196,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"You may often need to change your column names to make them more descriptive or remove unnecessary columns. After watching this video, you'll be able to rename pandas DataFrame columns using dictionary substitution and list replacement to more suitable name. You'll also learn how to delete columns when needed.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6974200,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] When you have a data set,  \\n it's often the case  \\n where you want to change your column names  \\n to make them more legible, more understandable,  \\n or more easy to program with,  \\n or sometimes you want to remove unnecessary columns.  \\n An advantage of removing unnecessary columns  \\n is it can free up RAM on your computer  \\n and it's also a good data science practice.  \\n There are two popular ways to rename data frame columns.  \\n The first is dictionary substitution,  \\n which is very useful  \\n if you only want to rename a few of your columns.  \\n There's also a list replacement,  \\n which requires a full list of names.  \\n And in my experience, this is more error prone.  \\n The data set we're going to work with is the car loan data set,  \\n and we'll look at the first five rows using the head method.  \\n And one reason why I want to rename a column,  \\n in this case the principle paid column,  \\n is if I try to use the dot notation  \\n where the data frame and a dot  \\n and the column I'm interested in, this will yield an error.  \\n The reason why I have an error  \\n is that there's a space in my column name.  \\n This would've worked if I had single brackets  \\n and a string of the column name I'm interested in  \\n and closed brackets.  \\n However, I still want to fix this  \\n as someone else could run into the same error  \\n with this dataset.  \\n One approach is dictionary substitution  \\n using the rename method.  \\n And the way this works is that I have my data frame,  \\n I have a dot, I have rename,  \\n and I have columns here  \\n because I want to rename my columns as opposed to an index.  \\n I have the original column name,  \\n in this case starting balance,  \\n and I want to rename it to starting_balance.  \\n Additionally, I want to rename the interest paid column,  \\n the principle paid column, and the new balance column.  \\n And after I rename the columns,  \\n I want to take that data frame  \\n and assign it back to the original data frame.  \\n And now you can look at the data frame  \\n after renaming columns.  \\n And as you see, there's no longer a space  \\n between many of the columns.  \\n There's another approach to renaming columns,  \\n and this is through list replacement.  \\n In the case of the code below,  \\n I want to change the month column  \\n where it starts with a capital M to our lowercase m.  \\n However, with this technique,  \\n I still need to list the rest of the columns  \\n and assign it back to the df.columns attribute.  \\n And now as you can see,  \\n I have a lowercase m for the month column.  \\n There are also times  \\n where you may want to delete your columns.  \\n Approach one for doing this is to use the drop method  \\n where I have the data frame, I have .drop  \\n and I'm specifying that I want to drop my columns.  \\n In this case, I only want to drop the term column.  \\n But keep in mind that using this approach,  \\n you can drop multiple columns at a time.  \\n And now looking at the data frame,  \\n you can see that I no longer have the term column.  \\n The second approach to deleting columns  \\n is by using the Dell command.  \\n In this case, I'm going to delete the repayment column.  \\n As you see below, I no longer have the repayment column.  \\n And that's it.  \\n that you want to rename your columns or delete them,  \\n consider some of these approaches.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3373067\",\"duration\":159,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Aggregate functions\",\"fileName\":\"4499006_en_US_02_08_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":159,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Computing summary statistics is a good way of understanding the distribution of the columns of your data. After watching this video, you'll be able to compute summary statistics using aggregate methods like sum, cumsum, mean, median, min, max, std, and quantile.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6389173,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] When working with a dataset,  \\n it's often a good idea to compute summary statistics.  \\n Summary statistics can tell you about your outliers,  \\n if your data is symmetrical,  \\n and how tightly grouped your data is.  \\n For the car loan dataset  \\n where we have a payment table  \\n for a $34,690 loan at a 7.02% interest rate  \\n for a Toyota Sienna over 60 months,  \\n it would be interesting to find out  \\n how much total interest paid  \\n would be over the course of the loan.  \\n For this, we're going to use a sum method.  \\n And what the sum method does  \\n is it sums the values in a column.  \\n And the way this works is I have  \\n the name of the data frame, DF.  \\n I have single brackets,  \\n I have the column I'm interested in.  \\n In this case, interest_paid, closing single brackets,  \\n and I do .sum.  \\n And what this gives me is a total amount of interest paid  \\n over the course of the loan.  \\n And as you see, over the course of the loan,  \\n the interest paid is $6,450 and 27 cents.  \\n You can also use a sum method on an entire data frame  \\n to sum across all the values across all the columns.  \\n While the sum for the interest paid column  \\n is what we had before  \\n when we just summed across the interest paid column,  \\n the car type column seems to be a bit different.  \\n And the reason why this is the case  \\n is if you recall with Python,  \\n when you add two strings together, they can concatenate.  \\n It's important to note that if you ever don't know  \\n exactly what a method does,  \\n I encourage you to use the help method  \\n where you have help, open parenthesis,  \\n the name of the data frame, in this case DF,  \\n open brackets, the name of the column,  \\n .sum, shift plus enter.  \\n The way the sum method works  \\n is it sums down all the values in a column.  \\n However, by default, it ignores all NAs  \\n or null values when computing the result.  \\n So if this interest paid column had any NAs or NANS in them,  \\n it ignored them.  \\n And that's why it's always important to know  \\n exactly what a method does or an aggregate function  \\n in this case, as are some potentially did include any NANs.  \\n And the way to check if you have NANs in your data frame  \\n is by using the inbuilt info method.  \\n And as you see here, I have one NAN  \\n in the interest paid column.  \\n When working with a dataset, it's often a good idea  \\n to use aggregate functions on your various columns.  \\n This can tell you how well grouped your dataset is,  \\n what your outliers are,  \\n and potentially useful information  \\n like how much interest you'll pay over the course of a loan  \\n by using the sum method.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3372050\",\"duration\":221,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Identify missing data\",\"fileName\":\"4499006_en_US_02_09_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":221,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Before you graph data, you should determine if you have missing values.  After watching this video, you'll be able to identify where missing values are in your pandas DataFrames.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8151356,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] When working with a dataset,  \\n you'll often run into missing values.  \\n Values can be originally missing from a dataset  \\n or be a product of data manipulation.  \\n In pandas, missing values are typically called NaN or none.  \\n Missing data can hint at data collection errors,  \\n indicate improper conversion or manipulation of data,  \\n or they can actually not be considered missing at all.  \\n For some datasets, missing data can be listed as zero,  \\n false, not applicable, entered an empty string,  \\n among many other possibilities.  \\n This is a really important subject  \\n as before you can graph data,  \\n you need to make sure you aren't trying to graph  \\n some missing values,  \\n as that can cause an error or misinterpretation of the data.  \\n And in this notebook,  \\n we'll see where missing data  \\n can cause misinterpretation of our results.  \\n The first thing we're going to do  \\n is we're going to use the car loan data set  \\n and we're going to identify how many missing values we have  \\n by using the Pandas info method.  \\n So I have data frame here,  \\n and then I have the info method.  \\n I'll press shift enter,  \\n and then as you see here, for every other column,  \\n except for interest paid,  \\n I have 60 non-null values.  \\n But for the interest paid column, I have 59.  \\n This means I have one NaN value in that column,  \\n this will cause a problem.  \\n Two common methods to indicate where values are missing  \\n are isna and isnull.  \\n They're exactly the same methods,  \\n but with different names.  \\n The reason why this is the case is in the R language,  \\n NA and null are two different things.  \\n This is to make our programmers have an easier time  \\n when working with Python.  \\n I tend to prefer isna as this tends to be similar  \\n in naming to other Python methods.  \\n As you see it in the code over here  \\n I have the Panda Series interest_paid.  \\n I'm using the isna method,  \\n and what this does is  \\n this is producing a Panda Series of true and false values.  \\n It'll be true where I have a NaN value,  \\n and it'll be false where I don't.  \\n I'll then press shift and enter.  \\n And as you see in the first five rows,  \\n I don't have any NaN values.  \\n The next thing I'm doing  \\n is I'm assigning this true and false filter  \\n to the variable interest_missing.  \\n And the reason why I'm doing this  \\n is I want to take that filter  \\n and eventually use it to isolate my missing data.  \\n What the code here is doing  \\n is I want to look at the row where I have the missing values.  \\n And what you see here  \\n is I have a missing value in the interest paid column.  \\n This will be a problem for later.  \\n It's important to keep in mind,  \\n that you can also use the knot operator to negate the filter  \\n so that every row that's returned doesn't have a NaN.  \\n And as you see in the Pandas DataFrame,  \\n the row with index 35 is no longer here.  \\n It's important to note, you'll often see code  \\n similar to what you see here.  \\n What we have is that same Pandas filter  \\n of true and false values.  \\n And then after, you have the aggregate function sum,  \\n which then sums all the true and false values  \\n to produce a result.  \\n The reason why this works is in Python,  \\n Boolean are a subtype of integer where true are ones  \\n and falses are zero.  \\n So in the code here,  \\n I essentially have one plus zero plus zero.  \\n And the reason why I have a one here  \\n is that I have one NaN in the interest paid column.  \\n When working with a dataset,  \\n it's important to identify your missing values,  \\n as missing values can cause data misinterpretation errors,  \\n or even cause you an error when you try to graph your data.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3366074\",\"duration\":303,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Remove or fill in missing data\",\"fileName\":\"4499006_en_US_02_10_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":303,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Before you graph data, you need to handle missing values as they  can cause an error or misinterpretation of the data. After watching this video, you'll be able to remove missing values from pandas DataFrames, as well as fill in missing values.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10998897,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Once you've identified missing data,  \\n it's really important to either remove that data  \\n or fill in the missing data with a reasonable value.  \\n This is a really important subject,  \\n as before you can graph data,  \\n you need to make sure you aren't trying to graph  \\n some missing values, as that can cause an error  \\n or cause a misinterpretation of the data.  \\n We're working with the car loan dataset  \\n and the first thing we're going to do  \\n is we're going to utilize the info method.  \\n And what the info method does  \\n is it shows us how many missing values we have  \\n in each of our columns.  \\n And as you see, we have 60 non-null values  \\n for every column except for the interest paid column.  \\n This means that we have one null value.  \\n There are a couple different ways to deal with missing data.  \\n The first way is simply to remove the missing values.  \\n And in pandas you can remove the missing values  \\n by using the drop NA method.  \\n And what the code here does is I have a pandas data frame  \\n from index 30 up until, but not including index 40,  \\n and I'm dropping the rows where I have any NAN values.  \\n And as you see here, I don't have a row at index 35  \\n because I had a NAN value here.  \\n The other way to deal with missing values  \\n is simply to fill them in and there are a variety  \\n of ways to fill in missing values.  \\n The first thing we're going to do  \\n is we're going to look at where the missing data is located  \\n by using a pandas series  \\n and then slicing it to look at indexes 30 up until,  \\n but not including index 40.  \\n As you see here, I have a NAN at index 35.  \\n The first thing we're going to try  \\n is we're going to try to fill the NAN with a zero  \\n by using the fill NA method.  \\n The reason why filling in a NAN with a zero  \\n is often not a good idea,  \\n is originally the NAN could have been something else.  \\n A zero could help you misinterpret the data.  \\n It's just one option.  \\n The other method we could use is to fill in with a backfill.  \\n And the way this works is perhaps better to show you.  \\n Where at Index 35, before I had a zero or a NAN,  \\n now I have an 89.77.  \\n This is because the index after it was an 89.77.  \\n This is very commonly done with time series data  \\n when you have a missing value.  \\n Another way is to forward fill in your value.  \\n And this is also done with time series data.  \\n The difference between backfill and forward fill  \\n is backfill takes the value after the missing value  \\n and inserts it at the value that's missing.  \\n And what forward fill does is it takes  \\n the value before the missing value  \\n and inserts it where the missing value is.  \\n The reason why you use one versus the other  \\n is really dependent on your domain knowledge  \\n and your application.  \\n This is also a current area of research.  \\n It's called data imputation.  \\n Another way to fill in missing values  \\n is through linear interpolation.  \\n And what this does is it uses a linear model  \\n to fill in the missing value.  \\n And as you see here, this 93  \\n is between the 96 and the 89.  \\n What the code here is doing is I'm finding  \\n the total interest paid over the course of a loan  \\n by using the sum method.  \\n And I should note, the sum method doesn't account for NANs.  \\n And as you see here, this is the total amount of money  \\n paid toward interest over the course of a loan.  \\n It's important to keep in mind that the sum method  \\n by default ignores NANs.  \\n So after we fill in the NAN value with a real value,  \\n this might change.  \\n What the code here is doing is this is producing  \\n a Boolean array of true and false values  \\n where I'll have a true value where I have a NAN  \\n and a false value where I don't,  \\n and I'm assigning it to the variable interest_missing.  \\n From there, I'm utilizing the LOC operator  \\n and filling in that missing N value with the value 93.24.  \\n Now, when I sum over the entire column,  \\n I'll get a different number.  \\n This is perhaps more accurate,  \\n and I should note the value  \\n of removing or filling in your data  \\n is that oftentimes you get more accurate calculations.  \\n In this case, the reason why I filled in the value  \\n with 93.24 is because I knew what the actual value  \\n should have been.  \\n This is due to my domain knowledge of loans.  \\n For whatever application you're working with,  \\n it's often best to try to get the most accurate value  \\n to fill in for your missing values.  \\n And as you can see here,  \\n we don't have NAN values in the data frame anymore.  \\n Once you've identified your missing values,  \\n removing them or filling them in often gives you  \\n more accurate calculations  \\n and makes the results more interpretable.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3372051\",\"duration\":75,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Convert pandas DataFrames\",\"fileName\":\"4499006_en_US_02_11_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":75,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"While pandas DataFrames are highly useful, there are times when DataFrames are not ideal. For example, DataFrames are not an acceptable input for certain methods. After watching this video, you'll be able to convert pandas DataFrames to NumPy arrays and Python dictionaries.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3199851,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] When working with Pandas DataFrames,  \\n you'll oftentimes find you want to convert them  \\n to NumPy arrays or Python dictionaries.  \\n The reason why is because certain libraries  \\n prefer NumPy rays or Python dictionaries  \\n as inputs to their methods as opposed to Pandas DataFrames.  \\n In this video, we'll work with the car loan dataset again,  \\n and we're going to look at the first five rows.  \\n There are two ways to convert Pandas DataFrames  \\n to NumPy arrays.  \\n The first approach is to use  \\n the two underscore NumPy method,  \\n and what this outputs is a NumPy array.  \\n The second approach is to use the values attribute,  \\n and this also produces a NumPy array.  \\n I should note that either of these approaches works  \\n just as well as the other.  \\n You can also convert Pandas DataFrames  \\n to Python dictionaries.  \\n You can do this by using the two underscore dict method,  \\n and the reason why you'd want to do this  \\n versus convert your Pandas DataFrame to a NumPy array  \\n is oftentimes you're interested in preserving the indices  \\n of your Pandas DataFrame.  \\n In this video, we've gone over how to convert  \\n Pandas DataFrames to NumPy arrays in Python dictionaries.  \\n The practicality of this is that sometimes certain libraries  \\n don't accept Pandas DataFrames as inputs to their methods.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3370067\",\"duration\":88,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Export pandas DataFrames\",\"fileName\":\"4499006_en_US_02_12_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":88,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"After manipulating datasets, it is often a good idea to export your datasets to files to share with others. After watching this video, you'll be able to convert your pandas DataFrames to .csv and Excel files.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3210542,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] When working with Panda's DataFrames  \\n it's often a good idea to export them  \\n to either CSV or Excel files.  \\n This is because it's a great way to share  \\n your results with others.  \\n In Pandas, you can export your DataFrame to a CSV file  \\n by using the to_CSV method.  \\n In this case, I have a relative path  \\n where inside my current working directory,  \\n there's a folder called data,  \\n and inside that folder is where I'm exporting the CSV file.  \\n The parameter index equals false,  \\n just means I don't want to export  \\n the Pandas DataFrames indexes.  \\n You can export your DataFrame to an Excel file  \\n by using the to_Excel method.  \\n And in this case, I have a relative path  \\n where inside my current directory,  \\n there's a folder called data,  \\n and then inside that folder  \\n is where I'm writing the Excel SX file.  \\n The index equals false parameter,  \\n just means I don't want to have the indexes from my DataFrame  \\n export to the file.  \\n And keep in mind,  \\n if you ever don't know what a method does,  \\n you can always look them up using the help command.  \\n In this case, I'm looking up the to_CSV method.  \\n In this video, we've gone over  \\n how to export your Pandas DataFrames to CSV and Excel files.  \\n Keep in mind, it's also a good idea  \\n to check your exported files,  \\n in the sense that sometimes,  \\n when you export a file,  \\n it may not always be what you expect.  \\n \\n\\n\"}],\"name\":\"2. Leverage pandas for Analysis\",\"size\":83170622,\"urn\":\"urn:li:learningContentChapter:3366078\"},{\"duration\":1408,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3366075\",\"duration\":229,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Basics of Matplotlib\",\"fileName\":\"4499006_en_US_03_01_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":229,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"The Matplotlib library is a powerful tool capable of producing complex publication-quality figures with fine layout control in two and three dimensions. After watching this video, you'll be able to import Matplotlib, create basic plots using the plot method, and change the default figure style so you can choose an appropriate aesthetic style.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9150662,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this video,  \\n we're going to learn about the Matplotlib library,  \\n which is a powerful tool capable of producing  \\n complex publication-quality figures  \\n with fine layout control in two or three dimensions.  \\n While this is an older library,  \\n so many libraries are built on top of it and use its syntax.  \\n The first thing we have to do  \\n is import the libraries we're going to use.  \\n We're going to utilize the pandas and NumPy libraries  \\n to manipulate data in a format that's suitable for plotting.  \\n We're going to import matplotlib.pyplot as PLT,  \\n and PLT is just an alias for the matplotlib.pyplot module.  \\n And lastly, we're going to import the seaborn library,  \\n which is a wrapper for Matplotlib as SNS.  \\n Before we can graph data,  \\n we have to have data in a form that's suitable for graphing.  \\n So we're first going to import the car loan data  \\n into a pandas data frame.  \\n We're next going to look at the first five rows  \\n of the data frame as this is a good practice.  \\n Next we're going to check to make sure  \\n we don't have NANs in our data frame,  \\n as it is not easy to directly plot data that contains NANs.  \\n And to do this, we're going to use the info method.  \\n And as you can see, we have 60 entries in our data frame,  \\n and we have 60 non-null values for each column.  \\n In this video, we're going to graph month number on the X axis  \\n and interest paid and principle paid on the Y axis.  \\n But to first do that, we have to have our data  \\n in the form of NumPy array.  \\n And what this code over here does  \\n is I have my data frame, I'm using the LOC operator,  \\n I'm saying I want all the rows,  \\n and I just want the month column.  \\n So this is a panda series, and I'm turning this panda series  \\n into a NumPy array by using the values attribute  \\n and assigning this entire thing  \\n to the month number variable.  \\n And similarly, I'm doing the same  \\n for the interest paid column and the principle paid column.  \\n Keep in mind, you can also check that the values attribute  \\n converts a column of values into a NumPy array  \\n by using the inbuilt type function.  \\n Once you have data in an appropriate format,  \\n you can begin to plot it.  \\n In this case, we're going to plot month number on the X axis  \\n and principle paid on the Y axis.  \\n As a reminder, if you don't know what a method does,  \\n you can always use the inbuilt help function.  \\n Now it is time for our first plot  \\n and we're going to do PLT.plot.  \\n And for our X axis we're going to have our month number.  \\n And for our Y axis, we're going to have our interest paid.  \\n And as you can see, it's not the prettiest plot.  \\n You can also plot another line on the same graph.  \\n So before we only graphed month number and interest paid,  \\n but we can also graph month number and principle paid.  \\n This is also not the prettiest plot.  \\n One way to make our plots more aesthetically pleasing  \\n is to choose a figure style.  \\n We'll use PLT.style.available  \\n to select an appropriate aesthetic style for a figure.  \\n The default style is not the most aesthetically pleasing.  \\n On this line, I'm doing PLT.style.available  \\n and pressing shift enter  \\n to see the different styles I can choose from.  \\n The style of classic is very similar  \\n to what we've already plotted.  \\n However, the 538 style is more aesthetically pleasing.  \\n If you're coming from R,  \\n you can also use the GG plot style as you can see here.  \\n You can also use different styles like Tableau colorblind.  \\n You can also use the seaboard style,  \\n which is a wrapper for Matplotlib, as you can see here.  \\n In this video, we've gone over how to use  \\n Matplotlib.plyplot to make line graphs.  \\n And we saw how when you change style,  \\n you can make your graphs more aesthetically pleasing.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3369056\",\"duration\":112,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Set marker type and colors\",\"fileName\":\"4499006_en_US_03_02_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":112,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Changing marker type and color is a fundamental data visualization skill as they can effect the interpretability of your plots. After watching this video, you'll be able to change your Matplotlib plot's marker type and colors.\",\"captionsStatus\":\"NOT_AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3951663,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":null},{\"urn\":\"urn:li:learningContentVideo:3374062\",\"duration\":127,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"MATLAB-style vs. object syntax\",\"fileName\":\"4499006_en_US_03_03_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":127,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"There are two different styles of Matplotlib syntax. After watching this video, you'll be able to utilize Matplotlib MATLAB style and object-oriented style of syntax, and you will also understand how to combine the styles of syntax.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4021429,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] A potentially confusing part  \\n of the Matplotlib library is  \\n that it has two different styles of syntax.  \\n There's the MATLAB style,  \\n and this is a scripted interface designed  \\n to feel like MATLAB, where Matplotlib maintains a pointer  \\n to the current figure and sends commands to it.  \\n There's the object oriented syntax,  \\n and this is more often used in situations  \\n where you want more control over your figure.  \\n An important note is that you can  \\n and often will have plots that will be created  \\n through a combination of the MATLAB style  \\n and the object-oriented style.  \\n We'll start by looking at the MATLAB style syntax,  \\n and this typically starts by using the PLT plot command,  \\n where you have something in the x axis,  \\n you have something in the Y axis,  \\n and you have a bunch of parameters that you set.  \\n In this case, I'm setting the parameter C, which is color  \\n equal to K, which is the color black,  \\n and you can also have additional PLT dot plot commands  \\n to plot on top of the figure.  \\n And you can see the result here.  \\n The object oriented syntax is as follows.  \\n You have PLT subplots.  \\n In this case, I just want one plot,  \\n so I'm making n rows equal to one  \\n and n columns equal to one, and this returns a topple.  \\n And I'm topple unpacking the figure and the axes.  \\n And then from there, I'm just doing axes plot  \\n what I want in the X axis, what I want in the Y axis,  \\n and additional other parameters.  \\n Similar to the MATLAB style syntax,  \\n you can also plot additional things on the same plot.  \\n It's important to note that you can  \\n and often will see the MATLAB style  \\n and the object oriented style used together.  \\n So in this case, I have the object oriented style,  \\n I have the MATLAB style,  \\n and I have the object-oriented style,  \\n and it still produces the same plot.  \\n This video showed that there's two separate styles  \\n of Matplotlib syntax.  \\n There's a MATLAB style  \\n and there's the object-oriented style,  \\n and that sometimes they're used in combination  \\n with each other.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3364071\",\"duration\":261,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Set titles, labels, and limits\",\"fileName\":\"4499006_en_US_03_04_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":261,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Adding text to explain your graph can greatly enhance the interpretability of your plots. After watching this video, you'll be able to set plot titles, labels, and limits for the Matplotlib MATLAB style and object-oriented style of syntax.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9913490,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this video,  \\n I'll show you how to create plot titles, labels, and limits.  \\n Plot titles and plot labels are very important  \\n to convey what you're graphing  \\n and plot limits are very important as, oftentimes,  \\n the default limits aren't always ideal with Matplotlib.  \\n The first thing we're going to do  \\n is we're going to go for the MATLAB style  \\n for how to set plot titles, labels, and limits.  \\n So here's a normal MATLAB-style plot.  \\n Notice in the graph that there's no title in the plot.  \\n There's no y label, there's no x label,  \\n and the limits could be a little bit better on the x-axis  \\n where it looks like the plot should start at one  \\n and should end at 60.  \\n To change the x limit and the y limit, you can do .xlim  \\n where you have your limit on the left  \\n and your limit on the right.  \\n To change your y limit, you can do .ylim,  \\n you can have your lower limit  \\n and you can have your upper limit,  \\n and you can do Shift + Enter.  \\n And obviously, this is not the most practical use  \\n of changing the y limit,  \\n as before, it looked a little bit better.  \\n You can also set your x label and y label.  \\n To set your x label, you can do .xlabel  \\n and the name you want for your x label.  \\n For your y label, you can do .ylabel  \\n and the name you want for your y label.  \\n And as you see here, we have something on our y-axis now  \\n and something on our x-axis.  \\n To set the title, you can do plt.title  \\n and the name of your title.  \\n And as you can see, we have our title here now  \\n in addition to our y label and our x label.  \\n If you find that your x label, y label,  \\n or your title have too small of a font,  \\n you can always make it bigger by adjusting the font size.  \\n In this case, I clearly made it too big,  \\n so feel free to play around with the parameters,  \\n and you can continue to iterate on this almost indefinitely,  \\n and that looks a lot better.  \\n You can also change the size of your x ticks using .xticks  \\n and your y ticks using .yticks.  \\n So notice now that the title is too big,  \\n the x and y label is too big,  \\n as well as the x ticks and y ticks, so feel free to iterate.  \\n Graphing is a very iterative process.  \\n The object-oriented approach for changing the titles,  \\n the labels, and limits is very similar.  \\n Where I first have a graph, setting the x limits  \\n and the y limits is very similar to before.  \\n Where before I did plt.xlim,  \\n now I have axes.set_xlim,  \\n and I have axes.set_ylim for the y limits.  \\n Setting the x label and the y label  \\n is also a similar process to the MATLAB style.  \\n Where before I had plt.xlabel and plt.ylabel,  \\n now I have axes.set_xlabel and axes.set_ylabel.  \\n Similarly, before with the MATLAB style, I had plt.title  \\n and now I have axes.set_title.  \\n And as before, you can change the font size  \\n by adding the font size parameter.  \\n One difference with the object-oriented style  \\n is that you have to do axes.tick_params  \\n to change the tick font size.  \\n In this case, I'm changing the tick font size  \\n for the x-axis,  \\n making them font size of 20,  \\n in this case, it's called label size.  \\n And for the y ticks, I simply change axes  \\n to make it equal to the string y  \\n and make the label size equal to 20.  \\n In this video, I went over how to change the titles, labels,  \\n and limits for both Matplotlib styles of syntax.  \\n The benefit of this is that having labels, limits,  \\n and titles can make your graphs more interpretable  \\n and easier to understand.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3364072\",\"duration\":146,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Add grids\",\"fileName\":\"4499006_en_US_03_05_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":146,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Adding a grid to a plot can add to the aesthetic appeal to your figure. After watching this video, you'll be able to create and tune plot grids for the Matplotlib MATLAB style and object-oriented style of syntax.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5123779,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this video,  \\n we'll go over how to make grid lines in Matplotlib.  \\n Grid lines can't be visually appealing,  \\n but they could also help you determine fine differences  \\n between two quantities in a graph.  \\n The first thing we're going to do is we're going to create a plot  \\n without grid lines where we have month number in the x-axis,  \\n interest paid in the y-axis,  \\n and then we have a secondary y-axis  \\n where we have month number in the x-axis  \\n and principal paid in the y-axis, and we're going to plot it.  \\n This is the graph without grid lines.  \\n So now let's see how it looks with grid lines.  \\n In the MATLAB-style syntax in Matplotlib,  \\n you can add grid lines by using .grid.  \\n And as you can see, we have grid lines here.  \\n Keep in mind, you can also only have horizontal grid lines  \\n by specifying axis equals y  \\n and you can also only have vertical grid lines  \\n by specifying axis equals x.  \\n If you want more customization,  \\n you can also change the color of your grid lines.  \\n You can change the transparency of your grid lines.  \\n And keep in mind for alpha, lower means more transparent  \\n and up to one means not transparent at all.  \\n You can also change the line style of your grid lines,  \\n and this is not going to look too good  \\n because the green seems to really clash with the blue here.  \\n You can also add grid lines  \\n using the Matplotlib object-oriented syntax  \\n where you just do axes.grid.  \\n You can also specify that you only want  \\n horizontal grid lines by doing axis equals y.  \\n You can also specify that you just want vertical grid lines  \\n by setting axis equals to x.  \\n And similar to the MATLAB-style syntax, you can also specify  \\n what color you want your grid lines to be,  \\n how transparent you want them, and the line style.  \\n Keep in mind if you're finding setting grids to be tedious,  \\n use a style that already has grids in them by default.  \\n For example, the seaboard style  \\n has white grid lines by default  \\n as you can see here.  \\n Matplotlib allows you to set grid lines  \\n to make your graph more visually appealing  \\n and to help distinguish quantities across the graph.  \\n It's also important to keep in mind  \\n that just because you can set grid lines  \\n doesn't mean you have to.  \\n You can also use a style of Matplotlib  \\n that has them in by default.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3369057\",\"duration\":85,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Create legends\",\"fileName\":\"4499006_en_US_03_06_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":85,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Adding a legend can greatly enhance the interpretability of your plots. After watching this video, you'll be able to create legends in Matplotlib and put them in an optimal location in the plot.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3528166,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this video,  \\n we're going to learn about plot legends.  \\n Plot legends assist in assigning meaning  \\n to your various plot elements.  \\n It's therefore important to make sure  \\n your legend doesn't cover up your plot elements.  \\n We'll first start by utilizing Matplotlib's  \\n MATLAB-style syntax to create a plot legend.  \\n As you see in this image,  \\n the legend is not in an ideal location.  \\n You can use the lock parameter  \\n to change where your legend is located.  \\n In this case, I'm telling it to go to the center right.  \\n It's important to note you can also move your legend  \\n outside the plotting area.  \\n What the code here is doing  \\n is I'm moving the legend slightly outside  \\n to the right of my plot,  \\n because all the way to the right would be 1.00,  \\n and I'm moving it all the way at the base of my plot.  \\n And as you see here, my legend is outside the plotting area.  \\n Keep in mind, you can also use Matplotlib's  \\n object-oriented syntax to create a legend.  \\n In the case of the code over here,  \\n I'm doing axes.legend.  \\n Similar to Matplotlib's MATLAB-style syntax,  \\n the object-oriented syntax also allows you  \\n to move the legend's location.  \\n As you see here, I have the legend in the center right,  \\n and over here, just like before, I have it outside the plot.  \\n In this video, we learned some tools  \\n to not only create a legend in Matplotlib,  \\n but also to change its location.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3366076\",\"duration\":148,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Save plots to files\",\"fileName\":\"4499006_en_US_03_07_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":148,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"It is important to be able to share your visualizations to others. After watching this video, you'll be able to save images of your plots outside of Jupyter Notebooks.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5996589,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this video, we're going to go over  \\n how to save your plots to files.  \\n Saving your visualizations  \\n outside your Jupyter Notebook is important  \\n as it allows you to show your visualizations to others.  \\n Equally important is checking your saved visualization  \\n because there's always a possibility  \\n the graph doesn't look the same in the notebook  \\n as in the image file.  \\n So the first thing we're going to do is for this notebook,  \\n we're going to use a seaborn style in Matplotlib.  \\n This will make our graph more aesthetically pleasing.  \\n Using the MATLAB style of Matplotlib,  \\n we're going to save our file using plt.savefig.  \\n This is where we're saving our image.  \\n This is our DPI,  \\n you can think of it as resolution,  \\n and it's really important  \\n now that we have this outputted image  \\n in our Jupyter Notebook,  \\n let's now see how the image saved.  \\n Let's start by looking inside the Images folder  \\n for the filename MS Legend Cutoff.  \\n You'll see that the legend is cut off here.  \\n This is obviously a problem.  \\n You'll also notice that it looks like month  \\n is almost cut off here.  \\n Let's now learn how to fix this.  \\n So we're going to head back over to our Jupyter Notebook.  \\n The way we're going to solve this problem  \\n is we're going to utilize the plt.layout method.  \\n What this does  \\n is it automatically adjusts subplot parameters  \\n so that the subplot fits into the figure area.  \\n And we run the same code before with this new addition.  \\n And here we have the figure.  \\n Now looking inside the Images folder,  \\n we now see that the image is not cut off.  \\n Now there's another way to do this  \\n and that's by using the object-oriented Matplotlib syntax.  \\n And the only thing that differs  \\n is instead of doing plt.savefig,  \\n we now have fig.savefig.  \\n And in this image, it'll be cut off just like we had before.  \\n So inside the Images folder,  \\n we can look for Object Legend Cutoff.  \\n And as you see here, the legend's still cut off  \\n and the month is still cut off.  \\n So let's fix that.  \\n The way to fix this is by doing fig.tight_layout.  \\n Let's go ahead and run that.  \\n And now you'll notice in our Images folder,  \\n the objectlegend.png file  \\n doesn't have anything cut off.  \\n And here's the image. It looks like nothing's cut off.  \\n And that's it, the next time you want to export your images,  \\n consider using some of these techniques.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3369058\",\"duration\":300,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Create plots with Matplotlib wrappers\",\"fileName\":\"4499006_en_US_03_08_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":300,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Matplotlib has two prominent wrappers: seaborn and pandas. After watching this video, you'll be able to create plots using Matplotlib, pandas, and seaborn. Specifically, you will create boxplots using Matplotlib, pandas, and seaborn, you will be able to ascertain the use cases of when each library should be used.\",\"captionsStatus\":\"NOT_AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10901603,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":null}],\"name\":\"3. Simplify Visualization with Matplotlib\",\"size\":52587381,\"urn\":\"urn:li:learningContentChapter:3372053\"},{\"duration\":732,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3373068\",\"duration\":240,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Create heatmaps\",\"fileName\":\"4499006_en_US_04_01_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":240,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Making a plot in pure Matplotlib can be a lot more code than using a Matplotlib wrapper like seaborn or pandas. After watching this video, you'll be able to create Heatmaps using seaborn and Matplotlib.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8437118,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this video, we'll learn how to create  \\n heatmaps using matplotlib and Seaborn,  \\n a matplotlib wrapper.  \\n So what is a heatmap?  \\n A heatmap is a graphical representation of data  \\n where values are depicted by colors.  \\n Heatmaps allow you to easier spot where something happened  \\n and where it didn't.  \\n Consequently, what we choose  \\n for our color palette is important.  \\n Two types of color palettes are sequential,  \\n and this is appropriate when data ranges  \\n from relatively low values to relatively high values.  \\n There's qualitative,  \\n and this is best when you want to distinguish  \\n discrete chunks of data that do not have inherent ordering.  \\n To create a heatmap, we first have to import our libraries.  \\n We're going to make sure matplotlib is inline,  \\n which means that our graphics will display inline  \\n in the notebook.  \\n We're going to import NumPy as np, as well as Pandas as pd.  \\n To make a heatmap, we first have to import our libraries.  \\n We're importing matplotlib, seaborn, NumPy and Pandas.  \\n The data that we're going to load  \\n is the data from a confusion matrix,  \\n which is a table that is often used to describe  \\n the performance of a machine learning classification model.  \\n It tells you where your predictions went wrong.  \\n This particular table is derived from predicting labels  \\n for digits from zero to nine.  \\n For an understanding of the data,  \\n you can think of your columns as predictions for zero,  \\n for one, two, three, et cetera.  \\n You can think of your rows as their actual values.  \\n So this row would be actual value of zero,  \\n actual value of one, two, three, et cetera.  \\n So for this number, this is when we predicted a zero,  \\n and we predicted this 37 times,  \\n and the actual value was zero.  \\n For this column, this is when we predicted a one,  \\n and we predicted it correctly 39 times,  \\n because the actual value is one here.  \\n However, we have some misclassifications  \\n where we've made some bad predictions.  \\n In this case, three times we predicted an eight,  \\n when it was actually a one.  \\n To create a heatmap with Seaborn,  \\n you do .heatmap, you import your data.  \\n In this case, I want annotations,  \\n and you'll see what that looks like in a second.  \\n I'm specifying my color map.  \\n In this case, I'm choosing a sequential color map  \\n because my data ranges from relatively low values  \\n to relatively high values.  \\n I'm setting my X and Y labels, and here's our color map.  \\n 37 times we predicted a zero when it was actually a zero.  \\n 39 times we predicted a one when it was actually a one,  \\n but a couple times, we definitely did mess up,  \\n where we predicted a one when it was really an eight.  \\n This heatmap allows us to easily pick out  \\n our higher numbers, like this 51,  \\n from our lower numbers, like these zeros.  \\n This is because we have a sequential color palette.  \\n The only difference with the code here from before  \\n is before, we had a sequential color map  \\n that was called blues.  \\n In this case, we have a qualitative color map,  \\n which is called pastel.  \\n And as you see here,  \\n it's harder to determine which is a higher number  \\n based on color alone using this qualitative color map.  \\n It's important to keep in mind  \\n that you can also create a heatmap using pure matplotlib.  \\n This just happens to be a lot of code,  \\n and as you see the image, even though we had a lot of code,  \\n there's still other things that we could change  \\n to make it equal to our Seaborn color map.  \\n For example, the color here is black instead of white,  \\n which makes it harder to distinguish this 51  \\n from this dark blue.  \\n In this video, we've learned how to create heatmaps  \\n using Seaborn and matplotlib.  \\n It's important to note what you choose  \\n for your color palette can make your visualization  \\n more or less interpretable.  \\n Additionally, sometimes it's easier to use  \\n a matplotlib wrapper like Seaborn,  \\n as it involves less code,  \\n and it's easier to make it aesthetically pleasing.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3366077\",\"duration\":213,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Create histograms\",\"fileName\":\"4499006_en_US_04_02_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":213,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"A histogram is a summary of the variation in a measured variable. It shows the number of samples that occur in a category. After watching this video, you'll be able to create Histograms using the pandas library. By doing this, you will see how to tune a graph to make it more interpretable and more aesthetically pleasing.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7243406,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Narrator] In this video,  \\n we'll learn how to create histograms using Matplotlib.  \\n When first evaluating a dataset, it's a common practice  \\n to create histograms to explore your data,  \\n as it can give you a general idea  \\n of what your data looks like.  \\n A histogram is a summary of the variation  \\n in a measured variable.  \\n It shows the number of samples that occur in a category.  \\n A histogram is a type of frequency distribution.  \\n Histograms work by binning the entire range of values  \\n into a series of intervals  \\n and then counting how many values fall into each interval.  \\n While the intervals are often of equal size,  \\n they're not required to be.  \\n If you look at our import statements,  \\n we have Matplotlib inline.  \\n We're going to import pandas as pd,  \\n as not only can we manipulate data with pandas,  \\n we'll see how to create a histogram  \\n using the plotting functionality of the Pandas library.  \\n We're also going to import matplotlib.pyplot as plt.  \\n And even though we'll be creating a histogram  \\n through the plotting functionality of the Pandas library,  \\n you can always use base matplotlib to tune your figure.  \\n The data that we'll use to demonstrate histograms  \\n is the house sales in King County USA dataset.  \\n For convenience, I put a CSV file  \\n of this dataset into your data folder.  \\n We're going to load this data into a Pandas DataFrame  \\n by using pd.read_csv,  \\n and here's our relative path.  \\n And as you can see,  \\n we have various features of a home in this data set.  \\n And what we'll be doing is visualizing a histogram  \\n of the price column.  \\n To do this, we can use the hist method.  \\n And as you can see, this is not very readable.  \\n One way to fix this is by rotating our x tick labels.  \\n To do this, we can do plt.xticks  \\n and specify that we want the rotation to be 90 degrees.  \\n And as you can see,  \\n we no longer have our x tick labels overlapping.  \\n Alternatively, you could have changed  \\n the default plot style, as oftentimes,  \\n different plot styles have different defaults.  \\n And in this case, we're specifying the plot style  \\n to be seaborn.  \\n And as you can see,  \\n we don't have overlapping x tick labels.  \\n One problem with our current visualization  \\n is that we seem to have a lot of white space.  \\n This is most likely due to outliers.  \\n Oftentimes, you're only interested in a subset of your data.  \\n Say for example, you're only interested in visualizing  \\n a subset of your data of homes under $3 million.  \\n To remove homes under $3 million,  \\n we're going to do df.loc  \\n and specifying that we want the price column  \\n and that we only want homes under $3 million.  \\n We're going to assign this Pandas series  \\n of true-false values to the variable price filter.  \\n From there, we can utilize our price filter  \\n by doing df.loc, inserting our filter  \\n of true and false values,  \\n specifying that we only want to look at the price column,  \\n and then creating a histogram off it.  \\n As you can see, we have less white space in our figure.  \\n One important thing to keep in mind  \\n is that data visualization is an iterative process,  \\n so there's always something else you can tune.  \\n Say, for example, I want to be able to distinguish my bars  \\n from each other.  \\n You can do this by specifying the edge color.  \\n In this case, I want to be black.  \\n As you can see here,  \\n I can now distinguish my bars from each other.  \\n You can also keep on tuning your graph  \\n to be more and more visually appealing.  \\n Just make sure that it's worth the effort.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:3369059\",\"duration\":279,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Create subplots\",\"fileName\":\"4499006_en_US_04_03_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":279,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"It is often useful to compare different subsets of your data side by side. After watching this video, you'll be able to create subplots using the Matplotlib library.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10474684,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this video,  \\n we'll learn how to create subplots using Matplotlib.  \\n It is often useful to compare different subsets  \\n of your data side by side.  \\n To demonstrate this, we're going to visualize images.  \\n We're going to have our figures appear inline in the notebook,  \\n so we're specifying inline.  \\n We're going to import pandas, NumPy, and Matplotlib.  \\n The next thing we have to do  \\n is you have to load our dataset,  \\n and our dataset is the digits dataset from scikit-learn.  \\n They have arranged into a CSV file for convenience.  \\n The dataset consists of pixel intensity values  \\n for 1,797 images that are eight-by-eight pixels.  \\n This means that the dataset has 64 total values per image,  \\n and each image is labeled with a number from zero to nine.  \\n We're going to load our dataset into panda's data frame  \\n by using the read_csv method.  \\n With any dataset, it's always good  \\n to check the first couple rows  \\n to make sure everything loaded properly.  \\n Columns zero to 63 are pixel intensity values  \\n for an eight-by-eight image.  \\n The label column is what the image is supposed to be.  \\n Each row in the dataset represents one image.  \\n Before we can create a subplot,  \\n it's a good idea to know how to visualize one image.  \\n To get all the column names for the pixel intensity values,  \\n we're going to utilize the df.columns attribute,  \\n and what this slice does is it says we want every column  \\n except for the last column, which happens to be our labels.  \\n The next thing we're going to do is we're going to do df.lock.  \\n We're going to specify that we want the first image  \\n in our data dataset,  \\n and we want all of its pixel intensity values.  \\n I should note that this is not yet the correct form  \\n for viewing the images.  \\n As you can see, we don't yet have an eight-by-eight array.  \\n A lesson you can take from this is that isn't important.  \\n Keep in mind that just because a dataset  \\n is stored in a certain way  \\n doesn't mean it was meant to be viewed that way.  \\n To fix this problem, we're going to utilize the reshape method  \\n to reshape it into an eight-by-eight array.  \\n As it's not easy to understand pixel intensity values  \\n by looking at an array, let's visualize the image.  \\n To do this, we're going to utilize the imshow method,  \\n and we're going to specify that we want a gray color map.  \\n As you see here, this image looks like a zero.  \\n The reason why it looks a little blurry  \\n is because it's only an eight-by-eight image,  \\n We're now going to create a five-by-five subplot.  \\n The way to create a subplot  \\n is by utilizing the dot subplot command,  \\n specifying how many rows you want,  \\n how many columns you want.  \\n The one here is an index, so out of our five plots,  \\n this is saying that this is the first one.  \\n The next thing we have to do  \\n is we have to create our image values.  \\n This bit of code are the image values for our first image,  \\n as you can see here.  \\n What this code is doing is we're specifying  \\n that we want the first image and we want its label,  \\n and we are assigning it to the variable image_label.  \\n And then from here, we're visualizing our image,  \\n and as before, we're reshaping our array  \\n to be eight-by-eight.  \\n Next, we're going to insert a title on our plot,  \\n so that we know what the image is supposed to be of.  \\n From here, the next bit of code  \\n says that this is going to be the second image in our subplot.  \\n We also want the image at index one.  \\n We want the label for the image at index one,  \\n and similarly, we're going to do the same thing  \\n for the third image, the fourth image, and the fifth image.  \\n And as you see here, we have five plots side by side.  \\n As you've probably noticed in the code,  \\n we seem to have a lot of duplication of effort.  \\n Let's eliminate that with a for loop.  \\n In this code, what range is doing is in the first iteration,  \\n zero is assigned temporarily to the variable index.  \\n We're doing one plus our index, which happens to be one,  \\n so we're saying that this is the first subplot.  \\n What this next code is doing  \\n is it's saying that we want all the pixel intensity values  \\n for the row with the index label of zero.  \\n In the next iteration of the loop, we're taking one,  \\n and we're temporarily assigning it to the variable index.  \\n We're doing one plus one, which is two,  \\n which is saying that this is the second subplot.  \\n And what this next bit of code is doing  \\n is it's saying that we want all the pixel intensity values  \\n for the index label of one.  \\n A similar process happens for the rest of the for loop.  \\n As you see here, we now have our images side by side  \\n with a lot less code.  \\n In this video, we've learned  \\n not only how to visualize images,  \\n but also how to visualize images side by side  \\n by using subplots.  \\n It's important to note that there are cases  \\n where we can use a lot less code by utilizing for loops.  \\n \\n\\n\"}],\"name\":\"4. Customize Visualizations with Matplotlib\",\"size\":26155208,\"urn\":\"urn:li:learningContentChapter:3369061\"},{\"duration\":66,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:3374063\",\"duration\":66,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Next steps\",\"fileName\":\"4499006_en_US_05_01_MM24\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":66,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Data visualizations help you communicate and understand your data. After watching this video, you'll be able to feel confident and ready to go build your own powerful visualizations using Python.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2852992,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - I'd like to congratulate you on finishing the course.  \\n I hope you've enjoyed taking this course  \\n as much as I've enjoyed creating and teaching it.  \\n As you probably know, your learning doesn't stop here.  \\n Python is a very expansive topic,  \\n and there's a lot of resources available  \\n that can help you keep learning.  \\n Here are a few resources to help with your journey.  \\n First, check out the Matplotlib gallery.  \\n Here you can find a whole host of plots you can create.  \\n Next, check out interactive data visualization libraries  \\n like Folium and Boca.  \\n Also, be sure to check out my blogs  \\n or visit my YouTube channel.  \\n On my blog, I cover everything  \\n from installations to basic Python  \\n to advanced machine learning.  \\n If you're looking for a data science job,  \\n I encourage you to check out my blog post titled  \\n \\\"How to Build a Data Science Portfolio\\\"  \\n to help you on your way.  \\n And finally, stay in touch.  \\n You can connect with me on LinkedIn.  \\n I'd love to hear from you.  \\n So that's it.  \\n Thanks again for watching this course.  \\n Now get out there and go create  \\n compelling day visualizations.  \\n Good luck.  \\n \\n\\n\"}],\"name\":\"Conclusion\",\"size\":2852992,\"urn\":\"urn:li:learningContentChapter:3373069\"}],\"size\":178609500,\"duration\":4883,\"zeroBased\":false},{\"course_title\":\"Python Statistics Essential Training\",\"course_admin_id\":4433355,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":4433355,\"Project ID\":null,\"Course Name\":\"Python Statistics Essential Training\",\"Course Name EN\":\"Python Statistics Essential Training\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"The field of statistics has become increasingly dependent on data analysis and interpretation using Python. With the rise of big data and data science, the demand for professionals who can effectively analyze and interpret data using Python has skyrocketed. In this course, Matt Harrison teaches you how to collect, clean, analyze, and visualize data using the powerful tools of the Python programming language. Join Matt as he gives into the various techniques that form the backbone of statistics and helps you understand the data with summary statistics and visualizations. He explains how to create predictive models using both linear regression andXGBoost, and wraps up the course with a look at hypothesis testing.If you\u00e2\u20ac\u2122re interested in exploring statistics using a code-first approach, join Matt in this course as he shows you how to use Python to unlock the power of data.\",\"Course Short Description\":\"Learn to use Python to unlock the power of data and use it to inform decisions.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":9254436,\"Instructor Name\":\"Matt Harrison\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Python and Data Science Corporate Trainer, Author, Speaker, Consultant\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2023-08-17T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/python-statistics-essential-training-19258005,https://www.linkedin.com/learning/python-statistics-essential-training-2023-reboot\",\"Series\":\"Essential Training\",\"Limited Series\":null,\"Manager Level\":\"General\",\"LI Level\":\"Advanced\",\"LI Level EN\":\"Advanced\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Data Science\",\"Primary Software\":\"Python\",\"Media Type\":\"Video\",\"Has CEU\":\"Yes\",\"Has Exercise Files\":\"Yes\",\"Visible Duration\":9565.0,\"Visible Video Count\":31.0,\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":277,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4509084\",\"duration\":50,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Being a Python statistics MVP\",\"fileName\":\"4433355_en_US_00_01_WL30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"In this video, explore a compelling example of the topic in action and learn how to use this space to grab the browsing learner. The TOC and course description exist on the course's home page to communicate what's in the course and who it's for.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3187642,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - Have you ever wanted to explore statistics  \\n using a code-first approach?  \\n You've come to the right place.  \\n This Python-based course will dive into various techniques  \\n that form the backbone of statistics.  \\n We'll understand the data  \\n with summary statistics and visualizations.  \\n Then we'll create predictive models  \\n using both linear regression and XGBoost.  \\n Finally, we'll explore hypothesis testing.  \\n Because we use Jupyter Notebooks and GitHub Codespaces,  \\n you can easily run the same code  \\n and follow along in a web-based environment  \\n without any complicated installation.  \\n Hi, I'm Matt Harrison,  \\n a Python and data science corporate trainer.  \\n I'm excited to join you  \\n on this incredible statistics adventure together  \\n using a code-first approach.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4509083\",\"duration\":41,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What you should know\",\"fileName\":\"4433355_en_US_00_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, discover any use of GitHub or other project tools employed in the course.\",\"captionsStatus\":\"NOT_AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":962095,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":null},{\"urn\":\"urn:li:learningContentVideo:4505089\",\"duration\":186,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Using GitHub Codespaces with this course\",\"fileName\":\"4433355_en_US_00_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"In this video, learn how to use Codespaces with the course.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7492655,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson,  \\n I'm going to show you how to configure codespaces  \\n for this course.  \\n You want to navigate to the course repository  \\n and then click on this green Code button.  \\n You'll see that there's a local tab and a codespaces tab.  \\n If you wanted to run this locally, you could,  \\n if you know how to get a Python environment set up,  \\n there's a requirements.txt  \\n that you can use PIP to install the requirements  \\n if you want to run locally.  \\n Alternatively, you can use codespaces, which is really easy.  \\n Click on this codespaces tab  \\n and then come over here and say create codespace on Main.  \\n That's going to open up a new tab here,  \\n which is going to set up your codespace.  \\n This is going to take a while to run.  \\n We'll let it do its thing,  \\n and then we'll come back when it's finished.  \\n Okay, this has launched codespaces.  \\n If you're not familiar with codespaces,  \\n basically this is a web environment  \\n where you can work on a project.  \\n What it's done is it's launched a virtual machine  \\n and we have a web environment  \\n that lets us talk to the machine.  \\n You can see that I'm in a terminal right here.  \\n I can say LS and I have access  \\n to all of my favorite Linux commands here.  \\n We're also launched in VS Code, so for this course,  \\n all we have to do is click on this Pystats.ipynb.  \\n This is the notebook for the course.  \\n There is a solutions notebook as well  \\n that says -Solutions.  \\n Don't open that one.  \\n You're going to want to open the one that doesn't have  \\n the -Solutions.  \\n So double click on this  \\n and it will open the notebook in VS Code.  \\n If you want to get rid of the explorer over here,  \\n you can click on that to remove that.  \\n Okay, so here is our notebook,  \\n and what I want you to do is just click on  \\n this first cell that has code on it.  \\n I'm just clicking on the left hand side of this rectangle.  \\n Hold down Control and hit Enter.  \\n That's going to run this cell,  \\n and you can see it has some options pop up  \\n when we do that, it says type to choose a kernel source.  \\n It says Python Environments  \\n and it says Existing Python Server.  \\n So we have a Python environment configured here.  \\n Let's just click on that, which is this Python 3.10.8.  \\n Just click on that.  \\n We'll let it do its thing.  \\n You can see in the lower right hand corner,  \\n it says it's connecting to a kernel.  \\n Now you can see it's running and it has finished running,  \\n at this point, you can see it says 2.0.2.  \\n In this lesson,  \\n we showed you how to open a project in GitHub,  \\n create a codespace, and then connect a Jupyter Notebook  \\n to that codespace and execute it through codespaces.  \\n Codespaces is a nice tool that you find  \\n on GitHub that lets you easily start projects  \\n without having to worry about configuring environments.  \\n I recommend using them if you're not familiar  \\n with virtual environments in Python.  \\n \\n\\n\"}],\"name\":\"Introduction\",\"size\":11642392,\"urn\":\"urn:li:learningContentChapter:4509085\"},{\"duration\":2954,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4502196\",\"duration\":328,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Loading data\",\"fileName\":\"4433355_en_US_01_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"With the skill of loading data with Pandas, you can easily read data from various file formats such as CSV, Excel, SQL databases, and more, manipulate and clean data using Pandas' powerful tools, perform data analysis and exploration, and prepare data for modeling and visualization. Pandas is widely used in the data science community, making it a valuable skill for anyone working with data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12963459,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson, we are going to look at  \\n loading the data.  \\n I've opened the notebook in Codespaces.  \\n If you want to run this locally,  \\n you can run this locally as well.  \\n I'm running the PyStats, IPy and B notebook  \\n and I'm just going to hide this  \\n so we have a little bit more screen real estate.  \\n So we're going to be looking at loading the data.  \\n We're going to be using the Ames, Iowa Housing Data set.  \\n This is a nice little data set.  \\n It's got a couple thousand lines of data  \\n about housing sales in Iowa, and it's got 80 plus columns  \\n of information around those cells.  \\n So this is going to be a great data set for us to look at  \\n and examine some statistics using it.  \\n Let's go down and run some of these cells here.  \\n The first cell I'm going to run is this import cell,  \\n and to run that, I'm going to hold down control  \\n and hit enter.  \\n What that does is it tells Jupiter we want to run the cell.  \\n I like control and enter because it leaves the focus  \\n on the cell.  \\n What that allows me to do is hit enter  \\n and I can go in and edit this cell if I want to after that  \\n and then I can rerun that by holding down control  \\n and hitting enter again.  \\n So we're checking the version of Pandas pd.__version__,  \\n we call that Dunderversion  \\n and we see that we are using Panda's version 2.02.  \\n So one thing to be aware of in this course  \\n is that we are using the latest version of Pandas  \\n as of the recording, Pandas Two  \\n and Pandas Two has a few features that allow it  \\n to do things more efficiently  \\n and with less memory than Pandas One.  \\n So I'm going to be using some Pandas Two functionality  \\n in this course.  \\n If you don't have Pandas Two, some of the code might not  \\n work for you, but hopefully I can convince you  \\n that you probably want to upgrade to Pandas Two  \\n to take advantage of the new features there.  \\n Let's load the data set in this cell.  \\n Again, I'm going to hold down control and hit enter  \\n and this should run.  \\n At the bottom here, you see I'm using the read_CSV function  \\n to load the data.  \\n I'm passing in a URL.  \\n We've got this locally on our code space.  \\n I also have a variable pointing to where you can get this  \\n on the internet if you want to as well.  \\n When you look at this read_CSV function, this probably  \\n looks different if you're used to Pandas One  \\n with the two parameters, engine and dtype_backend.  \\n Both of those we are specifying as pyarrow.  \\n What engine equals pyarrow does  \\n is it uses the arrow library to load the CSV file.  \\n This can be a lot quicker than using the standard  \\n Pandas logic to do that.  \\n And what dtype_backend does is it allows you  \\n to store the data types using the arrow library instead  \\n of NumPy, and that gives you a lot of memory benefits  \\n as well.  \\n Let's inspect the shape of this dataset.  \\n I'm going to go down and execute this next cell.  \\n Again, holding down, control and hitting enter to run that.  \\n You can see that this returns a tupple.  \\n There's 2,930 rows and 82 columns.  \\n Let's look at the first few rows of data.  \\n I'm going to use this head method to do that  \\n and you can see the first five rows.  \\n There are 82 columns in here.  \\n You can scroll over if you want to and see some of it.  \\n Note that we are getting some of the columns truncated here.  \\n You can see that there's an ellipses.  \\n We're only showing you the first 10 columns  \\n and the last 10 columns, but we've got 82 columns of, again,  \\n housing sales data from Ames, Iowa.  \\n One of the things I like to do is run the describe method  \\n and what the describe method does is gives us  \\n summary statistics for each of the numeric columns  \\n in the data frame.  \\n In the index now you can see on the left hand side  \\n where it says count, mean, STD, min,  \\n that is the index of this data frame  \\n and indexes don't have to be numeric.  \\n Pandas will let them be non-numeric, and in this case,  \\n we can see that we have non-numeric,  \\n we have string entries for the index  \\n representing the statistical results  \\n of each of these numeric columns.  \\n So for order, this is the order of cells,  \\n and you can see in the count row, 2,930.  \\n Now, one thing to be aware of with Pandas  \\n is that count probably doesn't mean what you think it means.  \\n Count in Pandas means the number of non missing values.  \\n So, if we look over a little bit at the lot frontage count,  \\n it says that that is 2,440.  \\n So what that means is that there are 2,440 entries there  \\n that had values and that there are a few of them  \\n that are missing values.  \\n We also have the mean, the standard deviation,  \\n the minimum value, and the maximum value,  \\n and then the inner quartiles there.  \\n I like to use this for a gut check to understand  \\n what the data looks like.  \\n You can see that a lot of these numeric values  \\n don't go very high and they're also non-negative.  \\n By default, Pandas is going to use 64 bits of information  \\n to store this data.  \\n We can probably save more memory by changing those types  \\n and I'll show how to do that in a future lesson.  \\n In this lesson, we loaded the data  \\n for the Ames, Iowa dataset  \\n and then we did some basic inspection and sanity checks  \\n to make sure that we have the data loaded.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4504093\",\"duration\":959,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Strings and categories\",\"fileName\":\"4433355_en_US_01_02_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"With the skill of cleaning categorical data, you can ensure that the data is accurate and consistent, which is critical for making meaningful analyses and predictions. You can also effectively handle missing values, outliers, and other anomalies in the data, which can improve the accuracy of your models and prevent biased results.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":34827803,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson,  \\n we're going to look at strings and categories  \\n and understand these types of columns  \\n and how to deal with them in Pandas.  \\n I've listed out some of the goals here.  \\n We want to be able to use the dtypes attribute.  \\n We want to look at using the select_dtypes method.  \\n We're also going to do summary statistics  \\n using the describe method.  \\n And we're going to look at memory usage  \\n and converting string columns to category types.  \\n One of the attributes on a data frame  \\n is this dtypes attribute.  \\n And when you run this,  \\n what this is going to give you is a Pandas series.  \\n In the index of this series,  \\n we have the columns.  \\n Remember a Pandas index does not need to be numeric.  \\n In this case, Pandas is sticking the columns into the index  \\n and then the values for the index,  \\n we can see that says int64,  \\n and then in brackets, pyarrow,  \\n that is the values of this series.  \\n And we can see in brackets it says pyarrow  \\n indicating that we are using Pandas 2  \\n and we are using that pyarrow backend.  \\n Again, because we're using the pyarrow backend,  \\n we're going to get some speed improvements,  \\n we're also going to get some memory improvements as well.  \\n One of the big features of Pandas 2  \\n is that it has a native string type.  \\n We can see for MS zoning that the data type is a string,  \\n and in brackets, pyarrow.  \\n In Pandas 1,  \\n if we wanted to select the columns that were strings,  \\n we would say select_dtypes objects.  \\n And this doesn't work in Pandas 2  \\n because the strings don't have the object type.  \\n This is what we need to do in Pandas 2.  \\n We're going to say select_dtypes and say string,  \\n and we're going to pass that in as a string.  \\n If you look at the output of this,  \\n at the bottom it says you have 2,930 rows and 43 columns.  \\n So this is selecting only the strings column.  \\n You can see in the comment up there I said,  \\n alternatively you can say strings,  \\n and in bracket, pyarrow,  \\n if you'd like the type.  \\n Now, one of the things you can do once you have this  \\n is you can throw on this describe method.  \\n So I'm going to actually throw on a describe  \\n and throw on the transpose on that as well.  \\n I like to sometimes transpose my data  \\n just to see a little bit more.  \\n Let's run this without the transpose,  \\n that's the capital T there.  \\n And you can see that we have 43 columns  \\n and I have to scroll over a little bit to see that.  \\n You can also see that there's an ellipsis here,  \\n so it's not showing all of that.  \\n Sometimes transposing the data,  \\n again, that's flipping the rows and the columns,  \\n lets us see just a little bit more.  \\n Remember we ran describe previously on our data frame  \\n and it gave us summary statistics for the numeric columns.  \\n In this case, we don't have any numeric columns  \\n because we said select_dtypes string.  \\n We just have string columns.  \\n And so when we run describe  \\n on a data frame composed of only string columns,  \\n we get summary statistics for the strings.  \\n In this case, we get the count of non-missing values.  \\n We get the number of unique entries for that.  \\n We also get the most common entry,  \\n so that's the entry with the column top,  \\n and the frequency of that.  \\n So for the MS zoning column,  \\n there are 2,930 values in the rows.  \\n There are seven different values.  \\n The most common value is RL  \\n and it occurred 2,273 times.  \\n You can see for most of these,  \\n that unique value is not very high.  \\n Statisticians sometimes call that cardinality.  \\n So a low cardinality might be a prime candidate  \\n for us to convert this into a categorical type in Pandas,  \\n allowing us to save even more memory.  \\n So in this cell, I'm going to run select_dtypes string  \\n and then I'm going to say memory_usage, deep is equal to true,  \\n and then I'm going to do sum.  \\n And what this is going to give me is the number of bytes  \\n used by the string type columns.  \\n This code is written as a chain.  \\n You can see that we have highlighted the parentheses here.  \\n There's a parenthesis above and below it.  \\n I like to write my code that way.  \\n What it allows me to do is put each operation  \\n on a single line  \\n and it makes your code read like a recipe.  \\n So let me just comment these out  \\n and we'll walk through what's going on in our chain.  \\n Here's our original data frame,  \\n and you can see that there's 2,930 rows here and 83 columns.  \\n If I select just the object columns,  \\n you can see that that goes down to 43 columns.  \\n And then I'm going to say on that,  \\n memory_usage deep is equal to true.  \\n So this is going to return a series  \\n and it's going to tell us how many bytes each column is using.  \\n And then I'm going to sum up this series  \\n and get the total of that.  \\n So it looks like this is using 957 kilobytes of data.  \\n In this next cell,  \\n I'm going to do a similar operation,  \\n but I'm going to insert this astype category.  \\n And when I run that,  \\n you can see that this uses quite a bit less memory.  \\n And what's going on under the covers  \\n is that Pandas is encoding all of the unique types  \\n into a number  \\n and then it's using basically an array  \\n with those unique values pointing back  \\n to the original values.  \\n So it doesn't have to store strings for every entry,  \\n it can just hold a numeric value.  \\n And you save quite a bit of memory if you do that.  \\n You can see that this is almost seven times  \\n more memory-efficient  \\n by converting that to a categorical type.  \\n Okay, so remember we have 2,930 rows.  \\n So one of the things I might want to do  \\n is look at the number of rows that are missing.  \\n And for numeric data,  \\n I would do a chain that looks something like this.  \\n Let me just walk through what's going on here.  \\n I'll comment this out  \\n and explain the code as we're going along.  \\n So the first operation I'm going to do is this isna method.  \\n What that's going to give me back is a data frame,  \\n but rather than having the original values,  \\n it has true/false values.  \\n And then what I'm going to do  \\n is I'm going to do the mean operation on that.  \\n One thing to be aware of in Python,  \\n and also in Pandas because Pandas is in Python,  \\n is that Python treats true and false values as one and zero,  \\n respectively.  \\n So if I take the mean of this,  \\n this is going to essentially be taking the mean  \\n of a data frame with zeros and ones in it.  \\n And what that's going to give me  \\n is the fraction of the values that are missing.  \\n So you can see, for example,  \\n it says that lot frontage,  \\n 0.167 of those are missing.  \\n So if I multiply this by 100,  \\n I get the percentage that is missing.  \\n So 16% of the lot frontage values are missing.  \\n Now what I'm going to do is I'm going to use the pipe method here  \\n and filter out the values that are zero.  \\n What pipe allows us to do is pass in a function into it.  \\n In this case, I'm using a lambda function  \\n that takes the series that is passed in,  \\n the series that you're seeing on the screen here.  \\n And then I'm just going to use a Pandas operation  \\n to filter out the values  \\n where the series is greater than zero.  \\n And here's the entries for that.  \\n You can see that a lot  \\n of the lot frontage values are missing  \\n and the basement year built values are missing,  \\n and then a few of the other values are missing as well.  \\n One of the things that stands out to me  \\n when I look at this is, for example, this 0.03413.  \\n This just kind of makes my spidey-sense tingle a little bit  \\n because that same value is repeated in multiple places.  \\n So that probably means that there's probably a row  \\n or a few rows where these values are missing  \\n for all of those.  \\n That's probably what's happening there  \\n when you see those repeated values.  \\n That previous code cell that we ran,  \\n if you think about it,  \\n is actually not containing any string columns in it,  \\n these are all numeric columns.  \\n So this is a difference in Pandas 2.  \\n In Pandas 2, if a string column is missing a value,  \\n Pandas will actually set that to an empty string  \\n rather than the NA value.  \\n So if we want to find the missing string values,  \\n we actually need to,  \\n instead of making this isna data frame,  \\n we're going to check  \\n whether the data frame is equal to the empty string.  \\n Again, let me just run through this quickly.  \\n I'm going to comment this out.  \\n The chain makes it really easy to comment this out.  \\n So here's our data frame with the string columns  \\n and we're going to say,  \\n are these values equal to the empty string?  \\n And this gives us back our Boolean array,  \\n and then we can just repeat the process  \\n that we did there before  \\n and get the percentage of values that is missing.  \\n In this cell, what I'm going to do is I'm going to select  \\n any of the rows where the values are missing.  \\n So I'm going to pull this out here  \\n and I'm actually going to walk through what's going on here.  \\n Because this is a useful piece of code,  \\n let's walk through what's going on here.  \\n So I have it written in a single line.  \\n It's easier for me to show you what's going on  \\n if I write it in this chain style like this.  \\n And I'm going to comment this out  \\n and we'll just walk through this.  \\n Okay, so here is my string columns.  \\n Where are they equal to the empty string?  \\n And this is a data frame of true/false values.  \\n Now I'm going to do the any along the axis is equal to columns.  \\n So this is going to keep the same index.  \\n So if you look at this, you'll see the same index,  \\n but this is going to return back a series  \\n whether any value in the row was true.  \\n And you can see in this case, there are a bunch of falses,  \\n but presumably some of those are true.  \\n And that's what I'm passing in here,  \\n but I'm also passing in this tilde at the front here,  \\n which negates that.  \\n So what this is going to give me  \\n is it's going to give me the rows  \\n where there are missing values.  \\n And it looks something like this.  \\n We can scroll over a little bit,  \\n and it doesn't look like we're seeing  \\n very many missing values here.  \\n One thing to note  \\n is that we are seeing a bunch of NA values as well.  \\n This dataset is a little messy  \\n in that it has missing values that aren't in there at all,  \\n but it also has NA that is encoding missing values as well.  \\n So that's something to be aware of.  \\n Let's just repeat our operation here,  \\n but instead of looking for the empty string,  \\n looking for the NA string.  \\n And you can see, for example,  \\n that 93% of the alley values are missing  \\n and 48% of the fireplace quality values are missing.  \\n Okay, let's inspect this a little bit more.  \\n I'm going to inspect, where is the pool QC,  \\n the pool quality missing?  \\n Let's run that.  \\n And it looks like there aren't any values  \\n where it's missing.  \\n Again, remember that this Ames dataset  \\n encodes some of the missing values as NA.  \\n Let's run this with isna instead.  \\n And these are all the values where the pool QC  \\n or the quality of the pool is missing.  \\n Presumably, these houses don't have pools  \\n and hence this value is missing.  \\n So that's probably an appropriate encoding there for that.  \\n So one thing I might want to do is I might want to go in  \\n and clean up this data set a little bit.  \\n And so here's one of the ways that I would do that.  \\n I would use the assign method,  \\n and here I'm passing in a chain inside of assign.  \\n So I'm passing in select the dtypes of string,  \\n and with all of the string types,  \\n if there's any missing values,  \\n just fill that in with not applicable.  \\n And then what I'm going to do is I'm going to pass it into assign.  \\n Assign generally takes a parameter,  \\n which is the column name,  \\n and a value for the column name to make a new column.  \\n But if we pass in a data frame  \\n and we put these two stars in front of it,  \\n it will say, take in this data frame  \\n and just update the values with this data frame.  \\n So essentially, what this is giving us back  \\n is a new data frame with the empty string values replaced  \\n by the not applicable values.  \\n Okay, let's go a little bit further here.  \\n Let's look at electrical.  \\n There were some missing values in electrical,  \\n so let's look at that.  \\n And this is how I like to look at string columns.  \\n I like to use this value_counts method.  \\n What that gives me back is a Pandas series,  \\n and it gives me in the index the unique values,  \\n and in the values of the series, the counts of those.  \\n You can see, for example, this row right here  \\n is the empty string values there.  \\n So there's one that is missing, the electrical is missing.  \\n So it'd be good to check, why is the electrical missing?  \\n Is this a house that did not have an electrical system?  \\n Or did this data get dropped?  \\n We probably don't want to leave this as an empty string,  \\n we probably want to figure out what's going on there  \\n and clean that up.  \\n Let's look at the row where it actually is missing.  \\n This is how I would do that.  \\n I would use the query method and say a string, electrical,  \\n equals equals the empty string.  \\n And here's the row where that is missing.  \\n We can inspect what's going on here.  \\n It looks like this was a house that was sold  \\n in May of 2008 for $167,500.  \\n So let's look at another example.  \\n I'm going to do the same thing with fireplace quality.  \\n And in this case,  \\n you can see that there aren't empty strings,  \\n but we do have quite a few NA values,  \\n that's the most common value.  \\n Let's do the same thing with basement condition.  \\n Basement condition has both NA values  \\n and a missing value as well.  \\n So you can see that this dataset is not maybe as clean  \\n as we would like.  \\n Maybe we would want to do something like this  \\n where we say, okay, let's take this data frame  \\n and I'm going to update all of the string columns,  \\n replacing the empty values with not applicable,  \\n and then converting those to a category.  \\n This would be my operation that would do that.  \\n I can just test that out here and check that that does work.  \\n It looks like that was successful.  \\n And this returns 2,930 rows.  \\n Let's check how much memory usage that's going to use.  \\n That's going to give us a data frame  \\n that it looks like is using one meg of memory.  \\n Let's compare that with our original data frame  \\n that is not converted to a category,  \\n and that's using 1.8 megs of memory.  \\n So you can see by converting two categories,  \\n we will save quite a bit of memory.  \\n In this lesson, we looked at some of our string columns  \\n and we looked at various operations  \\n that we would do with string columns.  \\n We did summary statistics on them,  \\n but we also explored some of the missing values.  \\n We did that by using some Pandas manipulation  \\n to find the percentage of values that are missing  \\n and then we dove into those using value_counts  \\n and also using query to examine those.  \\n Understanding what is in your data is an important step  \\n and I highly recommend that you do this step  \\n and don't skip it.  \\n The more you understand your data,  \\n the better you'll be able to explain the data  \\n and share insights with others.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4507091\",\"duration\":957,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Cleaning numbers\",\"fileName\":\"4433355_en_US_01_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"The goal of this video is to learn how to clean numbers by handling missing values, handling outliers, and addressing inconsistencies in the data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":40337860,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson we're going to discuss  \\n cleaning numbers.  \\n We're looking at the PyStats notebook  \\n in the 0 1 0 3 section.  \\n Our goals are going to be using select D types,  \\n using describe, finding missing values,  \\n and I'll also show you how to view a little bit more data  \\n than the default values and style the output.  \\n In pandas one, there would be a lot of entries  \\n for the Float D type here.  \\n That's because in pandas one,  \\n when pandas encountered a integer value  \\n that had missing values, it converted it to a float.  \\n You can see that there aren't any missing values here.  \\n That's because we're using pandas two.  \\n Remember that PyArrow backend  \\n actually has support from missing values.  \\n And so if we say select D types is int  \\n we get 39 columns here.  \\n So there are no floating point columns  \\n there's just integer columns in here.  \\n So let's explore some of these columns  \\n and see what's going on.  \\n So again, one thing that we can do,  \\n is run the describe method  \\n to get summary statistics on those.  \\n One of the things I like to do  \\n is look at the count,  \\n just to see how many values are missing.  \\n For example, you can see that lot frontage is missing  \\n quite a few values.  \\n I also like to look at the minimum values  \\n and the maximum values to see what the range is.  \\n You can also look at the mean,  \\n which a lot of people would refer to as the average,  \\n though a statistician would probably get upset  \\n at you if you said that.  \\n And you can look at the 50th percentile  \\n which a statistician would call the median.  \\n If the mean and the median are around the same,  \\n you might have data that's somewhat normal,  \\n though I also like to look at visualizations  \\n to understand what the lay of the land looks like  \\n or the distribution of that data looks like as well.  \\n Okay, so remember,  \\n we've got 2,930 columns there.  \\n So a lot of these columns aren't missing any values  \\n but some of them are.  \\n So let's look at this lot frontage one.  \\n This is how I would explore it.  \\n I'm going to run the query method and say,  \\n I want to look at where the lot frontage is missing.  \\n Let's dive into what's going on inside of query here.  \\n So query expects you to pass in a string that  \\n takes a column and some operation on the column.  \\n In this case, the column is lot frontage  \\n and, because lot frontage has a space in it,  \\n it is not a valid Python attribute.  \\n So if you have a column name that has characters  \\n that aren't valid Python attributes,  \\n you need to surround them by backticks.  \\n That's what we see here.  \\n And then I'm going to say, okay,  \\n we've got that lot frontage column,  \\n let's just look where it is missing.  \\n And you can see that there are 490 rows  \\n where the lot frontage is missing.  \\n You can see it's represented here by NA  \\n in less than and greater than.  \\n One of the questions I frequently get when I teach people  \\n about missing values is how to deal with them.  \\n My typical response is, it depends,  \\n but probably the best way to deal with them  \\n is to find a subject matter expert,  \\n someone who knows why this data is missing,  \\n and they can often point you in the right direction.  \\n For example, lot frontage.  \\n Does NA mean that there is not a front area  \\n of this house or does this mean that lot frontage  \\n is missing that we don't have that information?  \\n That would be something that would be useful to find out.  \\n If there is not a front of the house,  \\n I would probably encode that as zero rather than NA.  \\n So again, it's important to talk to a subject matter expert  \\n and find out why exactly this data might be missing.  \\n Okay, remember I promised I would show you how to  \\n see more data and this is the secret to doing that.  \\n Pandas, by default, is not going to show you all of the columns  \\n and all the rows if you have a large data set.  \\n And, I think this is useful and a lot of people,  \\n as soon as I tell them this,  \\n ask me how do I get around this?  \\n I would caution you to refrain from trying  \\n to get around this as soon as you can.  \\n Humans aren't really meant  \\n for looking at large tables of data.  \\n We're not optimized for doing that  \\n and it's not an effective use of our time, really.  \\n What you want to do is you want to  \\n use something that is effective  \\n for pulling out data that is interesting.  \\n So my recommendation is when you feel that urge to look  \\n at more columns or more rows,  \\n think, can I use a query or a filter,  \\n to pull out the data that is interesting to me.  \\n Alternatively, can I visualize this,  \\n and might that give me more insight into the data?  \\n You can tell a really good story by visualization  \\n that's a lot stronger than presenting someone  \\n with a table of a bunch of numbers.  \\n So how we're going to do this in pandas  \\n is we're going to use the context manager,  \\n that's the with statement.  \\n And we're going to use this option context.  \\n Context manager is a construct in Python that allows you to  \\n enter into a context and exit out of that.  \\n In this case, when we use option context,  \\n in the indented portion here, we're calling display,  \\n while we are indented, what ever we pass  \\n into option context is going to overwrite the values.  \\n When we un-indent from the context manager  \\n it will revert back to what it was before.  \\n So we're going to change the minimum number  \\n of rows that we view to 30 and we're going to change  \\n the maximum columns to 82.  \\n So you can see that we're now seeing quite a bit more data  \\n and if we scroll over  \\n you can see that we are seeing a lot more columns  \\n than we were previously.  \\n If you're not familiar with this feature in Jupiter,  \\n one of the things you can do is put a question mark  \\n after a method or a function in Jupiter  \\n and it will pull up the documentation.  \\n So off of the style attribute,  \\n there are various methods on there.  \\n One of those is set sticky.  \\n What this allows us to do is set the column  \\n or the index to sticky so when we scroll  \\n we can see the index.  \\n This often comes in very handy.  \\n In this example I'm going to show a lot more data  \\n than I normally show.  \\n And then I'm going to run this query  \\n where I'm showing the missing values of lot frontage  \\n and then I'm going to set  \\n the sticky columns  \\n and the sticky index.  \\n The output looks like this, and if I scroll over,  \\n you can see that that index there  \\n stays on the left hand side.  \\n So this comes in useful when you need to find  \\n out what row is going on.  \\n You'll note that I do have a comment here  \\n at the top,  \\n that columns is broken.  \\n I think this is a pandas two bug, where,  \\n the columns are not being sticky here when I scroll down.  \\n Hopefully they fix that.  \\n Okay, let's look at where garage year built is missing.  \\n So here are the rows where garage year built is missing.  \\n It looks like there's 159 of those rows.  \\n Again, we could scroll through this data  \\n and try and determine what a missing garage  \\n year built means.  \\n I am assuming that it probably means  \\n there is not a garage on here.  \\n Hence, the missing value.  \\n In this case because year built is a year,  \\n putting in a default value of a zero to encode missing  \\n probably wouldn't be the right choice  \\n because if you wanted to look at  \\n the average of the year built that's going to throw that off.  \\n So in this case, garage year built,  \\n leaving those values as missing, might be useful.  \\n However, if you want to do some operations  \\n such as running linear regression on this,  \\n that might be problematic,  \\n because, linear regression does not like to  \\n have missing values in the data.  \\n You might need to find a way to encode that,  \\n such that you can run some of these other tools  \\n with columns that have missing data in them.  \\n I'm going to take that year built column  \\n and I'm going to run summary statistics  \\n on that just to see what's going on there.  \\n And, you can see that the count is demonstrating  \\n that we are missing quite a few values.  \\n I like to look at the minimum value and the maximum value.  \\n Now this actually stands out to me, the maximum value here.  \\n 2 2 0 7, remember this is a year,  \\n and as of this recording we're recording this in 2023.  \\n So this garage is being built a few years into the future.  \\n That's somewhat problematic to me  \\n and that probably indicates that there is  \\n some sort of a typo going on here.  \\n This was probably 2007  \\n but it got fat fingered on data entry.  \\n Let's look at that row where that is missing.  \\n And again, I'm going to use my option context  \\n just to display a few more of those columns.  \\n And, I'm going to put in as my query,  \\n garage year built,  \\n remember I'm going to use back ticks around that  \\n because there are spaces ,  \\n and then greater than 2020.  \\n And maybe I'll scroll over here.  \\n You can see that the year remodel or add  \\n is 2006,  \\n and the garage year built is 2,207.  \\n And the year sold is 2007.  \\n So again, I'm going to assume that this is a typo  \\n and we might want to go in and clean that up.  \\n Now, because I've found this by going through the steps  \\n of manually looking at the data,  \\n looking for missing values and also looking at describes,  \\n this makes me want to look at maybe  \\n some of the other year columns,  \\n maybe some of those are  \\n fat fingered as well.  \\n So, one method that comes in handy is the filter method,  \\n and it allows me to pull off various columns.  \\n So I'm just going to pull up the documentation there.  \\n You can see that this says it subsets the data frame rows  \\n or columns according to a specified index.  \\n So there are various ways to do this  \\n using items like or regex.  \\n We can scroll down here  \\n and see that items means keep labels,  \\n which are in items.  \\n So in this case we're passing in a sequence of labels  \\n and we keep the labels which are in that sequence.  \\n You can pass in like which is a string.  \\n It says keep labels from the axis  \\n for which like in label is equal to true.  \\n That would allow us to do SUBSTRING matching,  \\n but apparently it would only allow us to do one substring,  \\n whereas items allows us to match multiple strings.  \\n Doesn't look like there's an option to match  \\n multiple substrings.  \\n However, we could possibly also use this next option,  \\n which is a regular expression,  \\n and we could probably formulate a regular expression  \\n that would allow us to match multiple substrings as well.  \\n Here's an example of using that.  \\n I say filter like is equal to yr.  \\n You can see there are two columns that have yr in them.  \\n If I want to check which of those has  \\n a column greater than 2023,  \\n which is the current year,  \\n I'm going to do this chain here.  \\n So again, here's our first column here,  \\n and let's throw in this pipe.  \\n What is this pipe doing?  \\n It's saying, with the data frame we see on the screen,  \\n we are going to do an index operation  \\n with this result right here.  \\n So we saw something similar to this before,  \\n we said any along columns is equal to axis.  \\n What that's going to give us is a Boolean array.  \\n It's going to have the same index as the data frame  \\n but it's going to have true false values.  \\n The true values will be where it's greater than 2023.  \\n Let's run that.  \\n And we only see that one column,  \\n that index 2, 2, 6, O, so there aren't any other values  \\n for year sold that are greater than 2023.  \\n What about the other columns?  \\n There are some columns that have year in them.  \\n So this would be my process to filter those out.  \\n I'd probably do something like that,  \\n where I'd say, okay let's take our data frame,  \\n rename our columns that are yr and make those year  \\n and then with those, let's do a filter,  \\n and say, take the columns that have year in them  \\n and then with those, let's add in our pipe here  \\n and see which ones are above our limit.  \\n It looks like, in this case,  \\n it was only that one row that was fat fingered.  \\n In this next chain, I'm showing an example of clipping.  \\n So let's take garage year built.  \\n This is just the series here.  \\n And then I'm going to use the clip method from pandas  \\n to say I want to limit this and have the upper values  \\n just be the year built max.  \\n And if we do that,  \\n that gives us a new series that has been clipped.  \\n And if we look at the value counts from that,  \\n we should see that there aren't any values  \\n that are greater than 2023.  \\n Now, this is sorted by the frequency.  \\n We could come in here and say sort index,  \\n and that looks like the highest value in  \\n the garage year built is 2010.  \\n At this point,  \\n I might want to update my data frame with this information.  \\n Now remember, in our previous lesson,  \\n we looked at replacing strings  \\n that had values that were missing  \\n and converting those to categoricals.  \\n So I'm going to leverage that.  \\n I'm going to make a chain here,  \\n and, I'm going to put that operation  \\n to clean up my strings into my chain  \\n and then I'm going to follow that with this other operation  \\n to clean up the garage year built.  \\n Now let's examine this line right here a little bit.  \\n What's going on with this line?  \\n There's quite a bit of syntax here.  \\n I am using what's called dictionary unpacking.  \\n I have a dictionary literal.  \\n and here's my dictionary.  \\n Why am I using a dictionary?  \\n Why don't I just say garage year built  \\n is equal to the current value of garage year built  \\n that's clipped.  \\n I'm using a dictionary because there are spaces in here.  \\n Because there are spaces,  \\n I can't say garage space year built is equal to that,  \\n so I can't use a parameter to do that,  \\n however, I can stick this into a dictionary  \\n and then use dictionary unpacking and that will update  \\n the column with a non-valid attribute name  \\n using pandas.  \\n And then maybe let's just look at our D types  \\n value counts after doing that.  \\n Looks like there are 39 int columns, or integer columns.  \\n And then it looks like there are categorical columns  \\n but, it doesn't sum these up.  \\n Why doesn't it sum these up?  \\n Because presumably, there are four categorical columns  \\n that have the same entries in them.  \\n There are three that have the same entries  \\n and then all of these other ones down here are one-offs.  \\n In this lesson, we looked at examining our numeric columns.  \\n I showed you how to do summary statistics on them.  \\n I showed you how to look for missing values  \\n and I showed you an example of when I looked  \\n at those summary statistics,  \\n I found something off and then I examined it  \\n a little bit more and then I used that to actually feed back  \\n into a chain to clean up the data.  \\n This is a process that you're going to want to go through  \\n when you load your data.  \\n You want to make sure that your data is clean,  \\n so that when you start doing analysis of it,  \\n the analysis makes sense.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4509082\",\"duration\":350,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Shrinking numbers\",\"fileName\":\"4433355_en_US_01_04_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"With the skill of cleaning integer data, you can ensure that the data is accurate and consistent, which is critical for making meaningful analyses and predictions. Also, learn how to lower the memory requirements if possible.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14071253,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson, we're going to look  \\n at changing the size of our numbers to save memory.  \\n One thing is I will do is create a function  \\n that will automatically convert integer values  \\n to smaller types.  \\n And then I'm going to show  \\n how to apply this function to our dataframe.  \\n You can actually take this function  \\n and apply it to your data as well.  \\n We're also going to make another function.  \\n I'm going to call that clean_housing that combine  \\n some of the steps that we've been looking at  \\n to clean up our data.  \\n This is something that I recommend you do as well  \\n as you're going through your data, cleaning it up,  \\n throw that into a chain  \\n and then take that chain and put it into a function.  \\n This is going to be a practice that, if you use it,  \\n will make your life a lot easier.  \\n We're going to continue where we left off.  \\n In our last lesson, we saw this assign method  \\n where we converted strings into categoricals  \\n and we also cleaned up our Garage Yr Blt.  \\n Here's our summary statistics of that.  \\n Looks like we haven't lost any fidelity by doing that.  \\n We should be good.  \\n We've actually cleaned up that Garage Yr Blt.  \\n Now, remember I said  \\n that I like to go through the maximum values here,  \\n and for example, the Overall Qual column.  \\n You can see that right here has minimum values of one  \\n and maximum values of 10.  \\n Remember that Pandas is using a 64-bit integer  \\n to store that information which is actually a waste.  \\n So we could use a different data type  \\n and save memory without losing any information.  \\n Let me show you how we might want to do that.  \\n In this cell, I'm going to loop through NumPy,  \\n and NumPy has a function in it called, iinfo, integer info.  \\n I'm just going to loop through some NumPy types.  \\n Uint8 stands for an 8-bit unsigned integer.  \\n So if we say np.iinfo(uint8), it tells us  \\n that the minimum value for that is zero  \\n and the maximum value is 255.  \\n This is a type that we could use for that overall quality  \\n because zero and 10 certainly lie within this range.  \\n Now, sometimes you have positive numbers  \\n that go a little bit bigger.  \\n You might need to use 16-bit or 32-bit  \\n before you go up to 64.  \\n One thing to be aware of, these are NumPy types.  \\n These are the types that were used in Pandas 1.  \\n In Pandas 2, you can optionally use that PyArrow backend.  \\n However, these type ranges actually work  \\n for the PyArrow types as well.  \\n So in this cell, I've created a function.  \\n Again, you can use this function  \\n and leverage it for PyArrow-backed dataframes.  \\n What it's going to do is it's going to accept a dataframe  \\n as input, and then it's making a mapping dictionary.  \\n It's going to loop over all of the columns  \\n that are PyArrow 64-bit integers.  \\n And then it's going to look at the minimum  \\n and maximum values for those.  \\n And if the minimum is less than zero,  \\n it's going to continue.  \\n This is only dealing with unsigned integers.  \\n Otherwise, it's going to look at the maximum values  \\n and determine the appropriate type for that.  \\n Once you have this function, all you have to do is use it  \\n in combination with the pipe method.  \\n So if you're not familiar with that pipe method  \\n in Pandas, this is a super handy method,  \\n it allows you to do whatever you want to a dataframe.  \\n All you have to do is pass in a function into it.  \\n In this case, we're passing in the shrink_ints function.  \\n Your function should accept a dataframe  \\n if you're using pipe with a dataframe.  \\n If you're using pipe with a series,  \\n a series also has a pipe method,  \\n your function needs to accept a series.  \\n And then the function can return whatever it wants.  \\n In this case, we're actually returning a dataframe  \\n using that mapping to change the types there.  \\n And with the output of that, we can actually run  \\n a describe on that after we've done that.  \\n So let's run this and test this out.  \\n You'll note that I've still included  \\n what we've done previously here.  \\n So I want to impress upon you  \\n that when you see a chain that's a little bit longer,  \\n I don't create these chains from scratch.  \\n I am building them up as I'm going through my process.  \\n And so let's look at the output from that.  \\n The output looks good.  \\n Looks like we didn't lose any information by doing that.  \\n In this next cell, let's see how much memory this is using.  \\n And it looks like this is using 360 kilobytes of memory.  \\n Let's compare that with our default amount of memory.  \\n So shrinking our integers in combination  \\n with changing our types to categoricals has saved us  \\n five times the amount of memory by doing that.  \\n The next thing I would want to do after doing this  \\n is convert this into a function.  \\n I'm keeping my shrink_ints function that I defined above,  \\n but I'm also making a function called clean_housing.  \\n This is my chain.  \\n I'm just moving this chain into a function.  \\n Let's run the function and make sure that it works.  \\n I'm just going to inspect the dtypes after doing that.  \\n It looks like this is working successfully.  \\n In this lesson, I presented a function that you can use  \\n to shrink PyArrow types without losing data.  \\n We also looked at some of the benefits of that  \\n such as memory usage going down drastically by doing this.  \\n This is one of the ways that you can load  \\n much larger data sets and analyze them  \\n by using the correct types.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4503101\",\"duration\":97,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Clean Ames\",\"fileName\":\"4433355_en_US_01_05_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3154431,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] We're at our first challenge.  \\n These challenges are a great way for you to practice  \\n and I highly encourage you, in addition to following  \\n along with me in the code space, to try this challenge out.  \\n The science tells us that if you do the challenge,  \\n you're going to use a different portion of your brain  \\n and you will actually remember the content better  \\n if you do the challenge  \\n in addition to following along with the content.  \\n So I highly recommend that you do this challenge  \\n before proceeding to the next video.  \\n Let me explain what the challenge looks like.  \\n I want you to create a cell containing all  \\n of the imports necessary  \\n to run this notebook up to the current state,  \\n and then I want you to create a cell with the clean housing  \\n and the shrink functions that I showed you up above.  \\n I want you to also to add code to load the raw data  \\n and to create a housing variable from calling clean housing.  \\n Then I want you to take these cells that you just created  \\n and move them to the top of the notebook,  \\n restart your notebook, and make sure that these cells work.  \\n What this is going to do is give you a process  \\n that you can use in your own notebooks.  \\n I recommend following this process,  \\n once you've made a chain that is useful,  \\n put it in a function  \\n and then move that to the top of your notebook.  \\n This will allow you to come back  \\n to your notebook and quickly pick up where you left off.  \\n You don't have to scroll through a bunch  \\n of cells to figure out where you were.  \\n You can just have that at the very top,  \\n and then you can just keep working on your data analysis.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4504092\",\"duration\":263,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Clean Ames\",\"fileName\":\"4433355_en_US_01_06_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14240562,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] In this lesson, I'm going to show you  \\n how to do the first challenge.  \\n So here is my cell.  \\n I'm going to get my imports.  \\n My imports were up at the top already,  \\n so these are my imports right here.  \\n We used NumPy to inspect the types  \\n but we didn't really use it in processing our data  \\n but I will keep the numpy import there as well.  \\n That will allow me, if I'm using this notebook,  \\n to not have to worry about importing NumPy.  \\n Okay, the next thing that it wants me to do  \\n is use the clean_housing and shrink_ints function.  \\n So I'll just come up here and I'm going to copy these  \\n and we'll paste those in here as well.  \\n I'll just put them in the same cell.  \\n I think that's fine.  \\n And then the next thing it says is add code  \\n to load the raw data  \\n and create a housing variable by calling clean_housing.  \\n So I'm going to do that,  \\n and that's this code right here.  \\n And I'll put that at the bottom here.  \\n Maybe I'll just call this raw.  \\n Let me explain why I like to use the raw data  \\n and make a chain.  \\n Invariably, what I've found in my experience working  \\n and consulting, it's that when I make a report  \\n or I make a model, someone asks, \\\"What's going on?  \\n Why is it doing this?\\\"  \\n And if I have the raw data and a chain,  \\n it's really easy to trace back through that  \\n and understand what's going on.  \\n So I'm going to keep that raw data there.  \\n And this exercise asked me  \\n to create a variable called housing.  \\n So I'm going to make a variable called housing  \\n and say that is equal to clean_housing  \\n and passing in raw into it.  \\n At this point, I will have the original data  \\n and my cleaned up data if I need to compare those two.  \\n The next thing it asks me to do is move this cell  \\n to the top of the notebook.  \\n So I'm just going to click on the side here.  \\n I'm going to keep this cell here, but I'm going to also click  \\n on the side so I'm not in edit mode in my notebook  \\n and hit C to copy this.  \\n And then I'm going to scroll up  \\n to the very top of the notebook here.  \\n I'll actually put it below this heading section down here  \\n and hit V to paste it right here.  \\n So the next time I came to this notebook,  \\n I wouldn't have to scroll through all  \\n of those cells to find out what's going on here.  \\n It's just for me right at the top,  \\n the code to get my data so I can be off and running.  \\n And finally, it says restart the notebook  \\n and make sure that those cells work.  \\n You'll notice that as I've been going through my notebook,  \\n I've been trying out my code, commenting it out,  \\n going through it, making sure that it works  \\n and you will want to do that as you're using your code.  \\n I'm going to hit this restart button to restart my kernel.  \\n It's going to pop up this warning.  \\n I'll hit restart.  \\n Note that when you do restart,  \\n you will lose all variables that you've created.  \\n I'm okay with that in this case.  \\n And then I'm going to run the cell  \\n and make sure that it works.  \\n Okay, it did not complain.  \\n I'll just make a new cell below this and look at housing.  \\n And it looks like it worked.  \\n In this lesson, I've given you a nice hint  \\n for how to make your notebooks a lot easier to use.  \\n And that is make a chain that cleans up your code,  \\n stick that into a function, and then pull that up  \\n to the top of your notebook  \\n so that when you come back to your notebook,  \\n you can easily start going and not have to scroll through  \\n and figure out what's going on with your code.  \\n \\n\\n\"}],\"name\":\"1. Loading and Cleaning Data\",\"size\":119595368,\"urn\":\"urn:li:learningContentChapter:4503102\"},{\"duration\":3034,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4504091\",\"duration\":242,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Categorical exploration\",\"fileName\":\"4433355_en_US_02_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"With the skill of calculating and interpreting summary statistics, you can gain valuable insights into the central tendency, variability, and distribution of your data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7604335,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Narrator] In this lesson, we're going to look at exploring  \\n our categories a little bit more.  \\n We're going to show the unique values of that  \\n but we're also going to visualize what's going on with that.  \\n Remember, visualization is a powerful tool  \\n you can use to tell a story very quickly  \\n and we're going to show how to do that for categoricals.  \\n I've got my cell that loads my data.  \\n So I've got the raw data and I've got the housing data.  \\n Remember, we can use describe to get summary statistics.  \\n By default, this is going to give us  \\n numerical summary statistics.  \\n It's not going to include the categoricals,  \\n or strings by default.  \\n Let's pull off a categorical.  \\n I'm going to use the zoning value.  \\n One of the things that's interesting about this  \\n is if you look at the dtype at the bottom here,  \\n it says that it is category  \\n and then here are the unique types for that category.  \\n Generally, when I have a category or a string column,  \\n I like to use the value counts method.  \\n We'll tack that on here.  \\n And this gives us a series.  \\n Pandas uses series and data frames all over the place  \\n so it's good to get comfortable with those  \\n and also understand that these operations that we're doing  \\n with series and data frames are like tools  \\n in your tool belt and once you understand  \\n these basic operations, you can start leveraging them,  \\n combining them with other ones.  \\n So one of the things I might want to do with this  \\n is visualize what's going on here.  \\n This is a nice summary.  \\n We can see that the RL Zoning has 2,273 entries  \\n but maybe want to visualize that  \\n and let's see how we can do that.  \\n The common visualization that we would use for this  \\n is a bar plot.  \\n Now look how easy it is to go from this value count  \\n to a plot.  \\n All I need to do is tack on the next operation in our chain  \\n which is .plot.bar.  \\n Let me explain how bar plots work in Pandas.  \\n Once you understand how they work,  \\n they're very easy to leverage.  \\n What a bar plot is going to do is it's going to take  \\n the index and put the index in the X axis.  \\n Note that the index here is not numeric.  \\n It is the unique entries from the MS Zoning column.  \\n That's perfectly fine.  \\n In fact, Pandas converts any index when you do a bar plot  \\n into a categorical or a string in the plot,  \\n which sometimes comes to bite us  \\n if you have dates in the index and you do a bar plot  \\n of that, it actually converts those dates into categoricals.  \\n For this case, it's fine.  \\n So the index is going to go into the X axis,  \\n and then in this case, we only have a single series.  \\n If you have a data frame, each column will be a bar  \\n and those bars will be located above the index.  \\n Let's run this and see what happens.  \\n This allows us to clearly see the RL  \\n is the most common entry for MS zoning, followed by RM  \\n and telling off very quickly.  \\n One of the things I don't like about this bar plot  \\n is that the labels are rotated 90 degrees.  \\n In fact, a common thing to do with a categorical count plot  \\n like this is to convert it to a horizontal bar plot,  \\n and this again, is easy in Pandas.  \\n All we need to do is change the bar  \\n to barh, standing for bar horizontal.  \\n And we get something that looks like this.  \\n Again, telling the same story  \\n but a little bit easier for us to read.  \\n We don't have to tilt our head as much.  \\n In this lesson,  \\n I showed you how to summarize categorical columns.  \\n Again, value counts is going to be your friend  \\n and then once you know how bar plots work,  \\n it's easy to go from value counts to making a bar plot.  \\n And then generally you're going to want to make  \\n a horizontal bar plot.  \\n That is easy as well by using .barh instead of .bar.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4503100\",\"duration\":262,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Histograms and distributions\",\"fileName\":\"4433355_en_US_02_02_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"The goal of this video is to learn how to create and interpret histograms, which are graphical representations of the distribution of numerical data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7903762,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson, we're going to look  \\n at histograms and distributions for numerical columns.  \\n Again, we can start off  \\n with the describe method to get summary statistics  \\n but oftentimes, we want to visualize this as well.  \\n I'll show you some ways to visualize these.  \\n Let's take the SalePrice column.  \\n Note that we can run describe on a single column  \\n or a series in addition to a data frame.  \\n When we run describe on a series,  \\n we get back a series.  \\n Again, in the index of this series,  \\n we have the descriptions  \\n of the statistical summaries that are coming back  \\n and the values,  \\n we have the corresponding values for each statistic.  \\n I'm just going to quickly go through these.  \\n Look at the minimum value.  \\n It looks like there's a house that sold for $12,000.  \\n That seems pretty cheap.  \\n I might want to check out that row  \\n or check out those low values.  \\n Also might want to go on that high side.  \\n We have a house that sold for 755,000.  \\n Make sure that that looks about right.  \\n Also, I could look at the mean and the median.  \\n Those look pretty close to each other and remember,  \\n our count is not the count of rows  \\n but the count of non-missing rows, but it looks  \\n like this is, in this case, the same as the number of rows.  \\n We don't have any SalePrice values that are missing.  \\n One of the ways to visualize a numeric column  \\n is to do a histogram, and again,  \\n this is very easy in pandas.  \\n We just say .hist, and here's the histogram.  \\n You can see that this is somewhat skewed.  \\n It rises quickly to some value  \\n around $150,000 and then tails off  \\n and there's a long tail going out to around $700,000.  \\n We know that it goes up to $700,000 at least.  \\n If it didn't go up to that value,  \\n it wouldn't be on this chart.  \\n However, there aren't very many entries at that end.  \\n Now, you need to be careful with histograms  \\n because you can tell different stories with them.  \\n By default, pandas is going to give you 10 bins.  \\n People often ask, \\\"Is 10 bins the correct number of bins?\\\"  \\n I would say the answer is it depends, but it might not be.  \\n This is one of those things that people  \\n would write dissertations about the correct number of bins.  \\n I'm going to show you how you can change this.  \\n We can say bins is equal to 30,  \\n and we can get a little bit finer granularity in there.  \\n We could also come down here and say bins  \\n is equal to three.  \\n This probably isn't a story that we want to tell  \\n using bins is equal to three.  \\n From inspecting this data, it looks like somewhere  \\n around 150 is the most common value or the mode.  \\n When we do describe, we don't get a mode reported on there,  \\n but we did get a median and a mean,  \\n which were both around that value as well.  \\n You can see that for whatever reason,  \\n there's a little bump here around 550  \\n and there's another bump here around 600.  \\n Those are kind of interesting to me.  \\n They probably indicate that these houses sold  \\n at complete numbers like 600,000  \\n or 550 rather than some value in between there.  \\n It's interesting also that we're not seeing that happen  \\n at other values where we're not seeing like big spikes  \\n at 300,000 or 200,000 or 100,000, and it also doesn't look  \\n like we're seeing bumps at the 50,000 level as well.  \\n We can bump up the number of bins and look  \\n at the finer granularity.  \\n That might yield useful information.  \\n It might just be noise.  \\n In this lesson, we took the SalePrice column  \\n and we did summary statistics on that.  \\n We also showed how to look at a very common visualization  \\n of that, which is the histogram.  \\n We showed how to change the number  \\n of bins in a histogram to tell a different story as well.  \\n Leveraging a histogram is a great way to get a feel  \\n for how the data is distributed.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4502195\",\"duration\":412,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Outliers and Z-scores\",\"fileName\":\"4433355_en_US_02_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Discover how you can objectively identify data points that are significantly different from the majority of the data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":18691501,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson, we are going to look  \\n at how we might find outliers  \\n or values that are at the extremes  \\n and I'll show two different ways to do that.  \\n I'll show one using our classic Z score and another one  \\n by using the inner quartile range, and we'll look  \\n at how we can use the pandas functionality to do that  \\n and make functions that you can use  \\n and leverage in your own code if you need  \\n to calculate values that tend to be on the extremes.  \\n So here is a function right here, calc Z  \\n and you'll notice that the input to calc Z,  \\n the first parameter is a data frame  \\n so a function that has the first parameter is a data frame  \\n is a candidate to use with the pipe method.  \\n This lets us leverage this in a chain.  \\n Let's run this and see what happens when we do this  \\n and I'll walk through the code.  \\n Okay, so what this is returning is a series  \\n and this is the Z-score of the sell price column.  \\n Let's just look at what these lines are doing.  \\n In this case, you'll note, even though I like chaining,  \\n I'm not using chaining all over the place.  \\n This is a case where I'm not using chaining,  \\n I'm calculating the mean  \\n of this column that I'm passing in here.  \\n So the DF that is coming in is whatever is passed  \\n in from the chain.  \\n In this case, this is the original housing,  \\n just coming in as the DF here  \\n and sale price is coming in as the column value.  \\n So we first calculate the mean of sale price,  \\n then we calculate the standard deviation of sale price  \\n and then the Z score is calculated  \\n by taking that column, subtracting the mean,  \\n and dividing by the standard deviation.  \\n We can leverage pandas to make this a vectorized operation  \\n and to make it work relatively easily.  \\n The Z-score gives us a quantification  \\n how many standard deviations we are away from the mean  \\n and generally an outlier is a value  \\n that has an absolute magnitude above three  \\n for data that has a normal distribution.  \\n One of the nice things about making this  \\n as a function is we can also use it in assign.  \\n Let me show you how this is going to work here.  \\n I've got this little chain.  \\n I'm going to comment out this  \\n and if you look at the column at the end here,  \\n you can see that there is now a Z-score column.  \\n This Z-score column is the Z-score of the sale price.  \\n If I want to find outliers,  \\n I can combine this with the query method.  \\n So now that I have a Z-score column,  \\n I can use this query saying,  \\n take the absolute value of the Z-score column  \\n and find out where that is greater than  \\n or equal to three.  \\n You can see the values that we're looking  \\n at here on the screen, none of those are equal to three.  \\n Let's run this and see what happens.  \\n You can see that this row 15 and this row 44,  \\n if we were to scroll over, presumably would have a Z score  \\n with an absolute magnitude greater  \\n than or equal to three, and indeed they do.  \\n Let's look at these values.  \\n These values all look like high-end houses  \\n or houses that are very expensive.  \\n These values all look like they're greater than three.  \\n It's possible that there are values less than three.  \\n Let's just quickly check and see if we can get those.  \\n I'm going to copy this  \\n and I'm going to just change it a little bit and say,  \\n where are we less than or equal to negative three?  \\n So it looks like there aren't any values  \\n that are outliers on the low end.  \\n Again, you need to remember that this is assuming  \\n that we have a normal distribution of the sale price.  \\n I'll show later on in the lesson how to calculate  \\n if that distribution is normal.  \\n Another mechanism for determining outliers is  \\n to calculate the inner quartile range  \\n and then values that are outside  \\n of that are classified as outliers by some statisticians.  \\n Here I'm making a function called calc IQR outlier.  \\n Again, this takes a data frame and a column.  \\n In this case, what it's doing is it's pulling  \\n off the series from that column  \\n and then we're saying calculate the 75th percent quantile  \\n and subtract the 25th percent quantile from that.  \\n That will give us a series of the inner quartile range.  \\n We can also calculate the median  \\n which is the 50th percentile, and the definition  \\n for an outlier using the inner quartile range is  \\n to take the median  \\n and subtract three times the inner quartile  \\n or the median and add three times that.  \\n So I'm making a variable called small mask and large mask  \\n and then I'm using the pipe to combine those together.  \\n What this returns is a boolean array,  \\n let me just show you that.  \\n So boolean array is a series that has true false values.  \\n Where these are true means that either this is true,  \\n it's low or it's high, and when we throw that  \\n into our index operation here,  \\n this gives us back the rows where this is true.  \\n And you can see  \\n by this calculation that 15 and 44 are both outliers.  \\n Let's show how to throw this into a column.  \\n In this example,  \\n I'm going to make a new column called IQR outlier  \\n and that is going to be the result of calling our function,  \\n passing in our data frame  \\n and indicating the sell price column  \\n as the column that we want to figure  \\n out whether those values are outliers or not.  \\n Now let's look at that result here.  \\n I'll scroll down and scroll over.  \\n You see this is true false value,  \\n so this is our boolean array.  \\n What I'm doing in this next part  \\n of the chain is just saying, filter out the roads  \\n where this is true in the boolean array  \\n and all of these IQR outliers should be true in this case.  \\n In this lesson, we made functions that allow us  \\n to calculate the outlier in two different ways.  \\n One by using the Z-score and another  \\n by calculating the inner quartile range  \\n and using that to determine whether values are outliers.  \\n These are functions that you can leverage in your own code.  \\n Feel free to take them  \\n and use them for outlier calculations.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4503099\",\"duration\":422,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Correlations\",\"fileName\":\"4433355_en_US_02_04_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to identify relationships between variables, determine the strength and direction of those relationships, and make informed decisions based on the data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":24607809,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson,  \\n we're going to look at calculating correlations.  \\n We'll look at the Pearson and Spearman correlation.  \\n I'll also show you how to make a heat map  \\n and give you some tips for formatting that,  \\n so you can quickly see where your correlations are.  \\n This is one  \\n of those calculations that's really easy to do in Pandas.  \\n All you have to do is say .corr.  \\n Now, as I said that it was easy to do,  \\n it looked like it failed.  \\n Turns out that in this case, this is a Pandas 2 update.  \\n Pandas 2 is a little bit more strict  \\n about running operations on columns that aren't numerics.  \\n Let's scroll down to the bottom  \\n and you can see that the error is,  \\n that it says it \\\"could not convert string to float: RL.\\\"  \\n That's a little hard to understand  \\n but basically what this is saying is  \\n that we passed in string columns  \\n and it didn't like doing the calculation on those.  \\n To get around this,  \\n what we're going to do is specify this parameter,  \\n numeric_only=True.  \\n What this returns is a Dataframe.  \\n In the index is all the numeric columns  \\n and in the columns is also all of the numeric columns.  \\n So we see the correlation between each numeric column  \\n and all the other numeric columns.  \\n This is using the Pearson correlation coefficient.  \\n That is a value between -1 and 1.  \\n The closer those values are to 1,  \\n the stronger the positive correlation,  \\n meaning as one value goes up, the other value goes up.  \\n The closer that value is to -1  \\n the more negative the correlation is  \\n meaning as one value goes up, the other value goes down.  \\n If those values are around zero,  \\n that means that there is no correlation.  \\n As one value goes up, the other value might go up or down.  \\n They're not related.  \\n This is another one  \\n of those things that humans aren't optimized for looking at.  \\n I could give you this big table of data and ask you  \\n to find the biggest correlation and the smallest correlation  \\n and it would take a bit of time for you to do that.  \\n I'm going to give you some hints on how to get around that.  \\n Also, note that this is  \\n calculating the Pearson correlation coefficient,  \\n which assumes that there's a linear relationship  \\n between two values,  \\n ie. if you were to do a scatterplot of this,  \\n you should see a line.  \\n It's not always the case that there's linear relationships.  \\n Sometimes you have non-linear relationships.  \\n And you might even have a monotonic relationship  \\n meaning when one value goes up, the other value might go up,  \\n but it might be in a shape that is not a line.  \\n It might be like a logistic curve  \\n where it looks like a stretched out S.  \\n In that case,  \\n we might want to use the Spearman correlation coefficient.  \\n To do that, we specify the parameter, method='spearman'.  \\n And again, I'm going to say, numeric_only=True.  \\n And I'm also going to tack on this background gradient.  \\n Remember, we want to be able  \\n to see what's going on here easily,  \\n and one of the ways we can do that  \\n is leveraging visualization and color.  \\n So by default, when you do the background gradient,  \\n it puts on this blue background gradient  \\n that immediately draws our eyes to the diagonal.  \\n And you can see  \\n that along the diagonal is a correlation of one.  \\n If you look at the diagonal, those are actually  \\n the correlations of one value versus the same value,  \\n meaning that as that value goes up, it goes up.  \\n Which in itself is not a particularly interesting insight  \\n and actually we kind of want to ignore the diagonal values  \\n when we're looking at this,  \\n but you could look off diagonal  \\n and see where the largest correlation is.  \\n Remember, this is a Spearman correlation coefficient  \\n so it's not necessarily  \\n there would be a straight line between those,  \\n but as one value's going up the other value's going up  \\n this makes it relatively easy to see  \\n that, for example, right here or right here,  \\n these are actually the same correlation.  \\n Year remodel or added to against year built.  \\n So as the year built goes up,  \\n also the year that it was remodeled goes up,  \\n and we can scroll through here  \\n to see if we see any others pop up.  \\n Here's one that has a particularly high correlation,  \\n garage year built,  \\n and year built.  \\n The default coloring used by background gradient,  \\n this going from white to blue,  \\n is okay for looking at positive correlations,  \\n but it's actually not great  \\n for looking at negative correlations.  \\n Why is that?  \\n This color map starts at a white value  \\n and it's going to pin the white value, so to speak,  \\n at the smallest negative value.  \\n It looks like, for example, here  \\n it looks like pretty white and it's -0.4.  \\n We actually wouldn't want the smallest value to be -0.4.  \\n We would want it to be -1.  \\n And we actually don't want to use this color map  \\n that goes from white to blue.  \\n We'd want to use a different color map.  \\n So let's see if we can change this  \\n and use a better color map.  \\n I'm going to use this RdBu color map here.  \\n This is a red to blue color map.  \\n It's actually what's called a diverging color map.  \\n It goes from red through white to blue.  \\n And now we can look  \\n at the bluest things and the reddest things.  \\n The bluest things are the most positive  \\n and the reddest are the most negative.  \\n Again, we kind of want to ignore the diagonal.  \\n The diagonal by definition is going to be 1.  \\n So you'd want to look at the bluest off diagonal values  \\n and the reddest off diagonal values.  \\n Again, if you look at these,  \\n the reddest values look to be around -0.4,  \\n which is not what we want.  \\n So I'm going to give you one more hint.  \\n When you use a background gradient  \\n with a correlation heat map,  \\n you're going to want to pin the negative value.  \\n Let me show you how to do that.  \\n You specify vmin as -1.  \\n We can also put in vmax as 1,  \\n although by definition the diagonal will have a value of 1.  \\n And this tells a completely different story.  \\n If we look at this, you can see  \\n that those values around zero are indeed white  \\n which is what we want to see.  \\n Those are the values that don't have a correlation.  \\n We kind of want to ignore those.  \\n In the other examples, those were distracting us.  \\n And we can look  \\n for the reddest values now and the bluest values.  \\n We can still see that that -0.42  \\n looks like it is probably one of the reddest values.  \\n Let's just scroll through this and see.  \\n Here's another one that looks like it's pretty red,  \\n and here's one that is actually very red.  \\n This is an example  \\n of leveraging the power of color and visualization  \\n to help us read a table more effectively.  \\n In this lesson, we looked at correlations.  \\n We saw how we can easily calculate these in Pandas,  \\n and we saw how we can leverage color  \\n to help us understand these correlations  \\n and pull out the interesting values rapidly.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4502194\",\"duration\":457,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Scatter plots\",\"fileName\":\"4433355_en_US_02_05_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to create and interpret scatter plots, which are graphical representations of the relationship between two variables.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":15216488,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson,  \\n we're going to look at scatter plots.  \\n Scatter plots are an excellent way  \\n to take two numeric columns  \\n and look at the relationship between those.  \\n I'm also going to give you some tips  \\n on how to make more effective scatter plots.  \\n Again, I think the key to understanding scatter plots  \\n is understanding the interface.  \\n The interface here is a little bit different  \\n than what we saw with bar plots or histograms.  \\n In the case of scatter plots, we are going to call .plot.  \\n And .plot has various methods for plotting on it.  \\n One of them is scatter,  \\n and the parameters that we specify for scatter  \\n include the column that we want in the X-axis  \\n and the column that we want in the Y-axis.  \\n Let's run that, and here's our plot.  \\n So this is the year built versus the overall condition.  \\n We want to see what's going on between these two.  \\n I'm going to calculate the correlation between those  \\n so you get a feel for what this looks like.  \\n We'll take housing.  \\n And I'm going to say pull off year built,  \\n and do a correlation with that with housing,  \\n and then look at overall condition.  \\n So that has a -0.36 correlation, meaning  \\n as one value goes up, the other value goes down slightly.  \\n We can also change this  \\n to a Spearman correlation coefficient  \\n and see if that changes that at all,  \\n and it actually makes it slightly more negative.  \\n So just from looking at this, I wouldn't necessarily say  \\n that as one value goes up, the other value goes down.  \\n So let's see if we can tease apart this plot a little bit.  \\n Let me tell you what I'm seeing when I look at this.  \\n I'm seeing a bunch of layers.  \\n What those layers indicate to me is  \\n that our values have been encoded at a certain granularity.  \\n In this case, this is the overall condition  \\n and these are whole numbers between one and nine.  \\n You can kind of consider these to be categorical in nature  \\n though they're encoded as numbers.  \\n I'm also seeing a bunch of dark values,  \\n and when I see dark values in scatter plots,  \\n that gives me pause.  \\n I'll generally want to use transparency  \\n to see what's going on there.  \\n So let's apply this alpha transparency.  \\n I'm going to pass in transparency of 0.1,  \\n and this tells a different story, I believe,  \\n than what we saw above.  \\n This is telling me that we have a lot of values at five  \\n and some values at six and seven.  \\n We could also look at a histogram of overall condition  \\n to see where those distributions lie,  \\n but we're kind of seeing that here.  \\n Again, compare that to what we saw here.  \\n I would not say that we had the most values  \\n at five just by looking at this,  \\n but when we change that alpha, we're seeing that.  \\n Let me give you a hint on how I like to set alpha.  \\n Let's maybe put alpha at 0.5,  \\n and when I see that there are a bunch of dark values,  \\n and there's still a bunch of dark values,  \\n I'm not seeing a transition around those.  \\n I kind of want to keep going down.  \\n So maybe I'll go down to 0.3.  \\n I can see it, five and six and seven.  \\n There are still a bunch of dark values,  \\n and let's put it down to 0.1.  \\n There I'm seeing that six and seven start to get faded out,  \\n so I think that's okay.  \\n I'm also seeing  \\n So I think this is an okay value for alpha.  \\n Now I still am seeing those flat layers.  \\n How would I deal with that?  \\n I like to use jittering to deal with that.  \\n What is jittering?  \\n Jittering is shifting those values up by a random amount  \\n and I'm going to give you the code to do that right here.  \\n So this example, I'm taking my housing Dataframe  \\n and I'm overriding the overall condition column  \\n by taking the value of the overall condition  \\n and using NumPy to randomly add a value  \\n between zero and 0.8.  \\n And then I'm subtracting 0.4 to that, so I'm centering that.  \\n So this should give us a value  \\n between -0.4 and 0.4 on the positive side, but random.  \\n Now I'm going to plot this new overall condition.  \\n And this is what that looks like.  \\n You might ask why did I choose the value of 0.8 and 0.4?  \\n Well, if you look at the overall condition  \\n the overall condition is a whole numeric value,  \\n and I kind of want to see some segmentation  \\n between the five values and the six values.  \\n So if I would put that value instead of 0.8,  \\n put it up to one, then it would stretch all the way  \\n between one value and the next.  \\n So that leaves a little bit of breathing room between those,  \\n and then I subtract four so that it's centered at the value.  \\n Now, if you look at this,  \\n I think this tells a completely different story  \\n than what we're seeing above.  \\n This is telling me that we have a lot of values  \\n around the year 2000 in the condition five.  \\n Again, let's scroll back up to our original scatter plot  \\n and I don't know that you would've been able to tell that  \\n from looking at the original scatter plot,  \\n but by using some of these tricks  \\n such as adjusting the alpha and jittering,  \\n we are able to have some pretty big insight  \\n into what's going on here.  \\n This also helps us understand  \\n that correlation number that we looked at before.  \\n It looks like if you go from the left to the right  \\n the values are going slightly down, and that's easier to see  \\n after we've applied some of these tricks to our plot.  \\n Again, I'm going to go back  \\n to these traditional software engineering things that I do.  \\n If I want to use jitter in other places,  \\n I'm going to make it into a function  \\n so I can take advantage of it.  \\n In this case,  \\n I'm going to pass in a data frame as the first parameter,  \\n a column that I want to jitter, and the amount.  \\n I'm going to default the amount to 0.5,  \\n but we can bump that up to 0.8,  \\n and then I'll just test that function out here.  \\n Instead of using my code that I've commented out right here,  \\n I'm going to use my jitter function,  \\n and let's see if that works.  \\n Indeed, it looks like it does.  \\n One more way to visualize this  \\n that you get for free in Pandas is to use a hex bin plot.  \\n You can see that this plot's little six-sided figures  \\n and then colors those based  \\n on the number of values that fall into those.  \\n In this case, you can see around the year 2000  \\n the overall condition of five has a concentration of values.  \\n I personally don't like this plot as much  \\n as the scatter plot that I did above,  \\n but this gives you similar insights with very little code.  \\n In this lesson, we looked at creating scatter plots.  \\n Remember, scatter plots are a mechanism  \\n to look at the relationship between two numeric columns.  \\n We also looked at the correlation  \\n which is a quantification of that relationship  \\n and saw that we can explore that quantification  \\n by using a scatter plot,  \\n and adding some tricks on top of those.  \\n So the tricks that I like to use  \\n if I'm seeing that there's just a lot of dark spots,  \\n I'm going to lower the alpha  \\n until I start to see a transition between those spots.  \\n And then if I see things that are stacked  \\n up on top of each other in either columns or rows,  \\n I'm going to use jittering to spread those apart.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4506093\",\"duration\":642,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Visualizing categorical and numerical values\",\"fileName\":\"4433355_en_US_02_06_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Learn how to calculate and interpret numerical summaries of data based on categorical variables, such as means or medians of groups.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":24933557,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In our previous lesson, we looked  \\n at comparing two numeric columns to each other,  \\n what happens if you have a numeric column  \\n and a categorical column?  \\n Let's look at how we can visualize and understand those.  \\n We're going to take two columns.  \\n We're going to take year built and overall condition.  \\n Now, both of these are numeric, but I'm going to say  \\n that both of them are also somewhat categorical as well.  \\n Some columns can be both numeric and categorical.  \\n You can think of a year  \\n as a numeric sequence that increases, but you can also think  \\n of events that happened in a year that would be categorical.  \\n Similar with overall condition.  \\n Overall condition is a number that represents the condition  \\n of a house.  \\n You could also think of the condition,  \\n one, being a house that is  \\n in very poor condition and a house that has a condition  \\n of nine to be very good condition.  \\n In this example, let's assume  \\n that year built is a categorical value  \\n and overall condition is numeric.  \\n How could we look at the relationship between those two?  \\n One of the things that we can do is we can use  \\n what's called a box plot.  \\n I'm going to say in the X axis, use the year built column  \\n and in the Y axis use the overall condition column.  \\n We get something that looks like this.  \\n It's a little bit unclear what's going on here.  \\n It says overall condition in the bottom  \\n and then on the left it looks  \\n like we have the condition values from zero to nine.  \\n What is happening?  \\n This is basically taking all of the overall condition values  \\n and plotting a box plot for them.  \\n This isn't doing what we wanted to do, which was look  \\n at the conditions over the different categories of years.  \\n Let's see if we can figure that out here.  \\n I'm going to use some pandas code to manipulate my data.  \\n First of all, let's use what's called a pivot.  \\n You can see that I'm putting the year built  \\n into the column along the top, we have all the year built.  \\n I'm putting the overall condition column into the values  \\n but this is giving us as a data frame  \\n with the same number of rows as the original data  \\n but we see a bunch of missing values here.  \\n That's because this is a sparse data frame.  \\n What I want to do is I want to take those values  \\n for each year and basically push them up along the top  \\n and that's what this next line is going to do.  \\n I'm going to say, let's apply a function here.  \\n The function is going to take the values  \\n that are not missing and keep those in each column  \\n and then I'm going to reset the index and that'll gimme  \\n for every column, the values that are in there.  \\n That's what that looks like.  \\n You can see that all of the values are along the top.  \\n We still have missing values  \\n but those are just padding the bottom  \\n for the values that appear to be the longest,  \\n it looks like 2005 at least has 141 entries.  \\n In this case, when we call .plot.box,  \\n it is going to take each column  \\n and it's going to stick each column in the index  \\n and for each column it's going to take the values  \\n in that column and do a box plot.  \\n In my opinion, the key  \\n to plotting in pandas is understanding how pandas moves  \\n from a data frame into a plot and it's not always the same.  \\n Here's the output of that.  \\n I don't think this plot is particularly useful.  \\n There's too much information going on here.  \\n It's hard to even read the X axis.  \\n What would I do to this?  \\n I would limit it to a certain number of years  \\n or I would bin years maybe into decades.  \\n Here's a chain.  \\n Let's walk through this chain and see what's going on here.  \\n Here's our pivot, which looks like what we had before.  \\n I'm going to do my apply, which is going to again,  \\n move the values up to the top, and instead  \\n of having almost 3000 rows, now we have 142 rows  \\n and now I'm going to use loc to filter out rows and columns.  \\n Loc allows us to specify rows  \\n that we want to pull out and columns that we want to pull out.  \\n In this case, this colon here is the row selector.  \\n If you just put a bare colon in the row selector,  \\n it is going to take all of the rows.  \\n So then we have a comma,  \\n that comma separates the row selector  \\n from the column selector.  \\n Here is the column selector.  \\n It looks like we're just going to take the years  \\n 1900, 1920, 1940, 1960, 1980, and 2000.  \\n so at this point we can do our box plot on that  \\n and there are the box plots for those specific years.  \\n To me, it looks like this is saying the condition  \\n of the median house tends to go down over time slightly.  \\n Recall that this is only looking  \\n at the values from those specific years.  \\n Maybe we want to group by decade.  \\n Let's see how we could do that.  \\n I've got a chain that will do that.  \\n Let's walk through this chain.  \\n I've said this previously, but once you understand  \\n that pandas is basically a toolbox for manipulating data,  \\n if you can get your data into the specified forms  \\n by making these chains, that will give you super powers.  \\n Let's look at what this chain is doing.  \\n It is making a new column called decade.  \\n How are we calculating that?  \\n We're taking the year built column  \\n and we're doing an integer or a floor division by 10  \\n and then we're multiplying that by 10,  \\n so if I do a floor division,  \\n let me show you an example of that,  \\n I'm going to do 2006 and then I'm going to do double divide 10  \\n and that gives me 200.  \\n If I just do divide 10, that's going to gimme 200.6,  \\n so this is called floor division or integer division.  \\n If I did 1993, 10, that should give me 199  \\n and then I'm just multiplying  \\n so this is giving us every decade.  \\n Again, we can scroll over and just validate that that works.  \\n Can see that the decades are all multiples of 10.  \\n Now that we have that we can group by the decade  \\n and get summary statistics for each of those decades.  \\n Now that we have that, we can pivot by decade.  \\n I'm going to put the decade into the columns  \\n and we get values that look like this.  \\n This is what we had before.  \\n We're going to push those values up using the same code  \\n that we had before  \\n and now we're going to do our box plot with that,  \\n there we go  \\n and it looks like we're seeing that same thing  \\n that the condition of the house goes down over time.  \\n I'm not a subject matter expert in this dataset.  \\n This might indicate that people  \\n with old houses take good care of them.  \\n Maybe they're antiques.  \\n Maybe the new houses are not up  \\n to par compared to ones that are built in the past.  \\n It is interesting to see this.  \\n I would naively expect the opposite,  \\n that the overall condition  \\n of a newer house would be better than an older house.  \\n I've given you a bunch of tools leveraging pandas  \\n to group by category and look at numeric values.  \\n Let's see if we can use that using some other tools.  \\n I'm going to use the Seaborn library.  \\n If you're not familiar with the Seaborne library,  \\n it is a library that is built on top  \\n of mapplotlib and pandas for doing statistical plotting  \\n and we can say, let's do a box plot.  \\n Here's our data and here's the column, in the X,  \\n we want year built in Y, we want the overall condition.  \\n This allows us to get that plot that we got earlier  \\n but in one line of code.  \\n Again, is this plot useful?  \\n I don't think it's particularly useful.  \\n There's too much going on here.  \\n You can pull the documentation for this  \\n by just putting a question mark on here.  \\n There's actually a lot of documentation  \\n and we're going to use the order here  \\n to filter what is going on with the years.  \\n You can come down here and look at the output.  \\n Right now it's truncated  \\n but the order allows you to specify which things you want  \\n in the X axis.  \\n I'm going to specify a subset of those years.  \\n Let's run this again  \\n and now you can see a box plot for each of those years.  \\n Note that this is not doing any grouping.  \\n This is just pulling  \\n off the values that happened at those years.  \\n A trick that Seaborn has up its sleeve is it  \\n has other plots that you can do as well.  \\n One of those is the violin plot  \\n And this has the same interface as our box plot above.  \\n This is basically doing a kernel density estimation  \\n and flipping it on its side  \\n and putting another one that's mirrored on the other side  \\n of it, so you can see that, for example, there are a bunch  \\n of values here around six or seven for 1940.  \\n For 1900, there are a bunch of values around eight.  \\n Another plot that you might want to make is a boxen plot.  \\n This is a more modern box plot.  \\n In this case, it is showing you a big rectangle.  \\n The big rectangle is between the 25th percentile  \\n at the bottom and the 75th percentile at the top.  \\n The line in the middle is the 50th percentile or the median.  \\n This next rectangle down below here is  \\n from the 25th percentile to halfway between that and zero  \\n so this would be the 12.5 percentile  \\n and basically these diamonds are outliers, above that.  \\n Oftentimes you'll see another rectangle  \\n and another rectangle beyond that.  \\n In this case, it looks like the 12.5  \\n and in this case this is the 75th, and this is halfway  \\n between the 75th and the 100th, so that would be 87.5.  \\n Most of the values in this year, 1920,  \\n are between 12.5 and 87.5.  \\n You can see for 1900, most of the values lie  \\n between the 25th percentile and the 75th percentile.  \\n In this lesson,  \\n I gave you a bunch of tools that you can use  \\n to compare numeric values by category.  \\n Again, once you start understanding how to leverage pandas  \\n and these techniques in pandas,  \\n those are going to be nice tools  \\n in your tool belt to do some very powerful operations.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4506092\",\"duration\":362,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Comparing two categoricals\",\"fileName\":\"4433355_en_US_02_07_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13220169,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson, we are going to look  \\n at how you might take two categorical columns  \\n and compare them and understand  \\n the relationship between them.  \\n Generally we're going to want to use  \\n some sort of cross-tabulation.  \\n We can also do a visualization, a bar plot,  \\n or a stacked bar plot On top of that.  \\n Let's look at some of our dtypes here.  \\n We can see that we have a bunch of categorical values.  \\n Let's pick some of them.  \\n So I'm going to pick the Overall Quality  \\n and the Basement Condition.  \\n Generally when you compare these,  \\n you want to make what's called a cross-tabulation.  \\n For each category in one column,  \\n you want to count  \\n the number of categories from another column  \\n and you make a matrix of the result.  \\n This is how we do this in Pandas.  \\n Here's our original data.  \\n We're going to group by the two columns  \\n that we want the tabulation for.  \\n Pandas is lazy, when you do a groupby,  \\n it does not calculate anything.  \\n So let's do an aggregation to make it run a calculation.  \\n We're going to say size.  \\n Let's look at the output of this.  \\n This gives us a series  \\n and this is a series that has what's called a multi-index  \\n or a hierarchical index.  \\n In the left-hand side,  \\n we see Overall Qual and Basement Condition.  \\n Those are both in the index,  \\n so there are actually two indexes here.  \\n And then zero, zero, zero, three on the right,  \\n that's the values in the series.  \\n What I'm going to do now is a pretty powerful  \\n and sometimes confusing operation,  \\n I'm going to call unstack.  \\n What this is going to do is it's going to take  \\n that Basement Condition index.  \\n It's going to rotate it, pull it out,  \\n and put it in the columns  \\n so every value from Basement Condition  \\n will be a unique column.  \\n Let's run that and see what happens.  \\n Look at that, and here is our cross-tabulation.  \\n We have all of the Overall Quality values in the index  \\n and we have all of the Basement Conditions in the column.  \\n Because this is a common operation,  \\n Pandas actually gives this to us for free  \\n with the crosstab function.  \\n We can call crosstab  \\n and say in the index I want the Overall Quality column  \\n and in the columns I want the Basement Condition column.  \\n You can see that this is the same result  \\n that we see up above,  \\n but it's one line of code  \\n versus those four lines of code up there.  \\n I show you this because again, I think it's useful  \\n to have these powers to understand grouping,  \\n to understand size and unstack.  \\n Even though crosstab will give all of these to you for free,  \\n sometimes you do want to do these grouped by operations.  \\n Going back to what we've said previously,  \\n humans aren't great at looking at tables of data.  \\n This is a table of data.  \\n So generally if you have a table of data,  \\n you want to visualize it if you can  \\n or filter out the pieces of information  \\n that are useful in it.  \\n In this case, we're going to use style  \\n and background gradient to color it.  \\n I'm going to use a color map called Viridis  \\n which is a transitional color map.  \\n It goes from purple to yellow,  \\n passing through blue and green along the way.  \\n This tells us that we have a bunch of entries  \\n with Overall Quality of five  \\n and Basement Condition TA.  \\n So again, if we want to pull up the documentation,  \\n we can stick a question mark after background gradient  \\n and one of the things that you might want to do  \\n is figure out how it's doing this coloring here,  \\n and there is an access parameter here  \\n which determines how it calculates the color.  \\n Is it doing it by row, by column,  \\n or by everything in there?  \\n So you can pull up the documentation for that.  \\n Basically if you say axis is None,  \\n that's going to color by everything in the data frame.  \\n I'm also going to use loc here to reorder my columns.  \\n Note that these condition columns have a meaning.  \\n Ex means excellent, Gd means good.  \\n TA means typical, FA means fair,  \\n and PO means poor.  \\n So I'm going to order these from best to worst.  \\n I also have Missing and NA at the end here.  \\n Let's run this now  \\n and this tells us a little bit different story,  \\n that we have a bunch of values here  \\n that are in this typical value in the middle here.  \\n Another way to visualize this is to do a bar plot.  \\n I'm going to do a stacked bar plot.  \\n This is taking the results of my cross-tabulation  \\n and I'm going to say do a bar plot  \\n but stacked is equal to true.  \\n So let me walk you through what Pandas does  \\n when you do a bar plot.  \\n Again, it's going to take the index  \\n and it's going to put the index in the x-axis.  \\n In this case, the overall condition.  \\n We have multiple columns here  \\n so generally when we do a plot,  \\n it's going to plot each of those values above the index.  \\n So let's look at what  \\n it would normally look like, something like that.  \\n I'm going to say stack this  \\n and it's going to stack those values on top of each other.  \\n In this case, this is allowing us to see  \\n that for Overall Quality five, six, seven, and eight,  \\n we have a lot of TA entries.  \\n We can easily visualize that by looking at this plot.  \\n In this lesson, we looked at taking two categorical values  \\n and understanding how we can summarize those.  \\n Generally if we do that with a cross-tabulation,  \\n we said that a cross-tabulation is a table of data.  \\n We can use our techniques for visualization  \\n to understand that table better  \\n by either coloring the values in there  \\n or actually making a bar plot.  \\n Using a stacked bar plot  \\n is one way to understand a cross-tabulation.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4503098\",\"duration\":27,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Explore Ames\",\"fileName\":\"4433355_en_US_02_08_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":753160,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (electronic music)  \\n - [Instructor] It's time for our next challenge.  \\n In this challenge,  \\n of the first floor square footage against the sell price.  \\n We want to see what that relationship is between the size  \\n of the house and how much the house sold for.  \\n You might consider using some  \\n of the tricks that we talked about  \\n as we visualize the scatter plot.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4507090\",\"duration\":208,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Explore Ames\",\"fileName\":\"4433355_en_US_02_09_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6756332,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] Let's look at the solution for this.  \\n I'm going to take my housing data set.  \\n I'm going to call .plot.scatter,  \\n and let's choose to put SalePrice in the X axis,  \\n and we're going to put first floor square footage  \\n in the Y axis.  \\n Let's run that and see what it looks like.  \\n We get something that looks like this.  \\n So my first thought when seeing this is it looks like  \\n as sell price goes up, first floor square footage goes up.  \\n Also, it looks like there's a hard line here along  \\n the bottom edge, which looks like  \\n there's some relationship going on there  \\n that you can't have a house  \\n that is very small going for a high price.  \\n It looks like we're not really seeing that.  \\n There are some outliers here  \\n where we are seeing large houses that are cheap.  \\n It'd be interesting to know what's going on  \\n with these outliers that are sort of out here.  \\n It looks like there's a cone where most of the values lie.  \\n Now, another thought that I have, when I'm looking at this,  \\n I'm not seeing columns or rows necessarily,  \\n but I am seeing a big dark patch,  \\n and I don't like to see big dark patches in a scatterplot.  \\n Remember, one of the things I said you could do  \\n is adjust the alpha.  \\n Another thing you can do is if you have too much data,  \\n and you just keep lowering that alpha,  \\n but you're not seeing transparency  \\n is to come in here and do samples.  \\n We could say, let's look at maybe 300  \\n of those entries and see what's going on here.  \\n That might be another way to see where the majority  \\n of the data is lying.  \\n However, I'm going to go and use our alpha,  \\n and I'll start off by setting that .5.  \\n That looks okay.  \\n It's telling a different story,  \\n but you can see down here it's just completely dark.  \\n I don't like to see it completely dark.  \\n I like to see some transition in there.  \\n Let's go down to .2.  \\n I think that's looking better.  \\n I might even go down to .1,  \\n and I think that tells a different story.  \\n This is actually fascinating, if you look at this.  \\n I'm maybe going to try a little bit lower here.  \\n Okay, and I'm seeing like a cluster of data here,  \\n but I'm also seeing a cluster down here below that,  \\n which is kind of interesting.  \\n It looks like there's a pattern up here,  \\n and there's another pattern up here.  \\n This looks to me like there is a bunch of houses  \\n that for some given size are less expensive,  \\n and then there's a cluster of houses  \\n for the same size that are more expensive.  \\n Might be interesting to see what's going on there.  \\n Is that a different neighborhood?  \\n Are these built in different years?  \\n These are the the sorts  \\n of ideas I have when I see a scatterplot like this,  \\n and note that if I don't adjust that alpha,  \\n I'm not going to have that insight here.  \\n I don't see that at all  \\n if I leave the alpha as the default value.  \\n I hope this was useful to you.  \\n In this example, I showed you how to plot a scatter plot  \\n and how to adjust the alpha to see some patterns  \\n that aren't apparent with the default scatter plot.  \\n \\n\\n\"}],\"name\":\"2. Exploring and Visualizing\",\"size\":119687113,\"urn\":\"urn:li:learningContentChapter:4507092\"},{\"duration\":1880,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4503097\",\"duration\":441,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Linear regression\",\"fileName\":\"4433355_en_US_03_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Linear regression is commonly used in data analysis and can help you identify significant predictors of a target variable, as well as their effect size and direction.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":16203969,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson,  \\n we're going to look at linear regression.  \\n The goal of linear regression is to take data  \\n and predict numeric values from that.  \\n So we're going to take our housing data  \\n and we're going to see if we can predict the sales price  \\n given the other numeric values from that.  \\n I'm going to show you how to do that using Python  \\n and we're going to talk about splitting your data  \\n into a training set and testing set  \\n and evaluating how your model performs.  \\n We're going to be using the scikit-learn library.  \\n The scikit-learn library is a library  \\n that's very popular in academia,  \\n but it's also popular in industry.  \\n One of the nice things about the scikit-learn model  \\n is that it has a common interface.  \\n That common interface makes it very easy  \\n to try out different models.  \\n We're going to be looking at linear regression here,  \\n but we'll also be looking at some other types  \\n of models that can perform regression analysis.  \\n And because they conform to the scikit-learn API,  \\n it's very easy to swap them out  \\n and see how they perform.  \\n In order to run a model,  \\n we have to make two datasets.  \\n One is typically referred to as capital X  \\n and that is a matrix of data.  \\n Generally, it can be a Pandas DataFrame.  \\n And if you look at our X here,  \\n we are saying, okay, take our housing data  \\n and then pull off the numeric columns  \\n and also drop the sales price column.  \\n Remember, we're going to try  \\n This X is the features or the columns  \\n that we're going to try  \\n and use to predict sales price.  \\n So keeping sales price in there would be cheating  \\n if we're training the model with the sales price  \\n and asking it to predict the sales price.  \\n Then the other data set that we need  \\n is the target that we want.  \\n In this case, it is the SalePrice column.  \\n I'm going to use a function  \\n from scikit-learn called train_test_split.  \\n And what that's going to do is,  \\n it's going to take our X  \\n and our Y data and split it up.  \\n We're going to split it up  \\n into a training set and a testing set.  \\n This is very important to do  \\n when you're using machine learning.  \\n The reason is you want to understand  \\n how your model performs.  \\n And if you evaluate your model on data  \\n that's seen before, it's pretty trivial  \\n to make a model that can perform very well  \\n on data that's seen before.  \\n All it has to do is memorize that data.  \\n So we're going to hold out some of our data  \\n and that's what train_test_split does.  \\n The output of train_test_split is four things.  \\n It's the X training data, the X testing data,  \\n the Y training data, and the Y testing data.  \\n Let's run this code and make sure that it works.  \\n Looks like it did work.  \\n Let's look at what X_train looks like after running that.  \\n Here is X_train.  \\n If you look at the index on the left side,  \\n you can see that it's not sequential.  \\n What this has done is randomly pulled out  \\n different rows to train our model on.  \\n If we looked at the X_test data,  \\n it would be the data  \\n that is not in the training data.  \\n The remaining rows in there.  \\n Here's the Y training data.  \\n And you can see that the index for y_train  \\n is actually the same as the index for X_train.  \\n However, this is a series.  \\n Remember, in scikit-learn,  \\n our X is going to be a data frame  \\n and you can have a data frame with one column.  \\n But generally, data frames have multiple columns,  \\n but a series is only a single column.  \\n These are the values that we want to predict.  \\n Here is how we make our model.  \\n We're first going to say linear_model.LinearRegression.  \\n This makes an instance of the model.  \\n And then training the model is one line of code.  \\n It's calling lr.fit.  \\n We pass in the training data with the training label.  \\n In this case, it's y_train.  \\n And then the next line is a mechanism  \\n to score our data and see how it scores.  \\n And we're going to pass in X_test and y_test.  \\n And when we run this, it actually does not work.  \\n Let's look at our error and see what happens.  \\n And we got a value error.  \\n It says the \\\"Input X contains NaN.\\\"  \\n And if you remember,  \\n we had a bunch of missing data in there.  \\n It turns out that linear regression  \\n does not like missing data.  \\n In fact, if you have missing values  \\n in your data, it will not work.  \\n Let's just double-check to see  \\n if there is missing data in there.  \\n And it looks like, for example,  \\n in Lot Frontage, there's missing data  \\n and a bunch of other columns have missing data as well.  \\n We talked a little bit  \\n about handling missing data previously.  \\n Again, my recommendation is to find an expert  \\n to understand why the data might be missing.  \\n That expert might be able to guide you  \\n into appropriate actions to deal with the missing data.  \\n In our case, I'm going to make  \\n this new function called clean_housing_no_na.  \\n And if you look at that,  \\n what that's going to do is,  \\n it's going to add in this extra line  \\n into our chain here that's going to say,  \\n take the current state of the data frame  \\n after shrinking the integers,  \\n and pull off the numeric types,  \\n fill in the missing values with zero,  \\n and stick that back into your data frame.  \\n So housing2 after running this should have a bunch  \\n of zeros instead of missing numbers.  \\n Now, let me make an assign here.  \\n Is this the right thing to do?  \\n I'll say, probably not.  \\n We could have a whole course  \\n that talks about filling in missing values.  \\n For the purposes of this course,  \\n I'm introducing linear regression  \\n and not talking about all the ins and outs  \\n about filling in missing values.  \\n Let's, again, split our data  \\n and now let's run our model.  \\n And it looks like we get a score of 0.84.  \\n So the score that comes out of this  \\n is often called the R2 or R squared score  \\n or the coefficient of determination.  \\n This is a value between zero, typically, and one.  \\n The closer this is to one,  \\n the better the model is.  \\n What this represents is the amount of variance  \\n in the label or target that is explained  \\n by the features or columns in the data.  \\n So 84% of the variance in the label  \\n that we're trying to predict  \\n or that sales price is predicted  \\n by the data that we're passing into this.  \\n If we had another model that had a score of 0.9,  \\n that would be a better model.  \\n If we had a model that scored 0.8,  \\n that would be a worse performing model.  \\n Question people often asks,  \\n is 0.84 good enough or is your score good enough?  \\n And the answer to this is, typically,  \\n an unsatisfying it depends.  \\n It might be good enough, it might not be.  \\n Further analysis and understanding  \\n what the predictions often mean  \\n and how far off they are can often tell you  \\n from a business context  \\n whether it would make sense to deploy a model or not.  \\n We can have a whole class on linear regression.  \\n For the purposes of this class, we're introducing it.  \\n In this lesson, we created a linear regression model.  \\n We were able to predict the sales price  \\n from the training data  \\n from the other numeric columns in our dataset  \\n and we saw that we got a score  \\n of 0.84 when we did that.  \\n We also saw that linear regression  \\n did not like it if we had missing values.  \\n So we took a shortcut.  \\n We filled in those missing values with zero.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4504090\",\"duration\":260,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Interpreting linear regression models\",\"fileName\":\"4433355_en_US_03_02_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"By interpreting the coefficients, you can understand the magnitude and direction of the effect of each independent variable on the dependent variable.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9957878,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] We just made a linear regression model.  \\n Now we want to try and understand what's going on  \\n with the model.  \\n Let me tell you about a convention in scikit-learn.  \\n Again, scikit-learn is full of conventions.  \\n Once you understand these conventions,  \\n it makes it really easy to use the model,  \\n and it also includes a bunch of different models,  \\n so it makes it easy to try out different models  \\n and see how they perform.  \\n One of the conventions in scikit-learn  \\n is that if an attribute ends in underscore,  \\n it was an attribute that was discovered  \\n or learned by fitting the model.  \\n In this case, our linear regression model  \\n has an attribute called coef_.  \\n This attribute represents the coefficients.  \\n If you have taken a math class,  \\n you might have learned about the formula for a line,  \\n y is equal to mx+b,  \\n where m is the slope, and b is the intercept.  \\n It turns out that linear regression has the same thing.  \\n The coefficient is called the slope,  \\n and the intercept is the intercept.  \\n And this coefficient represents the weights  \\n that are multiplied by each of the values in the columns  \\n to make the prediction.  \\n We also have lr.intercept_,  \\n and this is the base value.  \\n In this case, this is saying that our housing value  \\n starts at it looks like $15 million.  \\n To create a prediction,  \\n we add up the intercept value  \\n with the linear combination of the coefficient,  \\n with the values from each corresponding column.  \\n That will give us what the prediction is.  \\n (typing)  \\n Here are the column names  \\n that correspond with those coefficients.  \\n We can stick those into a panda series.  \\n At this point, we can take this panda series and plot it.  \\n So we're going to do a bar plot with this.  \\n And if you look at this bar plot,  \\n what this is telling us  \\n is this is telling us the weights  \\n that impact the model the most.  \\n In this case, it is saying overall quality  \\n pushes the model towards a more positive price,  \\n and kitchen above ground pushes the model  \\n towards a more negative price, as that increases.  \\n You can see in the middle here,  \\n there are a bunch of the features or columns  \\n that have a small value.  \\n These columns are features that don't have a large impact  \\n on the output of the model.  \\n What I'm going to do is use some pandas code  \\n to just do a little bit of filtering here.  \\n So we're going to start off with our series  \\n of the coefficients,  \\n and then I'm going to use this pipe here  \\n to just get the values that have an absolute value  \\n of greater than 100, and we'll do a bar plot of that.  \\n So these are the coefficients  \\n that tend to have the most impact on our model.  \\n One of the nice things about coefficients  \\n is that they have both a magnitude and a sign.  \\n So again, if that sign is positive,  \\n if that value goes up,  \\n it tends to push the value of the final prediction up.  \\n If the sign is negative, if that value would go up,  \\n it would push the result down.  \\n In this cell, I'm pulling off the features  \\n that have the most impact on the model.  \\n It turns out that we might want to use these later on,  \\n and pull those out.  \\n You just pull off the index from the series.  \\n In this lesson, we looked at understanding a model.  \\n One of the nice things about a linear regression model  \\n is that it's completely explainable.  \\n What do I mean by that?  \\n I mean that you can take this intercept value,  \\n and you can take the labels  \\n that are passed into the model to make a prediction,  \\n multiply them by these coefficients,  \\n and if you sum those up,  \\n you get the prediction from the model.  \\n This turns out to be a nice feature  \\n that you can interpret the model.  \\n You can understand why it's making the predictions it has.  \\n Also, you can look at the magnitudes of these coefficients.  \\n The larger the magnitude of them,  \\n the more impact they have on the model.  \\n The coefficients that have a small magnitude  \\n mean that the feature is not having as much impact  \\n on the final prediction.  \\n In future lesson, we'll look at different models  \\n and compare and contrast those with linear regression.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4508090\",\"duration\":382,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Standardizing values\",\"fileName\":\"4433355_en_US_03_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"The goal of this video is to learn how to standardize variables, which involves transforming variables to have a mean of zero and a standard deviation of one.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":18358621,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson, we're going to look at  \\n standardizing values.  \\n We're going to talk first about what this concept  \\n of standardization means,  \\n and then we'll learn how to do this from scikit-learn.  \\n Again, once you understand the conventions of scikit-learn,  \\n this is not particularly hard.  \\n And then we'll look and see if this impacts our score.  \\n We're also going to reexamine our coefficients  \\n after doing that and see if those change as well.  \\n So standardization has a specific meaning to statisticians.  \\n And oftentimes, people say normalize,  \\n and when they say normalize, they mean standardization.  \\n What standardization means  \\n is that we're going to take our X or our dataframe  \\n and we're going to basically collapse the values  \\n or shrink them into a given range  \\n or maybe expand them into a given range and center them.  \\n More specifically,  \\n it means we give each column a mean value of zero  \\n and the standard deviation of one.  \\n Why is that?  \\n You could imagine that certain models  \\n are impacted by the scale of the data.  \\n And if you have a housing model that's predicting price,  \\n it might have one column  \\n that represents the number of bathrooms in a house,  \\n which might be like from one to five bathrooms,  \\n might be the maximum in a house.  \\n You might have another column  \\n that represents the square footage,  \\n which might be from like 200 for a mini house  \\n up to like 20,000 for a mansion.  \\n The scale of the 20,000,  \\n that's a couple order of magnitudes bigger  \\n than the number of bathrooms.  \\n And certain algorithms  \\n are impacted by the relationship  \\n between the magnitude of the columns.  \\n One of the ways to do that is using standardization.  \\n Let's look at how to do this using scikit-learn.  \\n There is a pre-processing module in scikit-learn.  \\n And you can make an instance of the pre-processing module.  \\n A StandardScaler is a transformer in scikit-learn.  \\n That means that it has at least two methods.  \\n One is fit and the other is transform.  \\n In the case of the training data,  \\n we're actually going to call fit_transform,  \\n which combines fitting and transforming.  \\n So fit trains the model,  \\n and then transform takes input  \\n and transforms the input to a given output.  \\n In the case of a StandardScaler,  \\n it takes data after it's been trained,  \\n the training tells the model, \\\"Hey,  \\n this column should have a mean value of this  \\n and this column should have a mean value of this,\\\"  \\n and it knows how to shrink and move those means around.  \\n And then the transform actually does that transform for you.  \\n So here we're going to call fit_transform  \\n on the training data.  \\n This gives us a new X_train.  \\n And then on the testing data, we have to be careful here.  \\n We don't want to call fit again on the testing data,  \\n we just want to call transform on the testing data.  \\n So, once we've made a model,  \\n we want to evaluate it on data that it hasn't seen before.  \\n If we retrain the model,  \\n call fit on data that it hasn't seen before,  \\n we're kind of cheating.  \\n So we want to make sure that we only call transform  \\n on that testing data.  \\n Let's run that and make sure that it works.  \\n It looks like it did.  \\n Let's run our model now  \\n with our new data that's been standardized.  \\n And it looks like in this case, we get the same score.  \\n So our score did not particularly change by doing that.  \\n Let's look at our coefficients.  \\n And it looks like our coefficients  \\n are actually different now.  \\n I'm going to sort the values of our coefficients.  \\n You can see that we have some coefficients  \\n that are pretty negative  \\n and some other coefficients that are pretty positive.  \\n A couple of orders of magnitude above the others.  \\n Let's do a bar plot there with some filtering  \\n just to zoom in on those.  \\n And it looks like we have basement square footage,  \\n basement unfinished square footage,  \\n ground living area, basement square footage 2.  \\n It looks like a lot of the features that impact our model  \\n are square footage features, or how big the house is.  \\n And intuitively, it makes sense that square footage  \\n would be one factor that determines  \\n how much a house sells for.  \\n And if we want to pull those values off of the index,  \\n we can.  \\n In this, I'm looking at the union of the features  \\n from our model that we had not standardized  \\n and the data that we did standardize here below.  \\n Let's look at the correlations between those  \\n and just see if there happened to be some correlations  \\n between all of these features  \\n that tended to be important in these two models.  \\n Again, I'm going to set a background gradient  \\n to this diverging color map.  \\n Red going through white to blue.  \\n And I'm also going to pin that negative value to -1.  \\n So we can look at the redest values  \\n and the bluest values here.  \\n So we do find some blue values that are off diagonal.  \\n That's because I filtered the columns here.  \\n So the diagonal here is not necessarily  \\n going to be the values that are one.  \\n But we can see the overall quality  \\n has a pretty high correlation with sell price,  \\n as does year built, year remodeled.  \\n Here I'll just scroll this over  \\n and look at the correlations there.  \\n You can see the garage area, wooden deck square footage,  \\n open porch square footage  \\n have a slightly positive correlation with our sell price.  \\n In this lesson, we looked at standardizing our data.  \\n Generally, you will want to standardize your data  \\n when you're doing linear regression.  \\n We saw in our model that performing standardization  \\n did not impact this model, but that's not always the case.  \\n It is possible that standardizing your variables  \\n can have a significant impact  \\n on how your model performs.  \\n We did see that it actually changed the coefficients.  \\n And we had different coefficients emerging  \\n as more important when we standardized our data.  \\n In the next lesson, we'll look at using other models,  \\n and we'll see how other models perform  \\n to our linear regression model.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4509081\",\"duration\":522,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Regression with XGBoost\",\"fileName\":\"4433355_en_US_03_04_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"The goal of this video is to learn how to build and interpret a regression model using XGBoost, which is a popular algorithm for gradient boosting.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":19400016,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson,  \\n we're going to use a very popular library called XGBoost  \\n to make a regression model as well.  \\n We're going to compare and contrast the performance  \\n and the interpretability of XGBoost with linear regression.  \\n Let me quickly explain what XGBoost does.  \\n It makes a decision tree to make predictions  \\n and those predictions, in this case,  \\n the price of the house are probably going to be off  \\n from the actual values.  \\n So the boosting comes from making subsequent trees  \\n and the subsequent trees take that same input  \\n and try and fix the error.  \\n The error is called the residual.  \\n And you can add as many trees as you want.  \\n Each of those trees keeps fixing that error.  \\n I like to compare this to golfing.  \\n When you golf generally you don't just hit the ball once.  \\n You have multiple times to hit the ball  \\n and that's what this is doing.  \\n The first tree hits the ball, so to speak.  \\n It's not going to go in the hole probably.  \\n It'll be away from the hole by some amount,  \\n there'll be some residual  \\n and then the next tree will basically take a new club  \\n and hit the ball from there and try and fix the error,  \\n get it closer to the hole, so to speak.  \\n So this is like golfing and having multiple times  \\n to hit the ball to get it in the hole.  \\n That's a good analogy for XGBoost.  \\n Now, one of the nice things about XGBoost  \\n is that it uses the same interface,  \\n that scikit-learn interface.  \\n So if you have data that works in scikit-learn  \\n you can easily use XGBoost and make a model.  \\n One of the downsides of XGBoost  \\n is that it is not completely interpretable.  \\n That is to say with linear regression,  \\n we could look at the intercept  \\n and we could look at that coefficient  \\n and we could explain how the model is working.  \\n With XGBoost, it's a little bit more difficult.  \\n We have a bunch of decision trees  \\n and you could have hundreds of decision trees  \\n and going through each of those decision trees  \\n and looking at how they sum up at the end  \\n could be a little tedious.  \\n You could do it if you wanted to, but in practice  \\n no one does because it's too much work.  \\n Okay, one more thing that's useful about XGBoost  \\n is that you don't have to clean up missing values.  \\n In fact, it will work with columns that have missing values.  \\n In addition, XGBoost will also work with columns  \\n that are not numeric.  \\n So this gives it a lot of power.  \\n You can take your data and without doing a ton  \\n of pre-processing, you can throw it into an XGBoost model.  \\n It also doesn't even require your data to be standardized.  \\n So this makes XGBoost a nice model to try out  \\n and see how it performs.  \\n Here, I'm going to make x and y.  \\n Note that I am not filling in my missing values with zero,  \\n I'm just leaving those missing,  \\n and then I'm going to split my data.  \\n In this case, I will standardize it.  \\n It turns out that standardization does not  \\n impact XGBoost model one way or the other.  \\n It will perform the same because  \\n if you think about how a decision tree is working,  \\n it's looking at a single column,  \\n it's not comparing one column to another,  \\n and so that relative size of the columns  \\n is not really important for a tree based model.  \\n Let's run that.  \\n Okay, it looks like that worked  \\n and we're going to come down here and make our model.  \\n Now look at this.  \\n This looks almost identical to what we had above  \\n when we made our linear regression model.  \\n We made an instance of the model.  \\n In this case, we're making an XGBoost regressor  \\n and then we called our fit with the training data  \\n and we called score with the testing data.  \\n Again, one of the nice things  \\n about scikit-learn is that it has a common interface  \\n and there are many models that have that interface.  \\n Look at this.  \\n Our score here of our XGBoost model is 0.89.  \\n So this is a better model,  \\n or it performs better than our linear regression model.  \\n A question you might ask yourself  \\n is 0.89 a sufficiently good model?  \\n And that's a different answer.  \\n The answer again, is this unsatisfying, it depends.  \\n It is a better model than linear regression  \\n in that it performs better,  \\n but we don't know if that performance  \\n is sufficiently good enough  \\n to impact our business.  \\n That would require us diving into how far  \\n off the predictions are and if we would make  \\n or save money by leveraging that model.  \\n Note that XGBoost has an attribute called  \\n feature_importances_ that is learned by doing the fit here.  \\n This is like the coefficients  \\n but it's a little bit different.  \\n It doesn't have a direction, it only has a size.  \\n So in this case, it is saying that the feature  \\n that impacted our model the most was the overall quality.  \\n After that, it was the garage cars.  \\n And at a high level what's going on here,  \\n you can think we have a bunch of trees.  \\n The features that are at the top of the trees  \\n where it makes those decisions  \\n tend to be the most important features.  \\n So this is basically a way of measuring which features  \\n come at the top of the trees.  \\n You can see that there are some of these features  \\n at the bottom that have very low scores,  \\n meaning that those features don't really impact  \\n or drive the model too much.  \\n In fact, we could probably make a semi-decent model  \\n just by looking at the overall quality,  \\n how big the garage is,  \\n the number of baths, and the living area.  \\n In this example, I'm going to use categories  \\n in my XGBoost model.  \\n One of the things about our XGBoost model  \\n is that our XGBoost model doesn't like the pyarrow types.  \\n So I've got a chain here.  \\n I am selecting the numeric values.  \\n These are the pyarrow numbers  \\n and I'm converting these to N64s.  \\n These are the pandas values that XGBoost is okay with  \\n and I'm also dropping the sales price column.  \\n Note that I did not remove the categorical values,  \\n like I did with the other examples here.  \\n Then I'm going to split my data again  \\n and I'm going to say XGBoost Regressor.  \\n I'm going to say enable categorical is true,  \\n telling it I want categoricals to work in here.  \\n And then if I want categoricals to work  \\n I also have to say that the tree method  \\n which is the way that XGBoost makes the trees  \\n is this string hist.  \\n And let's just run that.  \\n When you run that calling fit and score  \\n we see that we got a model of 0.91.  \\n So remember we went from 0.84 with linear regression  \\n to 0.89 with XGBoost with the numbers  \\n to 0.91 with XGBoost with the categories.  \\n So we do get a boost from using those categories  \\n indicating that those categories might be important.  \\n So let's ask XGBoost  \\n what are the important features of the categorical model?  \\n And it looks like the first categorical model  \\n that I'm seeing is neighborhood.  \\n If you remember back in our scatter plot  \\n we had it looks like two clusters.  \\n The neighborhood might have explained that.  \\n It might have been a fancier neighborhood  \\n that costs more for the same size house.  \\n So we are probably seeing that there is value  \\n including that neighborhood categorical into our model.  \\n Further exploration would be useful  \\n in validating that theory.  \\n In this lesson, we looked at using XGBoost,  \\n a popular library, to make our regression model.  \\n We saw that it performs slightly better out of the box.  \\n We saw that if we add categoricals, it performs even better.  \\n Again, one of the downsides of this XGBoost model  \\n relative to a linear regression model,  \\n is that the explainability suffers a little bit.  \\n A linear regression model is completely explainable.  \\n An XGBoost model is kind of explainable  \\n but it's tedious to explain.  \\n That might be a trade-off.  \\n The performance versus the explainability  \\n might be a business decision that you need to consider  \\n whether you use a better performing model  \\n if you can't explain it.  \\n Think of a situation like a business loan.  \\n You might want to offer someone a loan,  \\n but if you can't explain why you're denying them a loan  \\n that might make the customer upset, causing them to churn.  \\n Finally, remember, because we're using  \\n that scikit-learn interface  \\n it makes it easy to try out these different models.  \\n I highly encourage you to not just try one model.  \\n Try out a few models and see how they perform.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4509080\",\"duration\":40,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Predict Ames\",\"fileName\":\"4433355_en_US_03_05_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1159627,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] In this challenge,  \\n you're going to predict the sales price  \\n of the Ames housing data.  \\n What I would like you to do is make  \\n a linear regression model,  \\n but I want you to only use the top five features  \\n from the XG Boost model that was not using categorical data.  \\n I want you to pull off those features.  \\n We saw how to do that in a previous lesson,  \\n and then I want you to train a linear regression model  \\n with just those features from the training set  \\n and then evaluate the model.  \\n What is the score or coefficient of determination  \\n or our squared value of that model?  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4503096\",\"duration\":235,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Predict Ames\",\"fileName\":\"4433355_en_US_03_06_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8494356,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] Let's explore a solution to this challenge.  \\n The first thing I want to do is inspect my XG boost model  \\n and see if I can get those features from that.  \\n So here's my XG boost model.  \\n Remember, we discussed that it has an attribute  \\n called feature_importances_ that ends in underscore,  \\n again that underscore meaning  \\n that attribute was learned when we fit the model.  \\n So here's our feature_importances_,  \\n this is a NumPy array of the importance of each feature.  \\n This corresponds with the training data.  \\n So we should be able to stick this into a Panda series  \\n and get the names of that.  \\n So I'm going to say  \\n let's stick those values into a Panda series.  \\n And for the index  \\n I'm going to use the columns from the training data.  \\n Let's look at what that looks like.  \\n That's looking pretty good.  \\n I'm going to stick this into a chain.  \\n Now I want to sort these values.  \\n Let's just look at what that looks like when we sort that.  \\n It looks like we're going to want to take the values  \\n at the bottom.  \\n Those have the highest score.  \\n So to take the values at the bottom,  \\n I'm going to pull off the index.  \\n Let's just evaluate that  \\n and make sure that that looks correct.  \\n Okay, yeah, we want those values at the end  \\n and we want the last five values at the end.  \\n So to get those, I'm going to use a slice syntax  \\n and I'm going to say minus five colon.  \\n If you use that negative indexing, it's going to go back five.  \\n And then the colon says go to,  \\n and because we didn't put anything on the right side  \\n of the colon, it's just going to go to the end.  \\n So it's going to take the last five values.  \\n Let's just run that and evaluate that.  \\n It looks like that did work.  \\n Okay, these are the features that we want to use.  \\n So I'm going to come up here  \\n and I'm going to store these features in a list.  \\n I'm going to say top five is equal to,  \\n and I'll stick those into a list.  \\n And then what I'm going to do is  \\n I'm going to make a new model down here.  \\n I'm going to call this the linear regression model.  \\n I'm going to say it's the top five features  \\n and then I'm going to use my linear model.  \\n And I'm going to say linear regression.  \\n We'll make an instance of that.  \\n And then I need to fit my model.  \\n So I'm going to say linear regression, top five.  \\n We're going to call fit.  \\n I'm going to call it with X_train.  \\n And I'm going to use loc here to specify a subset of this.  \\n Now remember, loc takes a row selector  \\n and a column selector.  \\n I'm going to specify just a colon here  \\n for the row selector, that's a slice,  \\n that means take all of the rows.  \\n And then for my column selector, I'm going to pass in top five.  \\n Let's just run this right here  \\n and make sure that that works.  \\n And I got an error.  \\n It says it is missing a positional argument, y,  \\n so it did not work.  \\n Why didn't it work?  \\n 'Cause I need to pass in y_train as well.  \\n Let's try it again.  \\n The challenge is asking us what is the score of this model?  \\n To get the score of this model  \\n we need to call the score method.  \\n This looks very similar to what we have above.  \\n I'm going to say lr top five score.  \\n I'm going to say X_test.  \\n And then again, I need to subset what I'm selecting.  \\n So I'm going to say take all of the rows  \\n but just the top five columns.  \\n And then we need to pass in the actual true values  \\n so it can evaluate how it did on its performance.  \\n And we got a score of 79.  \\n So our score dropped a little bit when we used a subset  \\n of these features, but we're only using five features.  \\n This might be a trade-off that you're willing to make  \\n to have a very simple model  \\n even if it performs slightly less.  \\n \\n\\n\"}],\"name\":\"3. Linear Regressions\",\"size\":73574467,\"urn\":\"urn:li:learningContentChapter:4509086\"},{\"duration\":1342,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4507089\",\"duration\":160,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Exploring data\",\"fileName\":\"4433355_en_US_04_01_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"The goal of this video is to understand the data to perform statistical tests.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6301362,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] We're going to start looking  \\n at doing some hypothesis testing using Python.  \\n In this lesson, we're going to look at the data  \\n that we're going to explore.  \\n Let's run this first cell here.  \\n We are going to use the stats module from the SciPi library.  \\n Let's make sure that we import that,  \\n and we're going to be looking at neighborhoods  \\n and comparing neighborhoods.  \\n Because neighborhood is a categorical value  \\n I'm going to use my favorite method on that,  \\n value counts, to look and see what we have in here.  \\n It looks like the two most popular neighborhoods are NAmes  \\n and it looks like probably college, CR College Creek,  \\n College something.  \\n So let's limit our analysis to those two neighborhoods.  \\n Another way to look at these categoricals  \\n is to use a group by,  \\n and then for our aggregation, do a describe.  \\n Look at what the output of this looks like.  \\n This is kind of cool.  \\n In the index, that because we grouped by neighborhood  \\n we have all the neighborhoods  \\n and in the columns, now we have hierarchical  \\n or multi-index in the columns,  \\n we actually have two levels of columns.  \\n For each numeric value, we have the outer column  \\n and then in the inner columns,  \\n under that, we have the summary statistics for those.  \\n Let's limit those to the two neighborhoods  \\n that I'm interested in looking at.  \\n I'm going to call one College Creek  \\n and the other one North Ames.  \\n So let's look at those two rows  \\n and let's just pull out the sale price.  \\n Here we can see the summary statistics  \\n for those two neighborhoods.  \\n College Creek looks like it has 267 entries.  \\n The mean sales price for College Creek is around $200,000,  \\n whereas North Ames has a mean sales price around $145,000.  \\n We might want to ask ourselves,  \\n is this data different or is it the same?  \\n It looks like it's somewhat different  \\n in that the mean is different, let's look at the median,  \\n the median actually looks different as well.  \\n The maximum values are different  \\n and the minimum values are different.  \\n It looks like from just looking  \\n at these summary statistics here  \\n that College Creek is a more expensive neighborhood  \\n than North Ames.  \\n Let me just show you an option that you can do  \\n sometimes when you do a describe that gets a little wide,  \\n if you want to often see a little bit more data  \\n you can use T there to transpose the data.  \\n Makes it a little bit more compact sometimes  \\n and fits in the screen a little better.  \\n We want to explore two neighborhoods, so we're going to look  \\n at this North Ames in this College Creek neighborhood.  \\n In this lesson, we did some summary statistics  \\n to compare the sales price of those two neighborhoods.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4504089\",\"duration\":363,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Visualizing distributions\",\"fileName\":\"4433355_en_US_04_02_XR30\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"The goal of this video is to visualize distributions of data before performing a statistical test.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12359122,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In the last lesson,  \\n we looked at summary statistics  \\n to understand these two neighborhoods.  \\n Let's use some visualization now to understand them as well.  \\n I'm going to pull out the North Ames  \\n and College Creek data into their own series  \\n so that we can look at them a little easier.  \\n I'll just use some pandas to do that.  \\n And the next thing I'm going to do  \\n is I'm going to make a histogram of both of those.  \\n So I want to plot these histograms on top of each other.  \\n What I'm going to do is call hist on our North Ames series.  \\n I'm going to give that a label as well,  \\n so a legend will appear in that,  \\n and that will return a map plot lib axis.  \\n I'm going to take that axis  \\n and pass it into the College Creek histogram call  \\n so that College Creek is plotted  \\n on the same axis or same plot as we have.  \\n I'm also giving that a label,  \\n and then after I do both of those histograms,  \\n I'm going to tell map plot lib to throw a legend on there  \\n with AX dot legend.  \\n This is the result of that.  \\n Now, note that both of these are fully opaque  \\n and because we plotted College Creek  \\n after we plotted North Ames, it is occluding some of that.  \\n It might be the case, probably isn't,  \\n but it might be the case  \\n that what College Creek is occluding,  \\n North Ames matches directly,  \\n but it's hard to tell because we don't know  \\n what's going on underneath College Creek there.  \\n To deal with this, I'm going to adjust the transparency,  \\n and we do that by changing the alpha parameter.  \\n So here I'm going to set the alpha to 0.7.  \\n Remember, alpha is how transparent something is.  \\n This should let us understand what's going on below that.  \\n Let's look at this.  \\n From a quick look at the histogram,  \\n these look like different distributions.  \\n We already looked at the summary statistics.  \\n We thought that they were different distributions as well.  \\n It looks like this histogram is confirming that.  \\n Let's look at one more plot  \\n that's useful for looking at distributions,  \\n that's continuous distribution function.  \\n I'm going to show how to do that with pandas.  \\n Let me just walk through this chain here  \\n because we have a few steps of that.  \\n So we have our North Ames series.  \\n I'm going to convert this to a data frame by saying two-frame.  \\n Now it's a data frame with a single column.  \\n That's fine.  \\n A data frame is two dimensions, but it is possible  \\n to have a data frame with a single column.  \\n Now that I have a data frame, I can add a new column to it.  \\n I'm going to make a column called CDF,  \\n and that is going to be taking these values  \\n and ranking them.  \\n And the CDF column is going to be the result  \\n of calling the rank method on our data  \\n and converting that into a percentage.  \\n So it looks like the first entry there  \\n is around the 96 percentile.  \\n The next entry is around the six percentile of entries.  \\n The next thing I'm going to do  \\n and we should see when we do this, the index should change.  \\n And indeed it does.  \\n The next thing that I'm going to do  \\n is I'm going to plot this.  \\n I've said multiple times in this course,  \\n one of the keys to plotting in pandas  \\n is understanding how pandas makes plots.  \\n In this case, I'm just going to call the plot method,  \\n which will make a line plot.  \\n Now with a line plot, I can specify X and Y.  \\n If I don't specify X, it will use the index for X.  \\n I don't want to use the index for X,  \\n what I want in the X axis is the cell price  \\n and in the Y axis I want that percentage or that CDF.  \\n Let's un-comment that and run that, see what it looks like.  \\n We get something that looks like this.  \\n So if we created a CDF of College Creek and North Ames,  \\n and they had the same distributions,  \\n we would expect these plots to overlap each other,  \\n to trace each other, essentially.  \\n Let's see if they do.  \\n I'm going to create a function called plot CDF  \\n and it's going to take a series as the input  \\n and an optional map plot lib axis and a label.  \\n It's going to do the logic that I just showed above,  \\n converting a series to a frame, making a CDF column,  \\n sorting it, and then doing the plot,  \\n but it's going to return the series as the output.  \\n Let's run that and see what happens,  \\n just make sure that it works.  \\n You can see that this returned the series.  \\n You can see at the bottom, there is a series there,  \\n and then below that  \\n it had a side effect of making that plot.  \\n With that function in hand,  \\n let's now call that on both of our data sets.  \\n I'm going to call that with North Ames and College Creek,  \\n passing in the same map plot lib axis for both,  \\n and it should plot them on the same plot.  \\n I got an error, it says map plot lib is not defined.  \\n I ran into this because I restarted my code space  \\n and my map plot lib library was not installed.  \\n So if I were to do this in the real world,  \\n I would come up here to the top,  \\n where I've put my imports here,  \\n and note that I don't have map plot lib here,  \\n I would come in here and say,  \\n import map plot lib dot py plot as PLT.  \\n That will make it so, in the future,  \\n when I run this, I'm not going to have that issue.  \\n Let's run this again.  \\n You can see that it prints out a series here  \\n because plot CDF returns a series.  \\n Below that we should see our plot,  \\n and we can see that these CDFs do not overlap,  \\n giving us further evidence  \\n that these do not have the same distribution.  \\n In this lesson,  \\n we looked at comparing distributions by looking at plots.  \\n We looked at histograms  \\n and continuous distribution functions.  \\n We saw that,  \\n for these two neighborhoods that we're looking at,  \\n these don't appear to overlap,  \\n suggesting that they are not the same distribution.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4507088\",\"duration\":264,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Running statistical tests\",\"fileName\":\"4433355_en_US_04_03_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"By performing statistical tests, you can understand if two data sets have the same distribution.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12107398,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson,  \\n we're going to use the scipy.stats module  \\n to run a statistical test to give us a numeric value  \\n as to whether these two distributions are the same.  \\n Imported that scipy.stats module up above.  \\n Scipy is a collection of different utilities  \\n for manipulating numerical data.  \\n One of those modules is the stats library.  \\n Let's just look and see what's in there.  \\n There are a bunch of things that you can do  \\n with this stats module.  \\n One of them is to run statistical tests,  \\n another one is to create distributions,  \\n there are also some plotting capabilities as well.  \\n In this case, we're going to run this ks_2samp function.  \\n Let's pull up the documentation for that.  \\n It says, it performs a two sample Kolmogorv-Smirnov test  \\n for goodness of fit.  \\n Basically what this is doing is it's testing  \\n whether these two distributions are the same.  \\n Let's look at what we need to pass into it.  \\n We need to pass in data1 and data2  \\n and we already have those two series,  \\n so we should be good with that.  \\n What does this return?  \\n This says it returns a p-value.  \\n I'm going to view as a scrollable element here  \\n and let's scroll this up.  \\n This says, it returns an object containing two attributes,  \\n the KS statistic and a p-value as a result.  \\n And if we scroll down a little bit,  \\n we should see some examples.  \\n One of the nice things  \\n about a lot of the Python numeric libraries  \\n such as Pandas, Matplotlib, Scikit-learn and Scipy  \\n is they have really good documentation.  \\n You can see an example of using it right here.  \\n We just create the two distributions  \\n and then we call ks_2samp with our two samples  \\n and this gives us this KstestResult.  \\n You can see that the first part here is a statistic  \\n and the next part is a p-value.  \\n In this case, we're mostly interested with the p-value.  \\n You can see that says, if the p-value is lower  \\n than a threshold of 0.05, we reject the null hypothesis  \\n in favor of the two-sided alternative,  \\n which is the data were not drawn from the same distribution.  \\n Here's another example.  \\n You can see that these two samples, the p-value is 0.5  \\n and so because the p-value is 0.5,  \\n it's not below that threshold.  \\n We cannot reject the null hypothesis  \\n or we fail to reject that.  \\n The null hypothesis in this case would be  \\n that the two samples were from the same distribution,  \\n so we cannot reject that,  \\n in essence saying that we cannot prove  \\n that these were not from the same distribution,  \\n i.e. we think they are from the same distribution.  \\n Let's do this with our data now.  \\n I'm just going to pass in n_ames and college_cr into that  \\n and I'll print out the results there.  \\n We have the statistic and the p-value.  \\n The p-value is very low.  \\n Down here I have an IF statement  \\n that lets us check the p-value.  \\n If the p-value is greater than 0.05,  \\n we fail to reject the null hypothesis,  \\n i.e. these two are from the same distribution.  \\n If it is less than 0.05, we reject the null hypothesis,  \\n meaning that they are not from the same distribution.  \\n In this case, this is a very small number,  \\n it is certainly less than 0.05  \\n and we are going to reject the null hypothesis  \\n that these two data sets are from a different distribution.  \\n This confirms what we saw  \\n with both our statistical summaries  \\n and our visualizations of these two distributions.  \\n In this lesson, we looked at using the scipy.stats library  \\n to do a statistical test to compare  \\n whether two samples were from the same distribution.  \\n The scipy.stats Library makes this really easy  \\n to run statistical tests such as these  \\n and you can leverage the scipy.stats library  \\n to confirm what you see when you do visualizations  \\n or summary statistics.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4508089\",\"duration\":268,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Testing for normality\",\"fileName\":\"4433355_en_US_04_04_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"The goal of this video is to test whether a set of values has a normal distribution.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9502184,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In this lesson we're going to use  \\n the SciPi library to test  \\n whether a distribution is normal.  \\n We're also going to create a visualization  \\n to let us evaluate that visually as well.  \\n Typical test for testing for normality  \\n is called the Shapiro-Wilks Test,  \\n and it turns out that SciPi stats module  \\n has a function that will do that.  \\n And similar to the function  \\n that we looked at in the previous lesson,  \\n this will give us back a statistic and a P value.  \\n So let's see if northern aims is normal,  \\n We're going to run that  \\n and then we'll look at the P value there.  \\n It says that the distribution  \\n of the series is likely not normal,  \\n so presumably that P value was low enough,  \\n so we are going to reject the hypothesis  \\n which is that it is a normal distribution.  \\n Just print out that P value so you can see what it is,  \\n and it is indeed a small number.  \\n Another way to evaluate normality is to  \\n use a probability plot.  \\n It turns out that this module has that capability as well.  \\n You can pull up the documentation if you want to.  \\n The key thing here is that you pass in the data  \\n and I'm also going to pass in a parameter  \\n for this plot to tell us to use map plot lib  \\n to plot that.  \\n Here's my code.  \\n I am going to create a figure in an axis using map plot lib  \\n and then I'm going to call probplot and pass in the axes.  \\n Here's our probability plot.  \\n Let me explain how we interpret this.  \\n We don't have a hard numeric value to say  \\n whether or not something is normal.  \\n By looking at this, this is a visual interpretation.  \\n We would like to see the blue dots track that red line,  \\n and if they do track that red line,  \\n that would be a strong indication that this is normal.  \\n You can see that it looks like it's overlapping the red line  \\n at a few points,  \\n but it's a little bit above on the small end.  \\n Then it goes below the red line  \\n and then it goes back up above the red line.  \\n From looking at this probability plot,  \\n I would say that this is not normal.  \\n It's close to normal on the left hand side,  \\n but on the right hand side it is not.  \\n Let me just plot a histogram of this so we can see what  \\n that histogram looks like and understand what's going on  \\n with this probability plot.  \\n Again, we're going to look at the north aims blue histogram,  \\n and we said that the left hand side of this  \\n looks somewhat normal,  \\n but the right hand side does not.  \\n Indeed, the right hand side has a tail where it's dragging  \\n out a little bit, and you can see that that tail  \\n is represented by this going up and above.  \\n Now, on the left hand side, it's going above as well.  \\n That indicates that it's actually a little bit short here.  \\n We would like to see that drag out a little bit more,  \\n so if this had a long tail  \\n on the left hand side,  \\n we would actually see these blue lines  \\n go below that red line.  \\n So that's the interpretation there.  \\n Let's actually repeat this with the College Creek data.  \\n This doesn't look normal either.  \\n Let's look at what the probability plot looks like for that.  \\n So here's the probability plot for our College Creek.  \\n You can see that it's quite a bit above  \\n on the left-hand side.  \\n What that indicates is that it's not tailing off  \\n as much as we would like for it to be normal.  \\n You can see that it then goes below here.  \\n I am interpreting that as this bump right here.  \\n We would like that to go up a little bit more,  \\n but it's not, looks like along the middle there we're okay.  \\n Then we're going back down below,  \\n and then it's going back up above,  \\n indicating that this tail is a little bit too long.  \\n In this lesson,  \\n we looked at the ability to do a probability plot.  \\n A probability plot is a visualization that lets us  \\n understand whether our data looks normal.  \\n It does not have a numeric yes or no answer,  \\n but this is somewhat fuzzy.  \\n We want to see the blue dots tracking that red line,  \\n and I showed you how you can interpret when  \\n these values are off by comparing that to the histogram.  \\n This is a nice tool to have in your tool belt  \\n if you need to check if your data is normal.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4507087\",\"duration\":20,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Checking square footage distributions\",\"fileName\":\"4433355_en_US_04_05_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":601364,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat electronic music)  \\n - [Instructor] In this challenge,  \\n you're going to get a chance  \\n to play around with statistic tests.  \\n What I want you to do is get the first floor square footage  \\n from North Ames and College Creek  \\n and compare those distributions.  \\n Are they the same or not?  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:4508088\",\"duration\":267,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Checking square footage distributions\",\"fileName\":\"4433355_en_US_04_06_XR30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8992336,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" (upbeat music)  \\n - [Instructor] Let's look at a solution for this challenge.  \\n The first thing that I want to do  \\n is I want to get the square footage  \\n from both of these neighborhoods.  \\n Let's use our pandas code to do that.  \\n Say from housing, let's do a query  \\n and I'm going to say neighborhood is equal to,  \\n and I'll put in double quotes here  \\n because my string has a single quote and NAmes.  \\n Close my double quote there, close my single quote  \\n and then I need another parentheses at the end for my chain.  \\n Let's run that and just scroll down  \\n and look at how many rows.  \\n Okay, there's 400 rows, so it looks  \\n like this is getting the Northern Ames  \\n or North Ames neighborhood.  \\n Let's pull off the first floor square footage from that.  \\n I'm going to use .loc to do that.  \\n So remember, loc takes a row selector.  \\n I'm going to take all of the rows from this  \\n and then a column selector.  \\n Now, if I want to get a series from this,  \\n I need to just put in a scaler value from this.  \\n That should give me a series.  \\n Let's run that and make sure.  \\n Okay, there's my series.  \\n If I wanted to get a data frame  \\n with just one dimension in that, I could put in a list  \\n of the columns and just have a single column in there  \\n and that should give me a data frame.  \\n Okay, so there's my data frame.  \\n I don't want a data frame. I want a series.  \\n That looks like that's pretty good.  \\n Okay, so this is going to be my NAmes  \\n and then I'll call this _sf.  \\n Okay, now I'm going to just copy this  \\n and I'm going to leverage this  \\n for making the College Creek value here.  \\n Let's change this to college_cr,  \\n and I'm going to come down here and change NAmes to CollgCr.  \\n Let's run that and see if it works.  \\n It looks like it does work.  \\n Let's just look at the output of that.  \\n Okay, it looks like that did work as well.  \\n Okay, now I have my two datasets here.  \\n I want to see if these distributions are the same.  \\n So before I run my statistical test,  \\n I'm going to visualize these.  \\n So let's make a histogram to visualize these.  \\n I'm going to say ax is equal to n_ames_sf  \\n and then I'll do a hist here.  \\n There's the histogram  \\n of the square footage of the first floor.  \\n And let's plot on top of that  \\n my College Creek square footage.  \\n I'm going to say hist, and then I'm going to say ax is equal  \\n to that ax that I just created from North Ames.  \\n I'm going to get something that looks like this.  \\n In this case, I can see that I don't really need to play  \\n with the alpha here, but I can tweak it a little bit.  \\n I'll just change it to 0.7 just on College Creek.  \\n These do not look like the same distribution  \\n but let's quantify that using our summary statistics.  \\n Okay, so we're going to use the Kolmogorov-Smirnov test  \\n and we're going to get a statistic from that  \\n and we're going to get a P-value from that.  \\n And we're going to say stats.  \\n And this is the ks_2sample  \\n and we're going to pass in n_ames_sf  \\n and College Creek sf  \\n and we'll just print the P-value from this.  \\n Okay, remember, our null hypothesis  \\n is that the values have the same distribution.  \\n The P-value is less than 0.05, giving us an indication  \\n that these do not have the same distribution.  \\n In this lesson, we looked  \\n at how we can apply a Kolmogorov-Smirnov test  \\n to two samples.  \\n We also verified that by looking  \\n at the histogram and understanding  \\n that these two distributions graphically also did not look  \\n like they were the same distribution.  \\n \\n\\n\"}],\"name\":\"4. Hypothesis Tests\",\"size\":49863766,\"urn\":\"urn:li:learningContentChapter:4503103\"},{\"duration\":78,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:4503095\",\"duration\":78,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Next steps\",\"fileName\":\"4433355_en_US_05_01_LA30\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5259035,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - Congratulations on completing  \\n the Statistics with Python course.  \\n Now that you've acquired a solid statistics foundation,  \\n you might wonder what are the next steps?  \\n I highly recommend practicing the concepts  \\n and techniques covered in this course.  \\n Like any skill, the more you practice,  \\n the more proficient you become.  \\n Consider working on improving your data manipulation skills.  \\n As we've seen, many libraries are easy to use  \\n if the data is in the appropriate format.  \\n One valuable resource for this is my book,  \\n \\\"Effective Pandas.\\\"  \\n This book provides comprehensive examples  \\n and snippets for data manipulation using Pandas.  \\n It offers real world examples as well  \\n as exercises to reinforce your learning.  \\n Finally, apply the knowledge and techniques you gained  \\n in this course to your own projects and data sets.  \\n Whether you're analyzing survey data,  \\n conducting AB testing,  \\n or exploring customer behavior,  \\n using statistics and Python will enable  \\n you to extract valuable insights  \\n and make data-driven decisions.  \\n Completing a project will impress your boss  \\n and is also helpful  \\n Thanks for completing this course  \\n and I wish you continued success  \\n in your statistical journey.  \\n \\n\\n\"}],\"name\":\"Conclusion\",\"size\":5259035,\"urn\":\"urn:li:learningContentChapter:4503104\"}],\"size\":379622141,\"duration\":9565,\"zeroBased\":false},{\"course_title\":\"Machine Learning with Scikit-Learn\",\"course_admin_id\":2861087,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":2861087,\"Project ID\":null,\"Course Name\":\"Machine Learning with Scikit-Learn\",\"Course Name EN\":\"Machine Learning with Scikit-Learn\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"The ability to apply machine learning algorithms is an important part of a data scientist\u00e2\u20ac\u2122s skill set. scikit-learn is a popular open-source Python library that offers user-friendly and efficient versions of common machine learning algorithms. In this course, data scientist Michael Galarnyk explains how to use scikit-learn for supervised and unsupervised machine learning. Michael reviews the benefits of this easy-to-use API and then quickly segues to practical techniques, starting with linear and logistic regression, decision trees, and random forest models. In chapter three, he covers unsupervised learning techniques such as K-means clustering and principal component analysis (PCA). Plus, learn how to create scikit-learn pipelines to make your code cleaner and more resilient to bugs. By the end of the course, you'll be able to understand the strengths and weaknesses of each scikit-learn algorithm and build better, more efficient machine learning models.&lt;br&gt;&lt;br&gt;This course was created by &lt;a href=http://www.onlymadecraft.com target=_blank&gt;Madecraft&lt;/a&gt;. We are pleased to host this content in our library.&lt;br&gt;&lt;br&gt;&lt;img src=https://media.licdn.com/media/AAYAAgCwAAoAAQAAAAAAAHppnBQxgeyWS2CsU3aDDPcMgw.jpg height=25% width=25%&gt;\",\"Course Short Description\":\"Learn to use scikit-learn, the popular open-source Python library, to build efficient machine learning models.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":\"21401000, 20517019\",\"Instructor Name\":\"Madecraft Licensor, Michael Galarnyk\",\"Instructor Transliterated Name\":\",\",\"Instructor Short Bio\":\"Full-Service Learning Content Company|Python Instructor and Blogger\",\"Author Payment Category\":\"LICENSED, NONE\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2020-10-15T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"No\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/machine-learning-with-scikit-learn\",\"Series\":\"Essential Training\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Advanced\",\"LI Level EN\":\"Advanced\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Artificial Intelligence for Technology\",\"Primary Software\":\"scikit-learn\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"Yes\",\"Visible Duration\":2637.0,\"Visible Video Count\":22.0,\"Contract Type\":\"LICENSED, NO_CONTRACT\"},\"sections\":[{\"duration\":0,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2366781\",\"duration\":55,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Get started with scikit-learn\",\"fileName\":\"2861087_04_01_XR30_GetstartedwithScikitLearn\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2337588,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Michael] Congratulations.  \\n I've hope you found this course useful  \\n in your machine learning journey.  \\n As you probably know, your learning doesn't stop here.  \\n Machine learning is an ever-expanding topic,  \\n and there are a lot of resources available  \\n that can help you keep learning.  \\n I'd like to offer a few resources on your journey.  \\n First, you can always check out Scikit-Learn's  \\n documentation to learn more about other algorithms.  \\n Next, if you want to learn about deep learning,  \\n you can check out deep learning frameworks  \\n like TensorFlow or PyTorch.  \\n Also, be sure to check out my blog on Medium.  \\n If you're looking for a data science job,  \\n I encourage you to check out my blog post titled  \\n \\\"How to Build a Data Science Portfolio\\\"  \\n to help you on your way.  \\n And finally, stay in touch.  \\n You can follow me on Twitter @GalarnykMichael  \\n and connect with me on LinkedIn.  \\n I'd love to hear from you.  \\n So that's it.  \\n Thanks again for watching this course.  \\n Now, get out there and take advantage of your data  \\n to create machine learning models.  \\n Good luck.  \\n \\n\\n\"}],\"name\":\"Conclusion\",\"size\":0,\"urn\":\"urn:li:learningContentChapter:2353859\"},{\"duration\":0,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2368043\",\"duration\":54,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Effective machine learning with scikit-learn\",\"fileName\":\"2861087_00_01_WX30_EffectivemachinelearningwithScikitLearn\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1786660,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - Machine learning is transforming industries  \\n and it's an exciting time to be in the field.  \\n A large amount of machine learning programs are written  \\n using open source Python library, Scikit-learn.  \\n Scikit-learn provides an easy to use streamlined  \\n API that provides efficient versions  \\n of a large number of common algorithms.  \\n And it makes it easy to train models.  \\n My name is Michael Galarnyk.  \\n I'm a data scientist, a machine learning instructor,  \\n and a blogger about all things data science.  \\n I'm also a big fan of Scikit-learn.  \\n In this course, I'll show you how to use several  \\n machine learning algorithms and when they're appropriate.  \\n I'll share with you how you can tune your models  \\n to better predict unseen data.  \\n So not only make your models better,  \\n but also help you understand the strengths  \\n and weaknesses of each algorithm.  \\n By the end of the course, you'll feel confident  \\n and ready to build your own powerful machine learning models  \\n using Scikit-learn.  \\n So if you're ready to dive in, then let's go.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2367237\",\"duration\":34,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What you should know before you start\",\"fileName\":\"2861087_00_02_XR30_Whatyoushouldknowbeforeyoustart\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"Get the most out of this course. In this video, discover what background knowledge would be beneficial to have before starting this course. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":964736,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] There are a few things I'd like you  \\n to be aware of as we get started.  \\n First, you should understand basic Python data structures,  \\n such as lists, tuples, and dictionaries.  \\n Additionally, you should have knowledge of Python libraries  \\n such as pandas and matplotlib.  \\n If you'd like to know more about these libraries,  \\n you can check out my course, Python for Data Visualization.  \\n Don't worry if you feel your background knowledge  \\n could be better.  \\n Throughout the course,  \\n Regardless of your background,  \\n you'll still be able to follow the course  \\n and learn how to create machine learning algorithms  \\n with scikit-learn.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2367238\",\"duration\":21,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Using the exercise files\",\"fileName\":\"2861087_00_03_XR30_Usingtheexercisefiles\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"The course includes a folder called Exercise_Files. In this video, learn how to utilize the course files and code along with the videos. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":778209,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] To learn the most from this course  \\n and to help you follow along,  \\n I've included a ZIP file of the contents of this course.  \\n You can find these included  \\n as the exercise files for this course.  \\n Once you've unzipped it, you'll see this folder here.  \\n If you click here,  \\n you can see the contents  \\n of a particular chapter of the course.  \\n If this sounds great to you,  \\n you can open the different notebooks and follow along.  \\n \\n\\n\"}],\"name\":\"Introduction\",\"size\":0,\"urn\":\"urn:li:learningContentChapter:2366782\"},{\"duration\":0,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2367239\",\"duration\":81,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"What is machine learning?\",\"fileName\":\"2861087_01_01_XR30_Whatismachinelearning\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Machine learning is an exciting new technology. In this video, learn why machine learning is useful and how it can solve problems that are normally too difficult or tedious for humans to solve.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2220700,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Narrator] What is machine learning and why is it useful?  \\n You can think of machine learning  \\n as a field of study that gives computers the ability  \\n to learn from data without being explicitly programmed.  \\n Here's an example of why machine learning is useful.  \\n Say you have 150 flowers.  \\n You're interested in determining which flowers are  \\n which flower species based on a feature  \\n like their petal length.  \\n There are three different flower species:  \\n Iris Setosa, Iris Versicolor, and Iris Virginica.  \\n From analyzing the data, you can mainly write rules  \\n to separate out the flowers from each other.  \\n The first rule could separate  \\n out the Setosa from the other species.  \\n From there, you can make another rule to try  \\n and separate out the Versicolor from the Virginica.  \\n You're going to keep on writing rules  \\n until you completely separate  \\n out the flower species from each other.  \\n For something more complicated with more features,  \\n your program could have a very long list of complex rules.  \\n This manual writing of rules might even seem feasible  \\n for some applications like flower classification.  \\n What if your data changes faster  \\n than you can update the rules?  \\n Can this approach work for more complicated problems  \\n like image or speech recognition?  \\n It turns out that these are the sort of problems  \\n where humans don't necessarily have a good understanding  \\n of how to solve the problem with rule-based approaches.  \\n This is where machine learning approaches shine.  \\n Machine learning can extract structure from data  \\n and solve problems that are normally too difficult  \\n or tedious for humans to solve.  \\n So that's it.  \\n Machine learning is an efficient means  \\n of building models from data.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2367240\",\"duration\":67,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Why use scikit-learn for machine learning?\",\"fileName\":\"2861087_01_02_XR30_WhyuseScikitLearnformachinelearning\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Scikit-learn is a free library software used by many data scientists. In this video, learn why the Python library scikit-learn is a great library for machine learning due to its large number of efficient machine learning algorithms and an easy to use API. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1800963,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] With a host of machine learning tools  \\n and frameworks out there why should you scikit-learn?  \\n Scikit-learn is a very popular  \\n open source machine learning project  \\n which is constantly being developed and improved.  \\n In this video, I'll share with you  \\n some of the major advantages of scikit-learn.  \\n First scikit-learn provides a large number  \\n of machine learning algorithms.  \\n This is important as programming machine learning algorithms  \\n from scratch is not easy task.  \\n Most models in scikit-learn  \\n also have reasonable default values for hyper-parameters.  \\n This means that a machine learning algorithm  \\n might work well with little tuning.  \\n Next, scikit-learn has a clean uniform, and streamlined API.  \\n Scikit-learn also works well with other Python libraries,  \\n such as NumPy, Pandas, and Mathplotlib.  \\n Also, once you understand the basic use in syntax  \\n of scikit-learn for one model,  \\n switching to another is very straightforward.  \\n Finally, the advantage of the library being popular  \\n cannot be overlooked.  \\n It's widely used in academia and industry.  \\n There are always more scikit-learn tutorials being written.  \\n This also means it's easier to get  \\n your questions answered on stack overflow.  \\n So that's it.  \\n Due to its easy to use and popularity,  \\n scikit-learn is a great library for machine learning.  \\n \\n\\n\"}],\"name\":\"1. Input and Loading Data\",\"size\":0,\"urn\":\"urn:li:learningContentChapter:2353858\"},{\"duration\":0,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2367241\",\"duration\":54,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"What is supervised learning?\",\"fileName\":\"2861087_02_01_XR30_Whatissupervisedlearning\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"Supervised learning is a common type of machine learning. In this video, learn how to set an algorithm with a matrix and a target vector to make predictions. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1445274,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] The most common form of machine learning  \\n is supervised learning.  \\n In Scikit-Learn, a supervised learning algorithm learns  \\n a relationship between your features matrix  \\n and your target factor.  \\n A feature is a measurable property.  \\n A target is typically what you want to make predictions for.  \\n Once a model learns a relationship between a features matrix  \\n and a target factor,  \\n it can make predictions for unseen or future data.  \\n Supervised learning can generally be thought of  \\n to solve two different types of tasks.  \\n The first is when you try to predict a continuous value.  \\n This is considered a regression problem.  \\n This means that your target factor contains  \\n continuous qualities like home prices.  \\n The second is when you're trying  \\n to predict a categorical value.  \\n This is considered a classification problem.  \\n This means that your target factor contains  \\n categorical values like different flower species.  \\n So that's it.  \\n Supervised learning is when an algorithm learns  \\n from a features matrix and target factor  \\n to make predictions.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2367242\",\"duration\":115,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"How to format data for scikit-learn\",\"fileName\":\"2861087_02_02_XR30_HowtoformatdataforScikitLearn\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Formatting is extremely important when working with data. In this video, learn how to format your data so that it is recognizable input for the Python library scikit-learn.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3945761,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - Scikit-learn is a great library for creating machine  \\n learning models from data.  \\n Before you fit a model using scikit-learn,  \\n your data has to be in a recognizable format.  \\n Scikit-learn works well with numeric data  \\n that's stored in numpy arrays.  \\n Additionally, you can convert your data from objects  \\n like pandas dataframes to numpy arrays.  \\n In this video, I'll show you how you can  \\n make your data a more acceptable input for scikit-learning.  \\n The first thing you have to understand is what scikit-learn  \\n expects for features matrices and target vectors.  \\n In scikit-learn, a features matrix is a  \\n two dimensional grid of data where rows  \\n represent samples and columns represent features.  \\n A target vector is usually one dimensional  \\n and in the case of supervised learning,  \\n what you want to predict from the data.  \\n Let's now see an example of this.  \\n The image is a pandas dataframe of the  \\n first five rows of the iris dataset.  \\n A single flower represents one row of the dataset  \\n and the flower measurements are the columns.  \\n In this dataset, the species column  \\n is what you're trying to predict.  \\n Let's now go over an example of how to make your data  \\n a more acceptable format.  \\n The first thing you have to do is import the libraries.  \\n In this case, you'll import matplotlib,  \\n pandas, as well as the Iris dataset.  \\n This code loads the Iris dataset into a pandas dataframe.  \\n From here, you can try to arrange your data  \\n into a features matrix and target vector.  \\n This is a multiple column panda's dataframe,  \\n which will then be converted into a numpy array.  \\n You can do this by using the values attribute.  \\n One important thing to do is to make sure your  \\n numpy array is two dimensional.  \\n This is the first dimension,  \\n and this is the second dimension.  \\n This piece of code is a panda series  \\n that you'll then convert to a numpy array.  \\n You can do this by using the values attribute.  \\n Notice that this is one dimensional.  \\n This is okay, as target vectors can be one dimensional.  \\n So that's it.  \\n Scikit-learn expects data in a particular format.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2367243\",\"duration\":272,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Linear regression using scikit-learn\",\"fileName\":\"2861087_02_03_XR30_LinearregressionusingScikitLearn\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Linear regressions are common models in data science. In this video, learn how to build and tune a linear regression model using the Python library scikit-learn. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10740831,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] How do you create a complex model  \\n using scikit-learn?  \\n An easy solution is to start with a simple model  \\n like linear regression and go from there.  \\n In this image,  \\n we see a best fit line for a bunch of points.  \\n In this video,  \\n I'll show you how you can create a linear regression model  \\n using scikit-learn.  \\n So that more complex models will be easier to create.  \\n The first thing you have to do is import the libraries  \\n that you want to use.  \\n In this case Matplotlib, Pandas,  \\n train_test_split,  \\n as well as the model LinearRegression.  \\n From there, you need to load a dataset.  \\n This particular data set  \\n shows that scikit-learn requires data  \\n to be free of missing values.  \\n The goal of this dataset  \\n is to use the feature column X  \\n to predict the target column Y.  \\n Notice it looks like we have a missing value here.  \\n This is really important.  \\n As in scikit-learn,  \\n you can't have missing values input into a model.  \\n The next step is to remove or impute values.  \\n If you want to build models with your data,  \\n null values are almost never allowed.  \\n It's important to always see how many samples  \\n have missing values,  \\n and for which columns.  \\n Let's start by looking at the shape of the data frame.  \\n There are 102 rows and two columns.  \\n The next thing is to see how many missing values  \\n there are in each column.  \\n Notice that there's eight missing values  \\n for the column Y.  \\n You can either remove rows where there's a missing value,  \\n or you can fill in missing values.  \\n The option used in this notebook  \\n is to remove rows with missing values.  \\n To do this,  \\n And look, there's no more missing values.  \\n Notice though,  \\n that the shape of the dataframe is different.  \\n Before we have 102 samples.  \\n And now we only have 94.  \\n From here,  \\n you can arrange your data into a features matrix  \\n and target vector.  \\n This code converts the X column to a Numpy array.  \\n Notice that this features matrix is two dimensional.  \\n You have dimension one,  \\n and dimension two.  \\n This is really important  \\n as to input something into scikit-learn,  \\n your features matrix needs to be two-dimensional.  \\n What this code is doing is,  \\n is it's create a target vector.  \\n Notice that this target vector is one-dimensional.  \\n Dimension one, and there's no dimension two.  \\n Let's now create a linear regression model  \\n using scikit-learn.  \\n The first step is to import the model  \\n that you want to use.  \\n In this case,  \\n it was already imported earlier in the notebook.  \\n So this is not necessary.  \\n From here,  \\n This is a place we tune the hyperparameters of a model.  \\n In the case of linear regression,  \\n you can set fit_intercept to true or false.  \\n This is a really important concept.  \\n As more complex models have a lot more you can tune.  \\n In the case of linear regression,  \\n this is about it.  \\n On the left,  \\n you have an image of a model where fit_intercept  \\n was equal to true.  \\n On the right,  \\n you have an image of a model  \\n where fit_intercept was equal to false.  \\n Here's the code to make a instance of the model.  \\n Note that fit_intercept in this case is equal to true.  \\n From here,  \\n you can use the fit method to train the model on the data.  \\n What's happening the model's learning the relationship  \\n between X and Y.  \\n From here,  \\n you can predict values of new data.  \\n The code here is predicting for one observation.  \\n The code here is predicting for multiple.  \\n With any model it's important  \\n to try to measure model performance.  \\n For regression tasks,  \\n one metric is R squared.  \\n Which is implemented by the score method.  \\n This model had a score of roughly .98.  \\n Which is actually pretty good.  \\n Scikit-learn also allows you  \\n to find the equation of the line.  \\n You can do this after you'd make an instance of a model,  \\n and then fit it on your data.  \\n The attribute coaf is essentially your slope.  \\n The intercept is your intercept.  \\n And here's the equation of the line.  \\n This next graph is plotting  \\n the best fit linear regression line.  \\n This next section is just showing  \\n how changing a single hyperparameter value  \\n can have a drastic impact on your model performance.  \\n By looking at these graphs,  \\n it's clear that fit_intercept equal to true  \\n makes a much better model.  \\n As the R squared is 0.98.  \\n Whereas where fit_intercept is equal to false,  \\n the R squared is 0.86.  \\n So that's it.  \\n I encourage you to create a linear regression model  \\n using scikit-learn.  \\n I hope you have a better understanding of how it works.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2366777\",\"duration\":113,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Train test split\",\"fileName\":\"2861087_02_04_XR30_Traintestsplit\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"A train test split is critical in testing your models. In this video, learn how to perform the train test split procedure using the Python library scikit-learn so you can simulate how well your model performs on new data. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4555737,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] The goal of machine learning  \\n is it build a model that performs well on new data.  \\n If you have new data,  \\n you can see how well your model performs on it.  \\n The problem is that you may not have new data,  \\n but you can simulate this experience  \\n with Scikit Learn's train test split.  \\n In this video, I'll show you how train test split works  \\n in Scikit Learn.  \\n The first thing that you need to know  \\n is what is train test split?  \\n Here's how that procedure works.  \\n The first step is to split your data into two pieces,  \\n a training set and a testing set.  \\n Typically, about 75% of the data goes to your training set  \\n and about 25% of your data goes to the test set.  \\n The second step of the process  \\n is to train the model on the training set.  \\n The final step is to test the model on the testing set  \\n and evaluate the performance.  \\n To do this in Scikit Learn,  \\n you first have to import libraries.  \\n The next step is to load a dataset.  \\n The dataset using this notebook  \\n is the Boston House Price dataset.  \\n The goal of this dataset is predict house prices  \\n based on features like number of rooms.  \\n The next step is to create a features matrix,  \\n as well as the target vector.  \\n From here, you can create a train test split.  \\n The colors in the image indicate which variable,  \\n X_train, X_test, Y_train, and Y_test,  \\n the data from the data frame derive from two  \\n for a particular train test split.  \\n Notice that roughly 75% of the data went to the training set  \\n and roughly 25% went to the test set.  \\n From here, you can utilize a machine learning model.  \\n In this case, it's linear regression.  \\n The final part is to measure model performance.  \\n By measuring model performance on the test set,  \\n you can estimate how well your model is likely to perform  \\n on new data.  \\n To do this, you can use a score method,  \\n just make sure that your inputs are your test set.  \\n So that's it.  \\n Train test split helps you simulate  \\n how well a model would perform on new data.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2367244\",\"duration\":235,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Logistic regression using scikit-learn\",\"fileName\":\"2861087_02_05_XR30_LogisticregressionusingScikitLearn\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"You can create logistic regression models in a number of ways. In this video, learn how to create a logistic regression model using the Python library scikit-learn and learn how to visualize the predictions for your model using Matplotlib.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8590508,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] How do you create a logistic regression model  \\n using scikit-learn?  \\n The first thing that you need to know  \\n is that despite the name logistic regression  \\n contain the word regression,  \\n logistic regression is actually a model user classification.  \\n Classification models can be used for tasks  \\n like classifying flower species or image recognition.  \\n All of this of course depends on the availability  \\n and quality of your data.  \\n Logistic regression has some advantages,  \\n model training and predictions are relatively fast,  \\n additionally, no tuning is usually needed for the model.  \\n Finally, it can perform well  \\n with a small number of observations.  \\n In this video, I'll share with you  \\n how you can create a logistic regression model  \\n for binary classification.  \\n The first thing that you need to do  \\n is import the libraries that you want to use.  \\n In this notebook, it's Matplotlib, numpy, seaborn, pandas,  \\n as well as train_test_split,  \\n StandardScaler, and LogisticRegression.  \\n While it may seem like a lot of libraries,  \\n a lot of model building involves oftentimes data processing,  \\n splitting your data up into training and test sets,  \\n and actually applying the model itself.  \\n The code below loads, a modified version  \\n of the Iris dataset, which has two classes,  \\n a one in the dataset is a virginica flower  \\n and a zero is a versicolor flower.  \\n From here, you can split the data  \\n into training and test sets.  \\n A really important part of this process  \\n is to standardize your data.  \\n Logistic regression is affected by scale,  \\n so you need to scale your features onto unit scale  \\n for optimal performance.  \\n Unit scale means having a mean of zero  \\n and a variance of one for your features.  \\n You can utilize scikit-learn's  \\n standard scaler to accomplish this.  \\n Note that you fit on the training set  \\n and transform on the training and test sets.  \\n From here, you could import and use the model.  \\n Logistic regression was already imported  \\n at top of the notebook, so this step is commented out.  \\n from here, you can make an instance of the model.  \\n This is normally a place  \\n where you can tune the hyperparameters of a model.  \\n From here, You can train the model on the data.  \\n What this means is the models learn the relationship  \\n between your X, in other words,  \\n your sepal width, sepal height, et cetera,  \\n and your Y, which are the flower species.  \\n Finally, you can predict labels of new data.  \\n What this code is showing is a prediction  \\n for one flower sample.  \\n Their prediction was zero.  \\n The reason why it was zero and not one  \\n is by looking at the line below.  \\n The probability of a zero, according to the model was 0.52,  \\n the probability of one was 0.47.  \\n If it happened to be greater than or equal to 0.5,  \\n it would have predicted a one.  \\n If this is unclear, let's visualize how logistic regression  \\n makes predictions by looking at our test data.  \\n If you don't know what this code is doing, don't worry,  \\n it's all about the visualization.  \\n What you can see in the graph  \\n is the probability of virginica  \\n given different petal lengths.  \\n When the probability is greater than or equal to 0.5,  \\n everything in this area is classified as virginica  \\n even when it may not be.  \\n If you look at these two blue points here,  \\n even though they're classified as virginica,  \\n they're misclassified, they're actually versicolor flowers.  \\n Everything below this threshold is classified as versicolor.  \\n An important part of a machine learning model  \\n is measuring its model performance.  \\n The code here utilizes accuracy as a metric,  \\n which is simply the fraction of correct predictions.  \\n Accuracy is one metric,  \\n but it doesn't give much say into what went wrong.  \\n Let's take a look at a confusion matrix.  \\n There are a couple of things to notice  \\n in this confusion matrix,  \\n the first is that it correctly predicted versicolor  \\n 10 times when it was actually versicolor,  \\n additionally, it correctly predicted virginica  \\n when was actually virginica,  \\n however, it incorrectly predicted virginica,  \\n when it was actually versicolor,  \\n these points were misclassified.  \\n So that's it.  \\n I encourage you to create a logistic regression model.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2367245\",\"duration\":216,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Logistic regression for multiclass classification\",\"fileName\":\"2861087_02_06_XR30_Logisticregressionformulticlassclassification\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Modeling multiclass classifications are common in data science. In this video, learn how to create a logistic regression model for multiclass classification using the Python library scikit-learn. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7754717,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] A lot of classification models  \\n like logistic regression  \\n were originally designed for binary classification.  \\n That's predicting whether something's one thing or another.  \\n For datasets with more than two classes,  \\n what do you do?  \\n For multi-class classification problems,  \\n one approach is to split the task  \\n into multiple binary classification datasets,  \\n and fit a binary classification model on each.  \\n In this video, we'll explore the one-vs-rest strategy  \\n and how you can apply it  \\n to logistic regression using scikit-learn.  \\n One-vs-rest, which is also sometimes called one-vs-all  \\n is a technique that extends binary classifiers  \\n to multi-class problems.  \\n Here's how it works.  \\n You train one classifier per class  \\n where one class is treated as the positive class.  \\n And the other classes are considered negative classes.  \\n For example, say you have an image recognition task.  \\n Your dataset has four classes  \\n the digits zero, one, two, and three.  \\n Your goal is to classify them.  \\n Using the one-vs-rest approach, you break down the task  \\n into four binary classification problems.  \\n Classification problem one is digit zero  \\n versus digits one, two, and three.  \\n Prom two is digit one versus digits, zero, two and three.  \\n And so on.  \\n From there, if you want to classify a new sample  \\n you'd use each of the classifiers.  \\n The model that predicts the highest class probability  \\n is the predicted class.  \\n Let's now see an example of this.  \\n The first thing you have to do is import the libraries  \\n that you want to use.  \\n In this case this notebook uses Matplotlib, pandas  \\n train-test-split, StandardScaler  \\n and of course, logistic regression.  \\n The dataset used in this notebook is a modified version  \\n of the digits dataset  \\n which is arranged into a CSV file for your convenience.  \\n The data consists of pixel intensity values  \\n for 720 images that are Eight by Eight pixels.  \\n Each image is labeled the number from zero to four.  \\n This code here loads into a pandas DataFrame.  \\n Before you create a machine learning model  \\n it's often a good idea to try  \\n to understand your data better.  \\n A great way to do this is to visualize your data.  \\n This code shows a sample of each individual digit  \\n as you can see, the images are rather low resolution.  \\n They're Eight by Eight pixels.  \\n From here, you can split your data  \\n into training and test sets.  \\n Logistic regressions are affected by scale.  \\n So you need to scale your features before using it.  \\n You can transform a data onto unit scale  \\n by utilizing scikit-learn's StandardScaler.  \\n This code creates an instance  \\n of multi-class logistic regression.  \\n Once you try to model,  \\n I encourage you to look at the attributes of a model.  \\n In particular for this model,  \\n I encourage you to look at the intercept and the shape.  \\n One thing to notice here  \\n is that we have four intercepts here.  \\n This is because we had a multi-class problem.  \\n We had binary classifier one, two, three, and four.  \\n The last thing I want you to look  \\n at is how the predictions are made.  \\n If you look at the predictive probabilities  \\n for this one simple digit,  \\n you notice that we have four different probabilities.  \\n We have the first one, the second, the third and the fourth.  \\n This is for class zero, class one, two and three.  \\n Notice that the predicted probability for class one is .98.  \\n What's going to happen here  \\n is it's going to predict class one  \\n because it has the highest probability.  \\n So that's it.  \\n I encourage you to use a logistic regression  \\n for multi-class classification.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2368044\",\"duration\":189,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Decision trees using scikit-learn\",\"fileName\":\"2861087_02_07_XR30_DecisiontreesusingScikitLearn\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"Decision trees are a great way to visualize your findings. In this video, learn how to create and tune a decision tree model using the Python library scikit-learn.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7559006,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor ] One of the most important considerations  \\n when choosing a machine learning algorithm  \\n is how interpretable it is.  \\n The ability to explain how an algorithm makes predictions  \\n is useful not only to you,  \\n but also to potential stakeholders.  \\n A very interpretable machine learning algorithm  \\n is a decision tree.  \\n You can think of it as a series of questions,  \\n designed to assign a class or predict a continuous value  \\n depending on the task.  \\n Example image is a decision tree,  \\n designed for classification.  \\n So you have a flower with the following feature,  \\n petal length of 4.5 centimeters.  \\n The way decision trees work  \\n is you start at the top of the tree  \\n and ask questions until you reach these leafy green nodes.  \\n You first has the question is 4.5 centimeters less than  \\n or equal to 2.45?  \\n This is false, so going this other node.  \\n Is 4.5 centimeters less than or equal to 4.95?  \\n This is true, so you'll end up with a leaf node.  \\n Leaf nodes our predictions are assigned.  \\n In this leaf node, there were 38 versicolor and virginica.  \\n Their prediction for this leaf node is versicolor,  \\n as is the majority class.  \\n In this video, I'll share with you  \\n how you can create intuitive decision tree  \\n using Scikit-learn.  \\n The first thing you have to do  \\n is import the libraries that you're going to use.  \\n In this case, you'll import Matplotlib, pandas,  \\n the Iris data set, as well as train-test split,  \\n and decision tree classifier.  \\n This next piece of code loads the Iris data set.  \\n From here, you can split your data  \\n into training and test sets.  \\n What this image shows,  \\n is which variable the data from the day from df one, two  \\n for particular train test split.  \\n This is a really important step  \\n as oftentimes this decision trees overfit the training set.  \\n Train test split will help you avoid that.  \\n It's also important to note  \\n that another benefit of decision trees  \\n is that you don't have to standardize your features.  \\n This is different from other algorithms  \\n like logistic regression, and K nearest neighbors.  \\n From here, you can create a decision tree model.  \\n This model is already imported earlier in notebook  \\n so it's commented out.  \\n The next step is to make an instance of your model.  \\n This is normally a place  \\n where you can tune the hyper parameters of the model.  \\n The code below constraints the model  \\n to have at most a depth of two.  \\n Tree depth is a measure of how many splits it makes  \\n before coming to a prediction.  \\n It's important to note that max_depth  \\n is not always equal to depth.  \\n Max_depth is simply something that pre-prunes a tree  \\n to only grow at most discerned depth.  \\n From here, you can train your model.  \\n I can also make predictions.  \\n You can also measure your model performance.  \\n This notebook uses accuracy as the metric,  \\n which is a fraction of correct predictions.  \\n This section shows how to tune max_depth.  \\n If you look at the graph,  \\n you'll see a couple of things.  \\n The first is that accuracy increases  \\n up to a certain max depth.  \\n There could be a couple reasons for this.  \\n One potential reason  \\n is that max_depth is not necessarily equal to depth.  \\n It's possible that trees  \\n with max_depth four and five have the same depth.  \\n It could also be that after a certain point,  \\n the models not getting any more useful information  \\n after a certain depth.  \\n So that's it,  \\n I encourage you to create a decision tree of your own.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2366778\",\"duration\":125,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"How to visualize decision trees using Matplotlib\",\"fileName\":\"2861087_02_08_XR30_HowtovisualizedecisiontreesusingMatplotlib\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"There are a number of ways to model a decision tree. In this video, learn how to make a visualization based on a decision tree model using the Python libraries scikit-learn and Matplotlib. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4763789,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] How do you understand  \\n how decision tree makes predictions?  \\n One of the strengths of decision trees,  \\n are they're relatively easy to interpret,  \\n as you can make a visualization based on your model.  \\n This is not only a powerful way to understand your model,  \\n but also to communicate how a model works as stakeholders.  \\n In this video, I'll show you how decision trees,  \\n can be plotted with Matplotlid .  \\n The first thing you have to do, is import libraries.  \\n Take note that you're also importing tree.  \\n This is what actually plots to the decision tree.  \\n The next step is a loaded dataset,  \\n in this case is the Iris dataset.  \\n From there, you can split  \\n your data into training and test sets.  \\n This is really important for decision trees, as they tend  \\n to be a high variance algorithm.  \\n What this means, is they tend to overfit  \\n on the training set.  \\n The next step is to create a decision tree model.  \\n Before you can make a visualization based  \\n on a decision tree, you need to make a decision tree first.  \\n So you make an instance of your model,  \\n you train the model on the data,  \\n you can also make predictions and measure model performance.  \\n After you fit a decision tree, you can make a visualization,  \\n based on the model, to do this,  \\n you can utilize tree.plot_tree, with the instance  \\n of a fit model.  \\n This is not a perfect visualization,  \\n there're a couple of reasons why.  \\n The first, it seems a bit small, so let's try to fix that.  \\n This code makes the figure size a bit bigger,  \\n as well as the DPI.  \\n The problem with this image,  \\n is even though the visualization is bigger,  \\n it's still hard to understand what's going on.  \\n The next step is to make the tree more interpretable.  \\n There're a couple of ways to do this,  \\n what you can do, is utilize the feature name parameter,  \\n as well as the class name parameter.  \\n This tree is more interpretable,  \\n there're a couple of reasons why.  \\n First, it's easier to understand the leaf node predictions,  \\n this leaf node predicts the setosa, this one versicolor,  \\n and this one virginica, additionally,  \\n this visualization is more visually appealing, so that's it,  \\n you can visualize decision trees using Matplotlid.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2368045\",\"duration\":120,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Bagged trees using scikit-learn\",\"fileName\":\"2861087_02_09_XR30_BaggedtreesusingScikitLearn\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Bagged trees, or a bagging regressor, are common models in data science. In this video, learn how to create and tune a bagging regressor model using the Python library scikit-learn.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4739896,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Each machine learning algorithm  \\n has strengths and weaknesses.  \\n A weakness of decision trees is that they're prone  \\n over fitting on the training set.  \\n A way to mitigate this problem,  \\n is to constraint how large a tree can grow.  \\n Bagged trees try to overcome this weakness  \\n by using bootstrapped data,  \\n to grow multiple deep decision trees.  \\n The idea is that matrix protect each other  \\n from individual weaknesses.  \\n What this image shows is that multiple decision trees  \\n come together to make a combined prediction.  \\n In this video,  \\n I'll share with you how you can build a Bagged Tree Model.  \\n The first step is to Import Libraries.  \\n The Dataset used in this notebook  \\n is a housing prices for King County.  \\n The code below loads the dataset.  \\n The goal of this dataset is to predict house prices  \\n based on features like number of bedrooms and bathrooms.  \\n This notebook only selects a small subset  \\n of the features for simplicity.  \\n However, if you have time I encourage you to play  \\n with this notebook,  \\n and add and subtract features.  \\n This code is just arranging the dataset  \\n into a features matrix,  \\n and target vector.  \\n From here you can Split the Data into  \\n Training and Test Sets.  \\n The next step is to build a Bagged Trees model.  \\n You import the model that you want to use.  \\n This is actually commented out because it was  \\n used earlier in the notebook.  \\n From here you can make an instance of the Model.  \\n Note that n_estimators is how many  \\n decision trees are coming together  \\n to make a prediction.  \\n Next you can train the model on the data,  \\n as well as make predictions.  \\n You can also measure model performance.  \\n For a Bagged Trees Regressor.  \\n You can use a metric R squared.  \\n The code below tunes and estimators,  \\n which in this case is the number of decision trees.  \\n This code can take some time to run.  \\n As you have multiple estimators coming together  \\n to make a prediction.  \\n In the graph notice that their score starts improving  \\n after a certain number of estimators.  \\n There could be a couple reasons for this.  \\n One potential way to get a better score,  \\n would be to include more features  \\n in the features matrix.  \\n So that's it.  \\n I encourage you to try building up Bagged Trees model.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2368046\",\"duration\":161,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Random forests using scikit-learn\",\"fileName\":\"2861087_02_10_XR30_RandomforestsusingScikitLearn\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"The random forest models are the construction of a multitude of decision trees. In this video, learn how to create a random forest model using the Python library scikit-learn as well as visualize individual trees from random forest models.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6695222,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Each machine learning algorithm  \\n has strengths and weaknesses.  \\n Bagged tree models use many trees  \\n to protect individual decision trees from overfitting.  \\n However, bagged tree models are not without weaknesses.  \\n Suppose you have one very strong feature in a data set,  \\n most of the trees will use that feature as the top split.  \\n This will result in many similar trees.  \\n You can think of random forest  \\n as a variant of a bagged tree model.  \\n The difference is that each time a split's considered,  \\n only a portion of the total number of features  \\n are split candidates.  \\n In short, random forests make the individual decision trees  \\n less correlated  \\n In this video, I'll share with you  \\n how you can build a random forest model using Scikit-Learn.  \\n The first step is to import libraries.  \\n The next step is to load a dataset.  \\n This dataset contains house sale prices for King County.  \\n The code below loads the dataset.  \\n The goal of this dataset is to predict house prices  \\n based on features like number of bedrooms and bathrooms.  \\n The code in this notebook only selects five features  \\n to make a prediction.  \\n This notebook only selects a couple of features  \\n for simplicity.  \\n However, I encourage you to play with adding  \\n and subtracting features.  \\n You can now create a features matrix and a target factor.  \\n The next step is to split your data  \\n into training and test sets.  \\n From here, you can build a random forest model.  \\n After importing the model,  \\n you can make an instance of the model.  \\n This is a place where you can tune hyperparameters.  \\n In the case of random forest, you have N_estimators.  \\n What N_estimators is, is how many decision trees you have  \\n coming together to make a prediction.  \\n From here, you can train your model and make predictions.  \\n It's important to note,  \\n that the training of a random forest model  \\n can take some time.  \\n In this case you have 100 estimators  \\n that need to be trained  \\n and come together to make a prediction.  \\n The next step is to measure your model's performance.  \\n In this case, the score method uses R squared as a metric.  \\n Since bagged tree and random forest models  \\n are ensemble models,  \\n you can visualize individual decision trees  \\n comprising those models.  \\n Here's the first decision tree  \\n of 100-estimator bagged tree model.  \\n You can also visualize individual decision trees  \\n for a random forest model.  \\n One of the benefits of using a random forest model  \\n is that they can give you a feature importance metric.  \\n Feature importance metrics can give you an idea  \\n of what features were important  \\n in making predictions for your model.  \\n In this example, the random forest is suggesting  \\n that pedal length was the most important feature.  \\n So that's it.  \\n I encourage you to build a random forest model.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2367246\",\"duration\":83,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Which machine learning model should you use?\",\"fileName\":\"2861087_02_11_XR30_WhichmachinelearningmodelshouldIuse\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"There are tons of machine learning models that you can use to visualize your data. In this video, learn how to determine which method is best used for which desired outcome. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3252642,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] With so many machine learning  \\n algorithms available from scikit-learn,  \\n which algorithm should you choose?  \\n Selecting a good enough model from among a large number  \\n of possible machine learning models is one  \\n of the hardest parts of machine learning.  \\n Some algorithms are better suited  \\n to different types of data and problems.  \\n Luckily, a quick answer to model selection  \\n with scikit-learn is, use the algorithm cheat sheet.  \\n It's meant to give you a rough guide  \\n in how to choose an algorithm.  \\n From the start point,  \\n you first ask, do you have more than 50 samples?  \\n From there, you keep on answering questions  \\n until you get an idea of what you should try.  \\n If you don't use the cheat sheet,  \\n here are a few things to consider when choosing a model.  \\n The first thing is a problem you're trying to solve.  \\n For example, if you have a supervised learning problem,  \\n figuring out if you're trying to predict a continuous  \\n or categorical value can be an important first step.  \\n Next, always consider the size,  \\n quality, and structure of your data.  \\n There's no machine learning without data.  \\n You should also consider the strengths and weaknesses  \\n of each algorithm you're considering.  \\n It's especially important,  \\n as some algorithms take longer to make predictions.  \\n Also, more complex models  \\n are often more difficult to maintain.  \\n Finally, consider the urgency of a task.  \\n Some models take longer to train and tune.  \\n Now, if you're feeling a little shaky,  \\n you're not quite sure which model to choose,  \\n don't worry, you've got this.  \\n I now encourage you to try a couple models  \\n and learn from the process.  \\n \\n\\n\"}],\"name\":\"2. Supervised Learning\",\"size\":0,\"urn\":\"urn:li:learningContentChapter:2368049\"},{\"duration\":0,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:2353857\",\"duration\":75,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"What is unsupervised learning?\",\"fileName\":\"2861087_03_01_XR30_Whatisunsupervisedlearning\",\"demo\":true,\"videoCreationMetadata\":null,\"description\":\"Unsupervised learning relies more on artificial intelligence and less on human intelligence. In this video, learn how to determine when unsupervised learning is most useful. \",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1992763,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] In machine learning,  \\n you aren't always tryna predict the value.  \\n Sometimes your goal is to find some structure  \\n in your dataset.  \\n Unsupervised learning is when you train an algorithm  \\n without giving it the answers for examples in your dataset.  \\n In the context of psychic learn,  \\n this means that you only provide a features matrix  \\n when you fit your algorithm.  \\n A features matrix is a two-dimensional grid of data  \\n where rows represent samples and columns represent features.  \\n Unlike supervised learning, there's no target factor.  \\n It's important to emphasize that unsupervised algorithms  \\n don't make predictions from the data.  \\n There are two common types  \\n of unsupervised learning algorithms.  \\n The first is clustering.  \\n Clustering is often used to discover natural groupings  \\n in a dataset, when common use is for market segmentation.  \\n Companies often have large amounts of customer information.  \\n By clustering customers into different segments,  \\n they can more efficiently sell  \\n or market to their customers.  \\n Another common type of unsupervised learning  \\n is dimensionality reduction.  \\n You can think of dimensionality reduction techniques  \\n as data compression algorithms.  \\n They can make your data take up less space on your computer.  \\n Having less features in your data can make visualizing  \\n your data easier as well speed up the fitting  \\n of your machine learning algorithms.  \\n So that's it.  \\n Unsupervised learning helps  \\n you discover structure in your data.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2366779\",\"duration\":148,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"K-means clustering\",\"fileName\":\"2861087_03_02_XR30_Kmeansclustering\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"K-means clustering is a method of vector quantization. In this video, learn how to create a K-Means model using the Python library scikit-learn to find some structure in your data.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5560618,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Clustering algorithms  \\n have identified this thing groups of data.  \\n One example is who's clustering the group customers  \\n based on their behavior.  \\n There are so many clustering algorithms.  \\n But the most commonly used algorithm is K-Means.  \\n In this video, I'll show you how to use K-Means Clustering  \\n to find some underlying structure in your data.  \\n The first step is to import libraries.  \\n The next step is to load a dataset.  \\n This notebook uses the Iris data set.  \\n From there, you can arrange your data  \\n into a features matrix.  \\n It's important to note that K-Means  \\n is considered unsupervised learning algorithm.  \\n This means that you only need a features matrix.  \\n In the Iris data set, there are four features.  \\n In this notebook, the features matrix  \\n will only be two features,  \\n as it's easier to visualize clusters in two dimensions.  \\n It's important to mention  \\n that you do not eat a target factor,  \\n as this is an unsupervised learning algorithm.  \\n Like a lot of different algorithms,  \\n K-Means is sensitive to the scale of your data.  \\n To standardize your data,  \\n you can use Scikit-Learns StandardScaler  \\n to help standardize your features.  \\n Before you cluster your data, it's often a good idea  \\n to try to understand your data better.  \\n If your data is two or three dimensional,  \\n it's a good idea to at least try to visualize your data.  \\n Hopefully you can see if there's a natural looking clusters.  \\n From this graph,  \\n it looks like there's at least two clusters.  \\n You can now apply K-Means Clustering.  \\n In K-Means clustering,  \\n you can specify the number of clusters you want.  \\n In Scikit-Learn, this parameter is called N-Clusters.  \\n In the case of the code below,  \\n the number of clusters is at three,  \\n because most people who use this data set  \\n happen to know that there are three species.  \\n After you fit your model on the features matrix,  \\n you can get the cluster labels  \\n as well as the cluster centroids.  \\n Let's now visually evaluate the clusters.  \\n Here are your three clusters.  \\n You have cluster one, cluster two and cluster three.  \\n Let's now see how well instead,  \\n compared to the Iris data sets labels.  \\n There are a couple things you notice  \\n when you look at the two graphs.  \\n On the left, you have the K-Means clustering graph,  \\n on the right you have the flower species.  \\n They actually look pretty similar.  \\n It looks at K-Means picked up some flower differences  \\n with only two features and not the labels.  \\n The colors are different in the two graphs,  \\n simply because K-Means is on arbitrary cluster number,  \\n and the Iris data set said has an arbitrary number  \\n in the target column.  \\n So that's it.  \\n K-Means is a clustering algorithm  \\n that you can use to find some structure in your data.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2366780\",\"duration\":133,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Principal component analysis (PCA) for data visualization\",\"fileName\":\"2861087_03_03_XR30_PCAfordatavisualization\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Principal component analysis, PCA, is a critical tool for dimensionality reduction and visualization. In this video, learn how to perform PCA for data visualization using the Python library scikit-learn.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5022077,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Narrator] Are all the features in our dataset needed?  \\n Say you have some flowers  \\n and you measure their petal length.  \\n If you have a column of that measurement in centimeters,  \\n and another column with measurement in inches  \\n do you need both columns?  \\n In that circumstance,  \\n you could probably drop either column  \\n without losing information.  \\n In other cases dropping a column could lead to issues.  \\n Principal Component Analysis  \\n better known as PCA.  \\n Is a technique that you can use  \\n to smartly reduce the dimensionality of your dataset  \\n while losing the least amount of information possible.  \\n One use of PCA, is for data visualization.  \\n In this video,  \\n I'll share with you how you can use PCA  \\n to help visualize your data.  \\n The first step is to import libraries.  \\n From there, you can load your dataset.  \\n The dataset used in this notebook is the hours dataset.  \\n The next step is to standardize your data  \\n PCA like a lot of different algorithms is affected by scale.  \\n You can transform your data onto unit scale  \\n by using scikit-learn standard scaler.  \\n Note,  \\n that PCA is an unsupervised learning algorithm.  \\n What this means is that when you use the fit step,  \\n you only fit it on your features matrix.  \\n From here, you can apply PCA  \\n the code below projects the original data  \\n which is four dimensional into two dimensions.  \\n Note that after you applied dimensionality reduction,  \\n there usually isn't a particular meaning assigned  \\n to each principal component.  \\n The new components  \\n are just the two main dimensions of variation.  \\n The next step creates a visualization  \\n of the first two principle components.  \\n One thing to note from the graph  \\n is that the Setosa class  \\n is well separated out from the other classes.  \\n One thing to look after PCA  \\n is the explained variance.  \\n What this shows is how much variance can be attributed  \\n to each of the principal components.  \\n This is important.  \\n As well, you can convert  \\n four dimensional space to two dimensional space.  \\n You lose some of the information when you do this.  \\n You can look at how much information  \\n is attributed to each of the principle components  \\n by using the explained variance ratio.  \\n These two principle components contain roughly  \\n 96% of the information in the dataset.  \\n The first component contains roughly 73%  \\n and the second one contains roughly 23%.  \\n So that's it.PCA can be used to help visualize your data.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2368047\",\"duration\":161,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"PCA to speed up machine learning algorithms\",\"fileName\":\"2861087_03_04_XR30_PCAtospeedupmachinelearningalgorithms\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Unsupervised learning can benefit greatly from PCA. In this video, learn how to perform PCA using the Python library scikit-learn to speed up machine learning algorithms.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":6220579,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - Do you want to speed up the fitting  \\n of your machine learning algorithm?  \\n Second learn offers quite a few ways to do this.  \\n One way is to train your model in parallel using n_jobs  \\n parameter, which exists for many psychic learn models.  \\n A really simple way is to reduce the number of columns or  \\n rows in your data.  \\n The problem with this approach is it's hard to know which  \\n rows and especially which columns to remove.  \\n Principle Component Analysis,  \\n commonly known as PCA is a technique that you can use to  \\n smartly reduce the dimensionality  \\n in your data while losing the  \\n least amount of information possible.  \\n In this video,  \\n I'll share with you the process of how you can use PCA to  \\n split the fitting of a logistic regression model.  \\n The first step is to import libraries.  \\n The next step is loaded dataset.  \\n The dataset is a modified version of the MNIST dataset  \\n that contains 2000 labeled images  \\n of each digit zero and one,  \\n the images are 28 pixels by 28 pixels. For your convenience,  \\n it's been arranged into a CSV file  \\n before you apply PCA or a machine learning model.  \\n It's often a good idea to try to visualize your data.  \\n The Coppola shows a sample image of each digit zero and one.  \\n The next step, it says, put your data  \\n into training and test sets  \\n before continuing it's important to standardize your data.  \\n PCA and logistic regression  \\n are sensitive to the scale of your features.  \\n You can standardize your data onto unit scale by using a  \\n psychic learns standard scaler.  \\n This next piece of code applies PCA and logistic regression.  \\n For the code here, note that n components equals 0.9.  \\n It means it's psychic learn  \\n we'll choose a minimum number of principal components such  \\n that 90% of the variance is retained  \\n From here, you fit PCA in the training set  \\n After that, you apply the mapping to  \\n both the train set and the test set.  \\n The final step is to create a logistic regression model.  \\n When you run this code,  \\n First, the number of dimensions before PCA is 784.  \\n This is because the original images  \\n were 28 pixels by 28 pixels and 28 times 28 is 784.  \\n The number of dimensions after PCA was 104.  \\n Additionally, the classification accuracy  \\n with this model was very high.  \\n So even after removing a lot dimensions going from 784  \\n dimensions to 104, the model still worked pretty well  \\n for this particular dataset.  \\n You can look at the relationship between cumulative  \\n explained variance and number of principal components.  \\n One thing to notice in this graph is that after roughly 150  \\n to 200 dimensions, there wasn't a lot of explained variance  \\n in the remaining principal components.  \\n So that's it.  \\n PCA can be used to speed up the fitting of your algorithm.  \\n \\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:2368048\",\"duration\":125,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"scikit-learn pipelines\",\"fileName\":\"2861087_03_05_XR30_ScikitLearnpipelines\",\"demo\":false,\"videoCreationMetadata\":null,\"description\":\"Pipelines can reduce the chances of error in your code. In this video, learn how to create pipelines using the Python library scikit-learn to make your code cleaner and more resilient to bugs.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4262686,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\" - [Instructor] Machine learning  \\n is not always about applying  \\n a single machine learning algorithm.  \\n For a lot of machine learning applications,  \\n you'll need to apply various data processing steps,  \\n data transformations,  \\n and potentially multiple machine learning algorithms.  \\n This can lead to a lot of code.  \\n The question becomes, how do you keep your code organized  \\n and as bug free as possible?  \\n In this video, I'll share with you how you can use  \\n Pipelines in Scikit Learn to make your code cleaner  \\n and more resilient to bugs.  \\n To demonstrate the utility of Pipelines,  \\n this notebook shows how much less code you need  \\n to chain together PCA and logistic regression  \\n for image classification.  \\n Before getting to that though,  \\n you need to import the libraries that you're going to use.  \\n The dataset using this notebook  \\n is a modified version of the MNIST dataset  \\n that contains 2000 labeled images  \\n of each digit, zero and one.  \\n The images are 28 pixels by 28 pixels.  \\n For convenience, I arranged the data into a CSV file.  \\n This code loads the data into a panda's DataFrame.  \\n This code is a chaining together  \\n of PCA and logistic regression.  \\n It's quite a bit of code.  \\n The first step is a train_test_split.  \\n From there, there's a standardization step.  \\n Notice that there's a fit step here,  \\n as well as when you apply PCA and logistic regression.  \\n There's quite a few places where an error can occur  \\n as there's quite a bit of code.  \\n Let's now try to do this with Pipelines.  \\n The first step as before is a train_test_split.  \\n The next step is creating a Pipeline.  \\n You still data scalar, PCA and logistic regression.  \\n You can also name the steps,  \\n you have scalar, PCA and logistic.  \\n Note, you still have fit step,  \\n but unlike before you don't have three of them.  \\n Another advantage of using Pipelines  \\n is you can visualize your Pipeline.  \\n You can see all the steps your machine learning model took  \\n to get to the end.  \\n So you have a Standard Scalar,  \\n you have PCA and logistic regression.  \\n So that's it.  \\n Pipelines can make our code more organized  \\n and easier to understand.  \\n \\n\\n\"}],\"name\":\"3. Unsupervised Learning\",\"size\":0,\"urn\":\"urn:li:learningContentChapter:2367247\"}],\"size\":0,\"duration\":2637,\"zeroBased\":false},{\"course_title\":\"Applied Machine Learning: Algorithms\",\"course_admin_id\":3806104,\"metadata\":{\"Locale\":\"en_US\",\"Course ID\":3806104,\"Project ID\":null,\"Course Name\":\"Applied Machine Learning: Algorithms\",\"Course Name EN\":\"Applied Machine Learning: Algorithms\",\"Activation Status\":\"ACTIVE\",\"Display to Public\":\"Yes\",\"Display to QA\":\"No\",\"Course Description\":\"&lt;p&gt;With the growing importance of machine learning in almost every sector, professionals need a deeper understanding and practical approach to implementing ML algorithms effectively. &lt;/p&gt;&lt;p&gt;This course covers commonly used machine learning algorithms. Instructor Matt Harrison focuses on non-deep learning algorithms, covering PCA, clustering, linear and logistic regression, decision trees, random forests, and gradient boosting. &lt;/p&gt;&lt;p&gt;Join Matt in this course to understand common ML algorithms, learn their pros and cons, and develop hands-on skills to leverage them by following along with challenges and solutions in GitHub Codespaces.\",\"Course Short Description\":\"Learn about common machine learning algorithms, their pros and cons, and develop hands-on skills to leverage them.\",\"Content Type\":\"TOOLS\",\"Localization Type\":\"ORIGINAL\",\"Original Course Locale\":null,\"Original Course ID\":null,\"Equivalent English Course\":null,\"Instructor ID\":9254436,\"Instructor Name\":\"Matt Harrison\",\"Instructor Transliterated Name\":null,\"Instructor Short Bio\":\"Python and Data Science Corporate Trainer, Author, Speaker, Consultant\",\"Author Payment Category\":\"NON-LICENSED\",\"Delivery Mode\":\"ALL_AT_ONCE\",\"Series End Date\":null,\"Course Release Date\":\"2024-04-15T00:00:00\",\"Course Updated Date\":null,\"Course Archive Date\":null,\"Course Retire Date\":null,\"Replacement Course\":null,\"Has Assessment\":\"Yes\",\"Has Challenge/Solution\":\"No\",\"LIL URL\":\"https://www.linkedin.com/learning/applied-machine-learning-algorithms-23750732,https://www.linkedin.com/learning/applied-machine-learning-algorithms-revision-2024-q2\",\"Series\":\"Deep Dive (X:Y)\",\"Limited Series\":null,\"Manager Level\":\"Individual Contributor\",\"LI Level\":\"Intermediate\",\"LI Level EN\":\"Intermediate\",\"Sensitivity\":null,\"Internal Library\":\"Technology\",\"Internal Subject\":\"Artificial Intelligence for Technology\",\"Primary Software\":\"Python\",\"Media Type\":\"Video\",\"Has CEU\":\"No\",\"Has Exercise Files\":\"No\",\"Visible Duration\":7083.0,\"Visible Video Count\":32.0,\"Contract Type\":\"PERPETUAL\"},\"sections\":[{\"duration\":113,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5903617\",\"duration\":49,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Applied machine learning: Algorithms\",\"fileName\":\"3806104_en_US_00_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Editors, this is a workspace cam video\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":117,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3806205,\"solution\":false,\"welcomeContent\":true,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Do you have data that you want to make sense of?\\nMaybe you want to make predictions,\\nthen you've come to the right place.\\nMachine learning allows us to cluster data, derive insights,\\nand make predictions,\\nand it isn't just for PhDs and tech giants.\\nIn this course, you'll get hands-on practice\\nusing powerful libraries and tools like Pandas,\\nScikit-Learn, XGBoost, and Jupyter.\\nWe'll introduce the algorithms, show how to use them,\\nand then let you try them out.\\nWe'll explore clustering principle component analysis,\\nand making predictions\\nwith both classification and regression.\\n\\nI'm Matt Harrison, a corporate trainer and author.\\nI've written multiple books on Python and data science\\nand helped thousands learn these topics.\\nNow you get to learn these algorithms with me.\\nLet's get going.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5905599\",\"duration\":64,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"What you should know\",\"fileName\":\"3806104_en_US_00_02_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Editors, this is a workspace cam video. Please add lower third in the following timecodes.\\n1) 00:33 (Fundational understanding of Python)\\n2) 00:46 (Jupyter Notebook)\\n3) 01:12 (Basic statistics and algebra).\\nNo need for additional visuals, some punch in would be sufficient if necessary.\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":93,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4074438,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Before embarking on this journey,\\nthere's a few prerequisites\\nthat will help maximize your learning experience.\\nFirst, a foundational understanding of Python is important.\\nWhile you don't need to be an expert,\\nfamiliarity with basic programming constructs like\\nvariables, loops, and functions,\\nis really important for this course.\\nSecond, experience with Jupyter Notebooks\\nis really beneficial.\\nJupyter provides a web-based, interactive computing platform\\nthat enables you to write code, visualize data,\\nand share insights in a single document.\\n\\nIt's invaluable for data science\\nand machine learning projects.\\nLastly, a basic grasp of statistics\\nand linear algebra will aid your comprehension of\\nhow machine learning algorithms work under the hood.\\nUnderstanding concepts like mean, standard deviation,\\nmatrix multiplication will equip you\\nwith the skills you need\\nto grasp complex machine learning concepts.\\nWith these skills in your arsenal, you're ready\\nto dive into the world of machine learning algorithms\\nand solve real world problems.\\n\\n\"}],\"name\":\"Introduction\",\"size\":7880643,\"urn\":\"urn:li:learningContentChapter:5905603\"},{\"duration\":2097,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5904635\",\"duration\":466,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"K-means\",\"fileName\":\"3806104_en_US_01_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":591,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about the fundamental workings of the KNN algorithm and its application in classification and regression tasks.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":17297610,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this video, we're going to talk\\nabout clustering.\\nSpecifically, we're going to talk about K-means clustering.\\nAnd K-means clustering is what we call\\nan unsupervised machine learning algorithm\\nto make clusters from data.\\nThe unsupervised portion means that we are going\\nto feed data in, we're not going to provide any labels,\\nand the algorithm,\\nbased on the data that we passed in,\\nwill do some processing.\\nIn this case, it will give us cluster labels.\\nThere are other machine learning algorithms\\nthat are unsupervised,\\nwe'll look at another one called PCA.\\n\\nPCA is also unsupervised,\\nthat means it doesn't have labels that come in with it,\\nit just has the data,\\nand PCA will do dimension reduction.\\nSo the algorithm for K-means clustering is as followed.\\nWe're going to choose a k where k is the number of clusters,\\nand then what happens is the algorithm\\nwill choose three points from the data that we passed in\\nand it will go through and label the points\\nthat are closest to each of those clusters,\\nit calls the clusters centroids.\\n\\nAfter it's gone through labeling those,\\nit will recenter the centroids.\\nIt will go through everything\\nthat's labeled in cluster one.\\nIt will move the centroid to the middle of that cluster.\\nEverything for cluster two,\\nsame thing, will move the centroid.\\nAnd then after it's moved the centroid,\\nit repeats the algorithm,\\nmeaning it goes and finds the points\\nthat are closest to each centroid.\\nAnd it does that, and eventually\\nthe centroids come to a common state.\\nI'll show you an example right here.\\nSo here we're using the scikit-learn library,\\nand it has clustering in it.\\n\\nLet me walk through the code.\\nAt the top we have our imports.\\nWe're going to be using the KMeans library.\\nWe're also importing the datasets library\\nto load the Iris dataset,\\nand we're going to be using Matplotlib to do some plotting.\\nWe'll load the Iris dataset\\nand make a variable called dataset.\\nAnd then I'm going to make a variable called \\\"X\\\".\\nCapital X is a common convention\\nthat you'll see used throughout the scikit-learn library.\\nIn linear algebra,\\na capital variable is used to describe a matrix\\nor a two-dimensional group of data with rows and columns.\\n\\nAnd that's a common convention\\nthat you'll see in scikit-learn.\\nWhen we're passing our data into scikit-learn,\\nit's generally in two dimensions, rows and columns.\\nSo each row represents a sample,\\nand then you can have columns of features\\nthat describe what is in the data.\\nYou can also have a Y dataset.\\nY is used for supervised learning.\\nIn this case, the dataset that we have,\\nwhich is the Iris dataset,\\nis a classic dataset for machine learning.\\nIt's got 150 rows,\\neach row represents a different flower\\nfrom the Iris family,\\nand there are three different types of flowers:\\nVirginica, setosa, and versicolour.\\n\\nAnd there are a few features in the X variable\\nthat describe the shape and dimensions\\nof the petals and the sepals\\nof each of those 150 flowers.\\nThe next thing I'm going to do\\nis I'm going to make a list of my centroids,\\nand then I'm just going to loop over 10 times.\\nI'm going to run this algorithm I just talked about\\n10 times in this little for loop down here.\\nSo you can see at the top, I am making a model,\\nand I'm saying I want to have three clusters in here,\\nand I'm initializing the KMeans algorithm.\\n\\nThen I'm going to call model.fit.\\nOne of the nice things about scikit-learn,\\nand we'll be using it throughout this course,\\nis that it has a consistent interface.\\nSo once you learn that interface,\\nit's really easy to use.\\nUnsupervised learning algorithms will use fit,\\nand you'll just pass in X along with that.\\nFor supervised learning algorithms,\\nwhich we'll see later,\\nyou will call fit with both X and Y.\\nThe next thing we're going to do\\nis we're going to predict our labels.\\nAnd because this is a prediction algorithm,\\nit can make predictions,\\nwe're going to pass in our X,\\nand it will give us out a label for each row.\\n\\nAgain, scikit-learn uses the same interface\\nall over the place.\\nWe'll see when we do classification of regression models,\\nthey will also have a predict method.\\nThis is really nice because when you understand\\nthe basics of scikit-learn,\\nit uses a consistent interface all over the place\\nand it will make your life a lot easier.\\nThe next line, we're going to create\\na Matplotlib figure and axis,\\nand then I'm going to plot,\\non top of that, a scatter plot.\\n\\nI'll also pull out the cluster centers.\\nYou can see that I'm saying model.cluster_centers_.\\nOne thing to note in scikit-learn\\nis that attributes ending in underscore\\nare learned when we call fit.\\nSo when we call fit up above,\\nit determined where those cluster centers are.\\nI'm also going to plot those as a star on my plot.\\nAnd I'm going to make a title for my plot,\\nit's called \\\"iteration\\\", what iteration I'm on.\\nAnd then I'm also going to,\\nif my i, if my round is greater than zero,\\nI'm going to plot the previous centroids.\\n\\nAnd then at the end of my for loop,\\nI'll just keep track of my centroids.\\nSo what this is going to show us\\nis that as we move through the algorithm,\\nthe centroids will move along as well.\\nAnd then at the end, outside of the for loop,\\nI will just plot the original data,\\nso you can see that compared to what's going on here.\\nLet's run this.\\nIf you're not familiar with using VS Code and Notebooks,\\nall you have to do to run this\\nis you can hit this little triangle up here.\\n\\nI like to just hold down Control and hit Enter.\\nYou can see at the top\\nthat we've got a little bar indicating that it is running,\\nand then we'll scroll down here.\\nOkay, so it looks like it just ran.\\nAnd here's our plots.\\nWe should have a series of plots.\\nHere's the first iteration.\\nYou can see that I have three stars here\\nand we have green labels, we have purple labels,\\nand then we have yellow labels.\\nSo this is the first iteration.\\nAfter we've done this iteration,\\nwhat's going to happen is the star will move,\\nit will center inside of the labels.\\n\\nAnd so you can see that in the next iteration,\\nyou can see that the stars have migrated a little bit,\\nand you can also see that the boundaries\\nbetween those different clusters have moved as well.\\nHere's another iteration.\\nYou can see that we're just slowly shifting\\nthose centroids a little bit.\\nAnd I'll just scroll down to the bottom here.\\nYou can run this on your own\\nand look at what's happening at the individual levels.\\nBut here is the original data.\\nAnd in the original data,\\nthis is colored by the target.\\n\\nNote that I did not include the target in my data\\nthat I passed into the algorithm,\\nI just passed in the dimensions.\\nBut here I have the dimensions,\\nand I'm labeling these\\nby the type of Iris that it is,\\nremember there's three different types there.\\nAnd you can see on the upper left, there's one type,\\nin the middle, we have that green type,\\nand there's some overlap there\\nwith the yellow type on the right.\\nHowever, if you look up above here,\\nour clustering algorithm did a decent job.\\nIt's not perfect, but it did seem\\nto do a decent job getting the upper left group classified.\\n\\nAnd it's not able to do\\nthe overlapping that we see at the bottom,\\nbut it does a decent job.\\nOkay, and this video I gave an introduction\\nto the K-means clustering algorithm.\\nThis is a unsupervised algorithm\\nthat you can pass in data in.\\nYou can tell how many clusters you want,\\nand it will return labels for pieces of data\\nthat are in the same cluster.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5904636\",\"duration\":490,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"K evaluation\",\"fileName\":\"3806104_en_US_01_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":626,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to master the technique of selecting the optimal number of neighbors (K) for improved model performance.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":16929638,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In the previous video,\\nwe introduced the K-means Clustering Algorithm.\\nAnd one of the things we said, when you run this algorithm,\\nyou need to provide a K or a number of clusters.\\nYou might wonder, \\\"How do I get those clusters?\\nWhat is the number of clusters?\\\"\\nSo in this video, I'm going to show you a few mechanisms\\nfor determining what might be a good value for K.\\nOftentimes, we don't know that ahead of time,\\nso we want to determine that.\\nHere's the basic thing that we're going to do.\\nWe're going to try different values of K\\nand keep track of some metric and see which value of K\\ngives us the best clustering for given metric.\\n\\nAnd specifically, I'm going to show you two mechanisms here.\\nOne is called the Elbow Method,\\nwe're going to track what's called the Inertia,\\nand the other one is called the Silhouette Coefficient.\\nBoth of these are a little bit different.\\nLet's walk through the Elbow Method first.\\nSo the Elbow Method, the idea here is when we run K-means,\\nK-means has an attribute called Inertia_\\nyou can see in my little FOR LOOP here,\\nI'm keeping track of the Inertias,\\nand I'm pulling it off of km.inertia_.\\n\\nRemember that underscore,\\nany attribute ending in underscore,\\nis learned by doing fit.\\nSo we're looping over different values of K here,\\nand we're keeping track of those Inertias.\\nAnd then what I'm going to do is I'm just\\ngoing to plot those.\\nSo we'll let those run.\\nAnd while that's running, I'll explain what the Inertia is.\\nSo the Inertia Score is the sum of the square\\nof the distances from the centroid to each point.\\nSo if each point is very close to the center,\\nthose squared distances will be small.\\n\\nAs they get bigger,\\nthat means that your clusters are more spread about.\\nSo here's our plot here.\\nYou can see along the bottom,\\nwe have the number of clusters,\\nand on the left hand side we have the Inertia.\\nNow, there's kind of an art and science\\nto interpreting this.\\nWe call this the Elbow Method,\\nbecause oftentimes, these plots kind of look like a bent arm\\nwith the elbow in the lower left.\\nThe idea here is as you're going along,\\nif you're losing a lot of Inertia, that's great.\\n\\nBasically that means that your clusters are getting tighter.\\nAt some point, you don't have as many gains\\nwhen you add more clusters.\\nAnd so what you want to do is when you stop getting\\na lot of gains, you choose to stop adding more clusters.\\nI think of this as we want to make our model\\nas simple as possible.\\nSo if I can have a model with fewer clusters,\\nthat's a little bit simpler than a model\\nwith a lot of clusters.\\nAlternatively, you would take an angled ruler\\nand move it along\\ntowards the right.\\n\\nAnd when you tap the corner or the elbow,\\nthat's kind of a good spot to stop.\\nIn this case, we might think that somewhere,\\nif this ruler or shape is going along in this direction\\nand we shift it to the right,\\nit's going to tap probably somewhere here,\\naround three or four.\\nYou can see that as we go from one cluster to two clusters,\\nwe have a lot tighter clustering.\\nEven as we go to three, it gets better,\\nand then it starts to give diminishing returns.\\n\\nI'm going to show now the Silhouette Visualizer.\\nThis comes from a library called, Yellow Brick.\\nIf you've installed the requirements\\nthat are included in the Repository, you should have that.\\nOne thing to be aware of is if you're inside of Jupyter,\\nyou can put a question mark out or something,\\nand pull up the documentation.\\nIt turns out that most of these libraries\\nwe're going to look at have pretty good documentation.\\nSo this visualization will create a visualization\\nof the Silhouette Coefficient.\\nThis is a score between negative one and one.\\n\\nPoints that have a score of one\\nindicate that they are very close\\nto the center of the cluster.\\nPoints that are negative one are samples that are far away.\\nLet me show you an example and it might make more sense.\\nI'm also going to tell matplotlib about some plots,\\nso it doesn't complain so much here.\\nSo that's what these cells are doing.\\nOkay, so we're going to, in this case, just loop over up\\nto zero to five clusters\\nand we're going to make these Silhouette Plots here.\\n\\nThis plot takes a little bit of explanation\\nto understand what's going on here.\\nAlong the bottom, we have that Silhouette Score.\\nAgain, that goes from negative one to one.\\nThe closer it is to one, the better it is.\\nAnd why are these kind of shaped like knives?\\nWell, what's going on here is on the left\\nit says Cluster Label, but we actually have,\\nyou can think of this as a slice,\\nand each individual point here is a slice in the cluster.\\n\\nSo at the top of this green one, there's a thin slice,\\na thin horizontal slice,\\nand that would be the point that's probably closest\\nto the center of the cluster.\\nAnd then we're going to put each individual point\\nas a slice below that,\\nas they're getting further away from the center.\\nWhat you want to see with these clusters\\nis that they are closer to one\\nand that they sort of bulge out.\\nIf the cluster was a rectangle that came straight down\\nand over and the score was at one, these would be points\\nthat all overlapped in the middle of the cluster.\\n\\nThis is a pretty good clustering.\\nIt looks like we're having a pretty good shape.\\nGenerally, you'll see a shape that looks kind of like this.\\nIt looks kind of like a knife or a butter knife, maybe.\\nWe also see this red dashed line in the middle.\\nThat's the Average Silhouette Score.\\nSo that's another way to interpret this.\\nGenerally, the school of thought is you want these knives\\nor plots to be poking through that average.\\nIf a cluster is not coming up to the average,\\nthat might indicate that it's bad clustering.\\n\\nOkay, so that is with two clusters.\\nLet's look at three.\\nAnd you can see when we go to three,\\nour average score goes down a little bit.\\nAnd you can also see that the shape of our clusters\\nare more pointy.\\nWe actually don't want them to be more pointy.\\nWe actually like them\\nto bulge out like they do in the two clusters up here.\\nAnd if you think about it from our previous plot\\nthat we did in the last video, there's really a strong case\\nto be made for two clusters,\\nbecause there's that cluster in the upper left\\nand then there's the overlapping cluster in the bottom.\\n\\nLet's look at four points.\\nAnd we can see that the average for four points,\\nthe average score goes down.\\nYou can also see that these are getting more pointy,\\nand again, we really don't want them to be pointy.\\nWe want them to be more bulgy.\\nHere's with five points, again, the average is going down.\\nYou can see at the bottom left of this blue one,\\nwe've got some negative scores.\\nMistake, that would indicate that we have outliers\\nor points that are far away from the center of the cluster.\\n\\nAnd then you can see as we add six,\\nit actually goes back up a little bit, the average score.\\nAnd here's seven, it goes back up a little bit for that.\\nSo one of the hard things with clustering\\nis that there aren't any hard answers.\\nIt's a little bit fuzzy.\\nIf I were looking at the Silhouette Plot,\\nI would choose two clusters for this.\\nIf I was looking at our Inertia Score,\\nI would probably choose three or four clusters.\\nIn this video, we discussed how we can determine the number\\nof K to choose for clustering.\\n\\nAnd again, this is a little fuzzy.\\nSo one of the things I like to tell Data Scientists\\nis that you need to be able to explain and justify\\nyour reasonings for choosing what you did.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5903618\",\"duration\":479,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Understanding clusters\",\"fileName\":\"3806104_en_US_01_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":527,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Gain insight into how KNN clusters data points and the underlying patterns in the dataset.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":18009512,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this video,\\nwe're going to look at understanding our cluster results.\\nOne of the key things for someone who's working with data\\nis to be able to communicate their results,\\nand clustering is great that we can make clusters,\\nbut often what's even more important is to explain\\nwhat those clusters represent.\\nSo there's a couple mechanisms that we can use.\\nSome of the more popular ones would be\\nto create what's called a surrogate model.\\nThat's a model that can predict things,\\nand so what we'll do is we'll take our data\\nand we'll have it predict our labels\\nor train it to predict labels\\nand then explain why it predicted those labels.\\n\\nAnother thing that you can do is you can summarize\\nthe clusters just using some Pandas code.\\nYou can also visualize the clustering results\\nin two or three dimensions.\\nI'm not going to be doing the visualization here\\nbecause the dataset that we're going to be using\\nhas a few more than two dimensions.\\nBut when we talk about PCA,\\nyou could use PCA to reduce your dimensions\\ndown to two or 3D,\\nand that would be a great way to explain or visualize that.\\nNow I'm going to be using this datasets library.\\nMake sure you install that if you haven't already.\\n\\nWe're going to load some electricity usage data from Australia\\nusing that datasets library.\\nOnce we've done that,\\nyou can see we have this electricity object.\\nThis looks kind of like a dictionary,\\nso we'll pull off the train value from that\\nand I'll just look at what's in there.\\nI'm not super familiar with the interface here\\nbecause it's not a common one,\\nbut if you look in here,\\nyou can see that we can say to_pandas here.\\nSo I'm going to say let's make a Pandas data frame from that.\\nOkay, now we're in a good place,\\ngenerally when we're using the Scikit-learn library,\\nwe like to use Pandas.\\n\\nNow on the right hand side,\\nwe can see that there is a class.\\nIf you're trying to predict things with this,\\nyou might want to predict whether that goes up or down.\\nIn this case, we're not doing that prediction,\\nso I'm actually going to drop that class\\nand I'm going to make my X without that.\\nOkay, so again, I'm going to make some clusters\\nand try and explain what's going on.\\nI don't even know how many clusters to make.\\nSo I'm going to go through my process here.\\nI'll just loop over from zero to 19\\nand see what that looks like with my inertias.\\n\\nAnd if I'm going to use this mechanism again,\\nremember I'm going to have like a angled line here\\nand I'm going to move it over probably somewhere\\naround five is the number\\nof clusters I would choose for this.\\nLet's do our silhouette plot visualization here,\\nand I'll kick that off.\\nOne thing to realize is that I'm sampling the data here.\\nI'm not using all of it.\\nIf I used all of the data,\\nit would take a little bit longer to run\\nto create these plots.\\nSo let's look at this. Okay, here's two clusters.\\n\\nI'm just going to come through\\nand look at my average scores here and the shape of these,\\nthat looks okay.\\nIf I do three, you can see my average goes up,\\nso that looks better.\\nFour, my average goes up a little bit more.\\nFive, my average goes up.\\nWell, it's probably a little bit less than four.\\nAnd then six and seven,\\nit looks like it's starting to go back down.\\nOkay, so from this I'd probably choose four,\\nbut based on my other one, I'd probably choose five.\\nSo I'm going to just say five here.\\n\\nIf you've got multiple clusters\\nthat look like they're similar,\\ngenerally I will take the smaller one.\\nIn this case, I'm feeling pretty good about five.\\nSo let's make a model with five clusters.\\nSo we're going to start N clusters or a K to five.\\nWe're going to call fit with that,\\nand then I'll get my labels here.\\nI'm going to explain this clustering\\nusing some Pandas code down here.\\nSo I'm just summarizing this.\\nLet me kind of walk through this\\nif you're not familiar with the Pandas code.\\nI'm going to assume that you have\\nsome basic familiarity with Pandas.\\n\\nIf not, my recommendation is\\nto check out my book, \\\"Effective Pandas,\\\"\\nbut there are other resources for Pandas as well.\\nSo here's my data frame, elec,\\nand I'm going to say let's stick on a new column\\ncalled cluster.\\nThis is the predictions,\\nand then I'm going to group by that and aggregate.\\nI'm going to take the mean of the numeric columns\\nfor each cluster.\\nSo you can see over here in the index we have\\neach cluster label and then we have the mean values.\\nSo this is pretty good right here.\\nI just want to add some style to it\\nto make it a little bit easier to understand.\\n\\nSo I'm going to transpose it first of all,\\nso that T does a transpose,\\nit flips the rows and the columns,\\nand then I'm going to use the style background gradient\\nto color each row from red to blue.\\nRed things will be negative, blue things will be positive.\\nThis makes it pretty clear that cluster four\\ntends to be the positive things,\\nthis is a blue cluster,\\nand probably cluster two and zero\\ntend to be negative things.\\nCluster one and three are probably\\nsomewhere in between them.\\nSo if I knew what each of these features was like period,\\nnswprice, nswdemand, et cetera,\\nI could say that cluster zero is low values for that,\\ncluster four is high values for that\\nand come up with a description for that.\\n\\nAnother thing I can do is make a surrogate model.\\nSo I'm going to use Scikit-learn to do that.\\nHere I'm going to use a decision tree classifier.\\nWe'll talk about decision trees later on,\\nbut one of the nice things about decision trees\\nis that they can give an explanation\\nof why they made their predictions.\\nWhat I'm going to do is train my decision tree,\\nand you can see I'm calling fit here.\\nRemember that Scikit-learn has a consistent interface.\\nA decision tree is a supervised learning algorithm,\\nso we need to pass in in addition to X\\na label or Y generally.\\n\\nIn this case, I'm just going to pass in my predictions\\nfrom my clustering algorithm,\\nand then I'm going to use the mechanisms from Scikit-learn\\nto plot what's going on there.\\nWe're going to plot the decision tree, and here we go.\\nSo this is a decision tree that explains our clustering.\\nLet me walk through what's going on here.\\nThe first thing that we're going to do\\nis we're going to look at our date\\nand it's going to say, okay, if your date is less than 0.6,\\nwe're going to go down to this side.\\n\\nIf it's greater than that value,\\nwe're going to go down this right side.\\nSo date is pretty important\\nfor determining which label a given row has,\\nand we're going to look at period.\\nAnd if period is less than some value,\\nwe're going to go down here.\\nIf it's greater than, we're going to go down here,\\nand you can keep going down this recursively on each side.\\nYou can see over here,\\nclass four is things that have high dates, high periods,\\nand high demand, and that makes sense.\\n\\nClass four was that one that was really high.\\nAnd so that is the explanation from this decision tree\\nof how it got there based on the date, the period,\\nand the demand.\\nYou can also look at these ones down here.\\nIt's a little bit hard to see,\\nif we maybe made this a little bit wider.\\nSo maybe I'll come over here and see if making this wider\\nactually helps us look at the labels.\\nOkay, now you can see the labels here.\\nHere is class two. Class two has low values here.\\n\\nYou can see class one has low values for these two,\\nbut not for the demand.\\nYou can see that this class three here has low values\\nfor these and has low values for date, not for period,\\nand date is super low.\\nYou can see that it's visiting date again.\\nSo this is a nice feature of a decision tree\\nis that you can walk through this\\nand you can explain what's going on here.\\nThis is a useful tool for explaining your clustering.\\nIn this video, we looked at mechanisms\\nto understand what's going on with clustering.\\n\\nAgain, really important thing is to be able to explain\\nto people why you chose the clustering\\nand what the clusters represent.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5907647\",\"duration\":164,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Other algorithms\",\"fileName\":\"3806104_en_US_01_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":200,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Explore alternative algorithms to KNN and understand their comparative advantages and disadvantages.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5562153,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- In this video, I want to briefly discuss\\nother clustering algorithms.\\nSo we've just looked at one clustering algorithm\\ncalled K-means, which is relatively simple,\\nbut there are a bunch of other clustering algorithms.\\nOne of the nice things about the site kit learn library\\nis that it follows the same interface.\\nSo once you can run K-means you can probably\\nuse your data and run it in other algorithms\\nand see how they perform.\\nThe process is probably similar to what we've just seen.\\n\\nYou're going to choose an algorithm to run.\\nYou might not know the number of K,\\nsome algorithms do give you the number of K,\\nbut some might not, and you have to evaluate the algorithm\\nand see what K works for it.\\nAgain, it's not hard to try these other algorithms\\nbecause you just swap out the class,\\nbut the rest of the code, the fit,\\nand the label is the same.\\nNow let me just talk\\nabout two clustering algorithms that might be useful.\\nOne is hierarchical clustering,\\nand what's unique about hierarchical clustering\\nis that it starts with every point as its own cluster,\\nand then it looks for the two points that are closest\\nto each other and quote clusters them into a cluster,\\nand it recursively repeats this keeping track\\nof how far apart each point was is added to the cluster.\\n\\nOnce you've done this, you can create a visualization called\\na dendrogram, which gives you some good insight\\ninto what is the appropriate size of your cluster there?\\nOne of the downsides of hierarchical clustering\\nis that it's relatively slow to run\\nbecause for every point it will make a cluster,\\nso that can take a long time to run.\\nAnother popular clustering algorithm is DBSCAN.\\nThis clustering algorithm creates clusters based on density,\\nand so you can think of it as sort\\nof moving a flashlight around your data,\\nand if things are close to each other, it keeps looking\\naround for other things that are close to that.\\n\\nAnd when there are things that aren't close to that,\\nthen it's kind of done with that cluster.\\nThat can be a nice one to find points that are close\\nto each other based on density,\\nand oftentimes you might have shapes\\nthat are around other shapes but not touching them.\\nThis has the ability to sort of tease that apart,\\nwhereas K-means might not be able to do that.\\nI recommend you try different clustering algorithms\\nand find out which ones might work for you.\\n\\nAgain, one of the main things that I think is important\\nwith doing these experiments is being able\\nto explain why you chose what you did.\\nSo it makes sense to explore some of these other algorithms\\nand understand their pros and cons\\nand why you might want to use them.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5906613\",\"duration\":54,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Apply KNN\",\"fileName\":\"3806104_en_US_01_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":105,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to apply KNN and associated techniques to a real-world problem, testing comprehension, and application.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":1487462,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] Now we're at the point\\nwhere you get to run clustering on your own.\\nSo what I want you to do\\nis using a dataset called the Titanic dataset,\\nload that into a Pandas DataFrame,\\ndrop missing values, drop any non-numeric values,\\nand then I want you to scale the features,\\nbringing them down so they're all on the same scale.\\nScikit-learn has a standardizer that will do that for you,\\nand then run K-means clustering\\nand use the elbow method to find the best value.\\n\\nI highly recommend you try out these exercises.\\nPracticing these, rather than just listening to me,\\nis going to be one of the best ways that you can learn this.\\nIf you find this challenging, what I would recommend\\nis try it out, see where you get stuck,\\nand then follow along with me\\nas you watch the solution video.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5909019\",\"duration\":444,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Apply KNN\",\"fileName\":\"3806104_en_US_01_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":572,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Review and learn how to assess the optimal approach to the challenge, solidifying understanding and filling gaps.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14945965,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(cheerful music)\\n- [Presenter] Okay, let's look at\\nhow I would probably do this.\\nNote that I do have GitHub copilot turned on,\\nso I'll probably take advantage of that.\\nWe do live in a world of AI\\nand I think you should start getting used to being able\\nto leverage that if you can.\\nIf you can, that's okay too.\\nYou can type this out.\\nThis is just going to save me a little bit of time typing this.\\nSo I need to load the Titanic data set.\\n\\nI have this stored on my GitHub,\\nso I'm going to just use the URL from my GitHub for that,\\nand it is as a Excel spreadsheet.\\nI'm also going to need to import pandas.\\nSo let's import pandas\\nand then we'll say, I'm just going to call this raw.\\nI'm going to say raw is equal to pd.read_excel,\\nand then we'll look at what the raw is when we've done that.\\nOkay, so it looks like that worked.\\nThere is our data, if you're not familiar with this,\\nthis is passengers on the Titanic.\\n\\nThis survived columns, whether they survived,\\nthey lived or they died.\\nThis is a common machine learning library used\\nfor making predictive models.\\nOkay, the next thing I want to do is drop\\nthe missing values.\\nSo let's just look at what values are missing here.\\nI'll play around with this a little bit.\\nSo in order to do that, I like to say is an A,\\nthis gives me a data frame, a true false values.\\nAnd then I like to sum that up.\\nThis is going to sum up each column in Python.\\nFalse is zero, true is one.\\nSo this will gimme the counts there.\\n\\nSo age is missing, a few cabins missing.\\nAlmost every one embarked as missing a, you know,\\ntwo fair is missing one.\\nA lot of boat and body are missing\\nand home destinations are missing.\\nSo I think what I'm going to do is I'm going\\nto drop the age column\\nthat are missing the fair and the embarked.\\nBut before I do that, I'm going to get rid\\nof the other columns that are not numeric.\\nSo let's see if we can look at that.\\nI'm going to see raw.dtypes\\nand it looks like anything\\nthat's an object here, I'm going to get rid of.\\n\\nSo I'll get rid of names.\\nSex, ticket, fare.\\nSo I am going to pull off the column names from this\\nso I can make my life a little bit easier.\\nAnd I'm going to make a little function here.\\nI'm going to call it tweak_titanic.\\nAnd it's going to take a data frame and say return.\\nAnd I want to return my data frame.\\nIt's having problems completing there.\\nOkay, so I'm going to say, let's pull off certain columns here.\\nSo the columns that I want are,\\nI'll just copy this whole thing here\\nand then delete what I want rather than typing that out.\\n\\nOkay. And some people use drop.\\nMy preference is actually not to drop things,\\nbut rather be positive about what we do want.\\nSo name and sex, we're going to get rid a ticket,\\nwe're going to get rid of cabin.\\nJust all of these we're going to get rid of as well, okay?\\nSo let's just try this out.\\nI'm going to say tweaked Titanic raw, see if it works.\\nOkay, that's looking pretty good.\\nAnd then we do have some missing valleys\\nin this probably, right?\\nWe can do, this is na, and then sum that up\\nand see what that looks like.\\n\\nSo age is missing and fair is missing.\\nSo I'm just going to do a dropna right here\\nand we'll run that.\\nThat looks pretty good. And we'll do,\\nthis is na and sum that up.\\nOkay, this is looking, okay, so now what I want to do\\nis I want to standardize this data.\\nWhy do I want to standardize this?\\nWell, if you think about clustering, it is doing distance\\nof how far apart things are.\\nAnd you can see that like fair looks like it goes up\\nto like 150 and age goes up to maybe 80,\\nbut some of these are ones and zeros and twos and threes.\\n\\nSo we want to get them all on the same scale.\\nSo I'm going to standardize the data using psychic learn.\\nSo because I'm using copilot here,\\nthis is probably pretty easy.\\nI'm going to say standardize the data here and look at that.\\nIt's kind of doing what I want.\\nHere's a scaler and I'm just going to say, oh, there we go.\\nSo we're going to use a scaler.\\nIt's from psychic learn,\\nand we're going to call fit Transform again,\\nbecause it's psychic learn, it has these common methods.\\nThis is what's called a transformer.\\n\\nSo you can pass in data\\nand it will transform it into a new form.\\nLet's run this and look at what X is when we do this,\\nand this should be the same data that we had before,\\nbut it's going to be standardized.\\nSo what does standardization mean?\\nIt means that every column has a mean value\\nof zero in a standard deviation of one.\\nNow this is returning a NumPy data frame.\\nI actually don't want a NumPy data frame.\\nI don't know if copilot will complete this.\\nHave sklearn output pandas.\\n\\nYeah, that's not what's going to do it.\\nSo I believe there is a set config function in sklearn.\\nSo I'm going to say from sklearn import set config,\\nthere we go.\\nAnd we'll say set config\\nand it's not display is equal to diagram here.\\nI believe it's transform output, that's what it is.\\nTransform output is equal to pandas.\\nOkay, let's try this again here.\\nI'm just going to put exit the bottom here to print that out.\\nOkay, so that looks better in that this is now coming out\\nas a pandas data frame instead of a NumPy array.\\n\\nJust makes it a little bit easier\\nto understand what's going on.\\nYou see these values are the same, negative 1.4,\\nbut if you look at the summary statistics here,\\nthe mean value is very close to zero, like one times 10\\nto minus 16, and the standard deviation\\nis very close to one.\\nSo that makes clustering not pay attention to the size\\nof the data, but rather the relative position of them.\\nOkay, so the next thing we're going to do is I'm going\\nto track the inertias here.\\nSo I think because I am using copilot\\nand it's seen this code, I should be able\\nto get this pretty easy for I in range.\\n\\nLet's just do 20 and let's see if it will complete for me.\\nOkay, so there's my inertias.\\nAwesome.\\nAnd then we'll plot the results.\\nOkay, I think this is pretty good.\\nOkay, yeah, that worked.\\nSo again, I did use AI to complete a lot of this for me,\\nbut one thing you'll note is that a lot of\\nwhat I do in this class is looking at patterns\\nlike these are patterns of code\\nand basically, a lot of these things you can take them\\nand just stick your data into them.\\n\\nSo this code looks very similar to what we saw up above.\\nLet's look at the results here.\\nSo how many clusters would I cluster\\nthe Titanic data set into?\\nAgain, I'm going to sort of take my imaginary ruler here,\\nstick it at an angle and bring it over here, maybe five.\\nAgain, if I've got it touching in multiple places,\\nI might want to do the simpler one.\\nSo I would probably start off with five\\nand start exploring that.\\nOkay, so hopefully, this was useful to give you a chance\\nto play around with the site kit learn library\\na little bit more,\\nand understand how to start making clustering models.\\n\\n\"}],\"name\":\"1. Clustering\",\"size\":74232340,\"urn\":\"urn:li:learningContentChapter:5903620\"},{\"duration\":1465,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5905600\",\"duration\":218,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"PCA\",\"fileName\":\"3806104_en_US_02_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":278,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Grasp the concept and mechanics of PCA for dimensionality reduction.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7728508,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Okay We're going to look at\\nprinciple component analysis or PCA.\\nPCA is an unsupervised machine learning technique,\\nmeaning that we are just going to pass in the data\\nto the algorithm and the algorithm will do something to it.\\nIn the case of PCA, what it does,\\nis it reduces the dimensions.\\nI like to use this for visualizing\\nand understanding my data,\\nbut oftentimes people will use this\\nwhen they have many columns\\nand they want to reduce the number of columns.\\nLet's load in our dataset here\\nand let's look at our dataset.\\n\\nSo X is going to be sepal length and sepal width\\nfrom our iris dataset.\\nThe first two columns there,\\nwe're going to run PCA on the first two columns.\\nAgain, this is using scikit-learn,\\nso we'll import the class and we'll call fit.\\nThis is a transformer,\\nso we're going to call transform after that.\\nThis will give us what I'm going to call XPCA\\nand then I'm going to plot XPCA.\\nSo let's do that.\\nAnd I got an error here.\\nAnd the error\\nis because this code is expecting NumPy as an output.\\n\\nIf we look at XPCA,\\nI believe XPCA is now panned as output\\nbecause of the solution that I did up above.\\nSo let's change this a little bit.\\nThis is what it wanted in pandas,\\nand I'm going to say PCA zero here as the column.\\nThe code is a little bit different and PCA one,\\nso just depending on if you're using\\nNumPy as an output or pandas,\\nyou will have different code here.\\nSo we'll make this work with pandas, I believe.\\nAnd it's not PC, it is PCA, the column name.\\n\\nOkay, there we go.\\nSo this is principle component one,\\nand principle component two of the first two components.\\nAnd this looks pretty similar to what we saw before,\\nwith just the first two columns of the data.\\nSo it, it doesn't look like there's\\na lot different going on here.\\nLet's try and see if we can make sense of what's going on.\\nHere is sepal length and sepal width,\\nand we've got PC1 and PC2.\\nWhat principle component analysis does\\nis it finds the linear combinations of these two columns\\nthat maximizes the variance.\\n\\nAnd so these are linear combinations\\nof these columns down here below.\\nThat might not make sense right now.\\nHopefully I can help you understand it\\na little bit more.\\nHere I'm going to do principle component analysis\\non all of the data.\\nAgain, this is expecting NumPy down here,\\nso I'm going to fix this so it works with panda as the output.\\nAnd here's a scatter plot\\nof the first two principle components.\\nOne of the things I don't like about scikit-learn\\nis that it labels the first component, PCA0.\\n\\nGenerally in the literature,\\npeople will call that principle component one not PCA,\\nPCA is short for principle component analysis.\\nSo I'm not sure why the scikit-learn developers\\nchose that nomenclature there,\\nbut I'm just going to label it PC1 and PC2.\\nBut the column names that scikit-learn gives us in pandas\\nis PCA zero and PCA one.\\nSo what is this?\\nThis is the linear combinations of all of those columns\\nthat add up to principle component one\\nand principle component two scatter plotted out there.\\n\\nOkay, in this video I showed you how to\\nrun principle component analysis.\\nI told you that it was the linear combinations.\\nIn subsequent videos,\\nhopefully I will give you the intuition to understand\\nwhat's going on under the scenes and why this is important.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5907648\",\"duration\":248,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Structure of components\",\"fileName\":\"3806104_en_US_02_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":266,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to delve into the principal components derived from PCA, understanding their significance and structure.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8493889,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Okay, so when we run\\nprinciple component analysis, we got back out\\nthe principle components again.\\nLike I said, these are the linear combinations\\nof the features.\\nWe also get some other artifacts as well,\\nwhich these artifacts are going to help us\\nunderstand what's going on.\\nOne is what's called the explained variance ratio.\\nThis tells us how much information,\\nand in principle component analysis,\\ninformation is the variance.\\nSo how much variance is in the first column,\\nrelative to the second, relative to the third, et cetera.\\n\\nAnd these are in descending, monotonic decreasing order.\\nSo the first principle component\\nwill have the most information,\\nthe second, the second most, et cetera.\\nAnd then we also have what are called the feature weights.\\nYou can think of the feature weights as coefficients\\nthat we multiply the original columns or features by,\\nand then we sum those up to get the principle components.\\nBasically what this is telling us\\nis how important a feature is to the component.\\n\\nOkay, so in this case, I am setting the output here\\nfor scikit-learn to be Pandas,\\nand you can see that here's the output.\\nWhen we run this with all of our data, we get a data frame\\nwith PCA zero, one, two, and three.\\nSo we have four principle components with 150 rows.\\nAgain, I don't like those names,\\nso I would probably do something like this.\\nThis is just pandas code to fix those names here.\\nOkay, so what I'm going to do here is I'm going to make\\nwhat's called a scree plot,\\nand this is going to show us the explained variance ratio.\\n\\nNow, if you look in this code, inside of it,\\nyou can see that it says PCA period\\nexplained variance ratio_.\\nAgain, those attributes ending in underscore\\nare attributes that are learned by fitting the data.\\nWhen we fit the data, it calculated\\nwhat the explained variance ratio is.\\nAnd I am going to plot what that looks like.\\nOkay, there we go.\\nSo this is telling me that the first principle component\\nof this data has 80 plus percent\\nof the information of all of the four columns.\\n\\nAnd the second principle component\\nhas probably around 8% of the information.\\nThe third one, a little bit less.\\nWe can actually pull that off and look at it here.\\nAnd I misspoke, it's not even 80, it's 92%.\\nSo 92% in the first principle component,\\nfive in the second, 1% or almost two in the third,\\nand a half percent in the last.\\nSo what is this saying?\\nIt's saying that if we wanted to reduce\\nthe dimensions of our data, we could use\\njust that first principle component.\\n\\nAnd it has about 92% of the information\\nin the other four columns, seems pretty cool.\\nWe can also do a cumulative sum of that,\\nand we can go the opposite direction.\\nSometimes you might want to say,\\nokay, I want to be able to capture 90%,\\n80%, some amount of percent of the information,\\nand I'm good with losing the rest.\\nIf you do a cumulative sum, you can say,\\nokay, if we want 95%, we have to take\\nthe first two principle components\\nand then we'll have 97% of the information.\\n\\nIt can give you a cutoff to determine\\nwhere you want to drop additional columns if you want to.\\nWhy might you want to drop columns?\\nWell, a lot of machine learning libraries or algorithms\\nwill run a lot quicker with less features.\\nSo if you can preserve the information,\\nbut have them run quicker, that might be something\\nthat is useful to your business.\\nIn this video, we talked about the amount of information\\nstored in the principle components.\\n\\nIn subsequent videos, I'll explain\\nhow we get those principle components.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5904637\",\"duration\":323,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Components\",\"fileName\":\"3806104_en_US_02_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":368,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":11329176,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Lecturer] In this video, I'm going to talk about\\nhow we get the components.\\nThis is the crux of principle components in my opinion.\\nOkay, so here, I'm making a data frame\\nfrom this components underscore attribute.\\nAnd if you just look at components underscore,\\nit's just going to be a NumPy array.\\nSo, I'm actually sticking into a Pandas DataFrame.\\nI hope this will help you understand\\nwhat's going on a little bit better.\\nIn the columns here, we have the original columns\\nthat we fed into our data,\\nand in the row we have each of the principle components.\\n\\nSo, what is this saying?\\nThis is saying that for principle component one,\\nto get principle component one,\\nyou take the sepal length and you multiply it by 0.36.\\nYou add that to the sepal width times negative 0.08.\\nYou add that to the pedal length times 0.85.\\nAnd you add that to the pedal width times 0.35.\\nAnd that gives you one number,\\nwhich represents principle component one.\\nNow, if you think about this,\\nthese weights have some magnitude, how big they are, right?\\nThey might be very big negative or very big positive.\\n\\nBut if your data is on the same scale,\\nand in this case, our Iris data is on the same scale,\\nit's all like numbers between like one and seven.\\nIf your data wasn't on the same scale,\\nwe could use something like standardization,\\nwhich is what we use with a Titanic dataset.\\nIf your data's all on the same scale,\\nthen basically what these weights are,\\nare they are saying how important a feature is\\nfor impacting that principle component, right?\\nBecause sepal width is a very small magnitude,\\nit's pretty close to zero.\\n\\nIf you change sepal width, it's really not going\\nto impact principle component one a lot.\\nHowever, if you change pedal length because it is 0.85,\\nthat will have a big impact on principle component one.\\nSo it's a little bit fuzzy,\\nbut you can think of these as feature importance.\\nThat principle component one is mostly made up\\nof pedal length, but also pedal width and sepal length.\\nIf you look at principle component two,\\nprinciple component two is mostly made up\\nof sepal width and sepal length.\\n\\nAgain, we don't have a bunch of features here,\\nbut in other data sets where you have 80 columns,\\nthis can tell you which columns have the most information\\nor the most variance in them.\\nOkay, so I'm just going to manually show you how to do that.\\nYou need to center your data\\nto get these numbers to pop out.\\nAnd if I center my data, you can see\\nthat like sepal length is negative 0.74.\\nSo, I'm going to multiply that by this, 0.36,\\nand I take the 0.44 and multiply that by negative 0.08.\\n\\nThe negative 2.35, multiply that by this number\\nand the negative 0.99 by this number.\\nIf we do all of that, we get this value right here.\\nAnd that value should be this value here,\\nprinciple component one for row one.\\nSo, that's where those values are coming from.\\nIt's a way to reduce all of those columns to a single value.\\nNow, one thing to note is that how the math works out\\nfor this principle component one is in one direction,\\nand principle component two is going to be orthogonal to\\nthat in ever how many dimensions this is.\\n\\nSo, this is four dimensions.\\nSo, this principle component one would be in one dimension,\\nprinciple component two would be orthogonal,\\nmeaning it's at a right angle.\\nAnd principle component three would be orthogonal\\nto both of those.\\nSo, this would be the third dimension,\\nand then principle component four would be orthogonal\\nto those which is orthogonal in the fourth dimension.\\nThe math works out such that is the case.\\nSo, that might be something that's useful to know is\\nthat all of these are mutually orthogonal to each other.\\n\\nFor those who are curious,\\nyou can manually calculate PCA using NumPy here,\\nand that's what's going on in this cell.\\nSo, you center the data, you take the eigenvalues\\nof the covariance of the data,\\nand you sort those in the order of descending strength.\\nAnd if you look at that,\\nthat will give you the explained variance ratio,\\nthose values there.\\nAnd if you do the dot product of the centered data\\nwith the components, those vectors that came out of there,\\nyou will get the principle components.\\n\\nSo, you can do this by hand if you want to,\\na few lines of code,\\nbut you can also just call fit_transform with scikit-learn,\\nand it gives you this,\\nalbeit I don't like those label names,\\nI wish they changed them to PC instead of PCA zero.\\nOkay, this video hopefully was super useful\\nto you in understanding what is going on with the weights.\\nWhat are principle components?\\nReally, you can think of them as a single number\\nthat represents the importance of the information\\nor variance that is in the original data.\\n\\nSo, it's a great way to collapse multiple pieces\\nof information into a single value\\nor into smaller dimensions.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5906614\",\"duration\":138,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Scatter plot\",\"fileName\":\"3806104_en_US_02_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":150,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to visualize high-dimensional data in lower dimensions using scatter plots of principal components.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":4077728,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Okay, in this video,\\nI only have one cell of code but I think it's really cool.\\nWhat I'm going to do is I'm going to scatter plot\\nthe principal components.\\nAnd I'm going to use the plotly library to do this.\\nYou can use matplotlib as well.\\nBut I like the plotly library\\nbecause it works well in Jupyter,\\nbut it also gives us interactivity.\\nLet's just play around with this\\nbecause I'm going to plot this in three dimensions.\\nAlso, I do like to plot scatter plots\\nof the first two dimensions,\\nbut we're going to do three dimensions here.\\n\\nI've imported the plotly library and I just say,\\nlet's do a scatter_3d on this pcas dataframe here.\\nAnd in the x dimension, I want PC1, y, PC2 and z, PC3.\\nAnd I'm going to color this by y.\\nLet's plot that.\\nOkay, and this is the result of that.\\nThis is a plotly scatter plot.\\nThe nice thing about this is you can hover over this,\\nyou can see what's going on here.\\nSo these colorings\\nare based on the variety of the iris flower here.\\n\\nAnd if I click and drag, I can view what's going on.\\nSo remember, we said PC1 has 90 plus percent\\nof the information here, so I might want to come down\\nand sort of rotate this like this.\\nYou can see, as we're going from the left to right,\\nwe're going through PC1 from negative to positive values.\\nAnd this has a lot of the information.\\nAnd maybe I'll zoom in here a little bit if I can.\\nLet's see if I can get this to zoom in.\\n\\nOkay, with with my mouse, I can sort of scroll in\\nand zoom in a little bit.\\nAs we're going from left to right,\\nyou can see a pretty clear delineation here\\nof like, one variety of flower, another one and another one.\\nAgain, I did not train the data on the variety.\\nI only trained it on the shape of that,\\nbut it's kind of clustering the data in the varieties.\\nTo me, this is pretty cool.\\nI can use this principal component analysis as a technique\\nto view multiple dimensions in two or three dimensions,\\nand see what's going on with my data.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5904638\",\"duration\":240,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Other algorithms\",\"fileName\":\"3806104_en_US_02_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":272,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Investigate other dimensionality reduction techniques beyond PCA.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7278208,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this video, I want to briefly touch\\non some other dimensionality reduction techniques\\nthat might be interesting that you might hear about.\\nOne is t-SNE,\\nor t-distributed stochastic neighbor embedding.\\nThis tries to preserve the local structure of the data.\\nAnother common one that you'll hear about\\nis uniform manifold approximation and projection, or UMAP.\\nAnd this tries to preserve the local\\nand the global structure of the data.\\n\\nNow, these both follow the scikit-learn interface.\\nSo if you can run PCA on something,\\nyou can probably run these others as well.\\nOne of the things I like about PCA\\nis it's relatively simple,\\nwe have those weights that explain what's going on with it.\\nBoth of these, t-SNE and UMAP,\\nare a little bit more complicated.\\nAnd so, while you can use them, oftentimes,\\nyou have to nurse them along a little bit more.\\nBecause, for example, they might behave differently\\nif you run them multiple times,\\nor you might need to tune some of the behavior of it.\\n\\nBut if you look at this, this is from scikit-learn,\\nit's in there and you can call fit transform.\\nAnd when we look at the output here,\\nwe get these embeddings, I would call these embeddings,\\nthat are similar to the principle components,\\nbut these are the t-SNE embeddings for that.\\nAnd you can throw these into Plotly just like we did before,\\nand you'll get something that looks like this.\\nSimilarly, we can use UMAP.\\nThis is not part of scikit-learn,\\nbut it follows the scikit-learn interface.\\n\\nSo if you install that, pip install UMAP learn,\\nwe can make a UMAP class,\\nand then we can call fit transform with that.\\nI'm going to stick that result back into a Pandas data frame\\nbecause for me, it's easier to work with Pandas.\\nAnd let's plot that.\\nAnd here is the result of UMAP.\\nWhich one of these is best?\\nThe answer is, an unsatisfying, it depends.\\nOne of the things that principle component analysis\\nmakes an assumption of is that\\nthere is a linear relationship with your data.\\n\\nAnd if you don't have that, doesn't mean you can't run\\nprinciple component analysis, but you might see\\nweird artifacts from running\\nprinciple component analysis on your data.\\nSo there's no guarantee, like in our example,\\nwe saw that the first principle component\\nhad 90% of the information.\\nThere's no guarantee, in your dataset,\\nthat that will be the case.\\nBut I like to have this in a tool for me,\\nbecause my brain can't go much beyond three dimensions\\nto understand or to visualize\\nwhat's going on at high dimensions.\\n\\nRemember when we looked at the clustering previously,\\none of the things I like to do is I like to cluster my data,\\nand it might be high dimensions that I'm clustering it on,\\nbut then how do I visualize those clusters?\\nWell, I'll often do principle component analysis\\non the data, on the original data.\\nAnd then color the plot of the principle components\\nby the labels of the clusters.\\nAnd that lets me kind of get a feel\\nfor what's going on in there.\\n\\nI can also look at the weights of the principle components,\\nand understand the clusters if they are related\\nwith the principle components that way as well.\\nSo for me, it's a good tool to explore data\\nand try and understand it.\\nThe nice thing about these unsupervised algorithms\\nis that they're just telling me what's in the data.\\nThey don't encode my bias,\\nor what I am trying to encode.\\nThey are just what's in the data.\\nSo the data might be biased, but it's going to just show\\nwhat is in the data, which is good for me\\nto look at the data in a fresh way.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5906615\",\"duration\":34,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Utilize PCA\",\"fileName\":\"3806104_en_US_02_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":37,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to utilize PCA on a provided dataset, emphasizing interpretation and application.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":888909,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] Okay, it's challenge time again.\\nYour challenge is to run PCA on the numeric columns\\nof the \\\"Titanic\\\" data set.\\nOne thing to be aware of is that PCA will not work\\nwith missing data, and it requires numeric data.\\nSo you might need to play around with the data a little bit\\nbefore you can run PCA on it.\\nAnd then after you've done that,\\nI want you to plot the results in plotly\\nof the first three components.\\n\\nGood luck.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5905601\",\"duration\":264,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Utilize PCA\",\"fileName\":\"3806104_en_US_02_07_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":339,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to analyze the optimal PCA application and understand potential areas for improvement.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10261542,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] Okay, let's look\\nat the Titanic data set here.\\nI believe I have it loaded as raw\\nand I believe I have that tweak Titanic function\\nas well around.\\nSo let's just run that\\nand see if that helps us get our data\\npaired down a little bit.\\nOkay, so here is our numeric data,\\nand we're going to run PCA on this.\\nAgain, note that we probably want to standardize the data.\\n\\nSo let's do that here.\\nWe're going to use scikit-learn's standard scaler to do that.\\nWe'll make a scaler, and we'll call fit transform\\nto get that scale data here.\\nLet's look at what that looks like.\\nOkay, here is our scale data. That's looking pretty good.\\nOkay, so we're going to now\\nrun principle component on the data.\\nSo we'll make a PCA object,\\nand we're going to call fit transform.\\n\\nThat should give us the transform data.\\nLet's look at what that looks like. Okay, here is the data.\\nAwesome. It is pca0, pca1, pca2. Cool.\\nSo we're going to plot this using Plotly now.\\nAgain, I keep coming back to this,\\nbut I want you to pay attention.\\nThis interface here, fit transform, fit transform,\\nit's the same interface.\\nSo once you get used to that, it makes it really easy.\\nPlot the first three columns in Plotly.\\n\\nLet's see if this works, okay?\\nI think that this should work.\\nOkay, it didn't show us anything\\nbecause we need to say fig.show at the end here.\\nLet's try it again.\\nOkay, so here are the first three components.\\nOne thing that might be cool here is\\nto color this based on whether they survived.\\nLet's do that.\\nAnd I got an error here, says\\nthat the arguments don't have the same length.\\nSo the issue here is that raw has all\\nthe data and I believe the X_pca doesn't.\\n\\nLet's look at X_pca.\\nSo if you look at the index here, the index goes up to 1308,\\nbut there's only 1045 rows.\\nSo what I can do is I should be able to come here\\nand say, let's take the raw data, but I'm going to use loc,\\nand I'm going to use the X_pca index,\\nand that should filter the rows.\\nLet's see if that works. There we go.\\nOkay, so there is our data, whether they survived or not.\\n\\nNow, note that this is including\\nthe survival column, I believe.\\nIf we look at the output of this X right here,\\nokay, so this is using survived.\\nLet's actually remove survived from this.\\nSo I'm going to come in here\\nand let's just run it one more time removing survived.\\nI'm going to say drop column survived here,\\nand I just want to see if there's still the separation\\nthat we get when we do that, so I'll run that again.\\n\\nWe'll run this again, and we'll plot this again.\\nOkay, so remember, I said there's no guarantee\\nthat your data will be like cleanly separated\\nlike our iris data is,\\nand you're kind of seeing that here, right?\\nIt was kind of nice that survived, sort of stuck out here.\\nI mean, it looks like we are seeing a little bit of like\\nthis blue edge down along this side here,\\nbut we also see like some outliers over here.\\n\\nSo what this is telling me is this relationship\\nbetween survived and not is not a linear relationship.\\nThere might be some sort of bathtub, curved-type effects\\nwhere if you're very young\\nor very old, you might've survived,\\nbut if you're middle age, maybe you didn't survive.\\nI think this is really cool\\nthat we can get insight into our data\\nand just by looking at this,\\nand I would encourage you to play around\\nwith this a little bit more.\\nLook at the weights, see what weights\\nare impacting those components.\\n\\nWhat original features are driving principle component one,\\nand principle component two, et cetera,\\nto kind of understand what's going on here?\\n\"}],\"name\":\"2. PCA\",\"size\":50057960,\"urn\":\"urn:li:learningContentChapter:5907651\"},{\"duration\":1431,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5906616\",\"duration\":274,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Linear regression algorithm\",\"fileName\":\"3806104_en_US_03_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":287,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn about the core principles of linear regression for modeling relationships between variables.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9129412,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this video,\\nwe're going to talk about linear regression.\\nAnd linear regression is one of a family\\nof machine learning algorithms\\ncalled supervised machine learning.\\nWe've been looking at unsupervised machine learning.\\nWith supervised machine learning,\\nwe're going to pass in this x data frame or matrix,\\nand we're going to also pass in a y.\\nSo if you think about what we've been looking at,\\nwe've been thinking at rows, which are samples,\\nindividuals with columns that are features\\nthat describe those samples.\\n\\nIn the case of supervised learning,\\nwe're also going to pass in some labels as y,\\nand that will be a one dimensional in our case,\\ngenerally a series that corresponds to each row.\\nSo each row will have a label.\\nWe're going to train our algorithm to be able to predict these.\\nIn the case of regression, these will be numeric values\\nand we're going to train the algorithm\\nto predict numeric values.\\nLinear regression is a common algorithm\\nand it's kind of the basic algorithm you might remember\\nfrom school where you learned a formula,\\ny is equal to mx plus b,\\ny is equal to the slope times sum x plus the intercept.\\n\\nAnd that is what linear regression is kind of doing it,\\nsolving that equation for us,\\ngiving us the slope and the intercept.\\nI've got a formula here\\nthat shows a generalized version of it,\\nbut you can think of this b0 here as the intercept.\\nAnd then for each x here,\\nyou can think of each of these x's as a column in our data.\\nWe're going to multiply it by some weight,\\nand we sum all of those up and that will give us a y value.\\nSo it's trying to solve this equation here.\\n\\nSo I'm going to load a dataset called Anscombe's Quartet.\\nThis is a popular dataset used for visualization.\\nGot it in our data frame here.\\nSo we've got x with y1 and y2 and y3,\\nand then this fourth dataset x4 goes with y4.\\nWe'll just plot x with y1 here and a scatter plot,\\nand it looks something like this.\\nOkay, so we're going to run the algorithm on this,\\nand here's the formula for doing this.\\n\\nIf you like the math, you can calculate the slope\\nand then you can calculate the intercept\\nand you'll get this equation right here.\\nSo let's do that right here.\\nWe're going to say that our x is the x column here\\nand y is this y column.\\nAnd then our slope, we're just plugging this in.\\nThis is using either Pandas or NumPy.\\nEither one of those will work.\\nAnd this should give us the slope for that.\\nIt's saying that the slope is 0.5.\\nAnd if you remember that, that's the rise over the run.\\nSo it looks like there's a positive slope there.\\n\\nThat's seems plausible.\\nAnd here's the intercept.\\nWe're just going to follow that calculation form.\\nThe intercept is our y bar, which is the mean,\\nminus the slope times x mean.\\nAnd we'll get 3 as the intercept.\\nAnd if we scroll up here to our graph,\\nit's not going to zero,\\nbut you can imagine zero would be over here.\\nAnd if you fit a line on this,\\nit looks like it would probably cross around 3,\\nwhich is what it's calculating there.\\n\\nOkay, let's plot this now.\\nI'm going to plot my original scattered plot,\\nbut now I'm going to plot my line as well.\\nAnd I'm just using NumPy to say,\\nlet's make a linear space here\\ngoing from 4 to 14 with 100 points.\\nAnd my y is going to be looks like math.\\nI'm going to say slope times my x, mx plus b,\\nplus my intercept, okay?\\nAnd we're going to plot that on the same plot here.\\nWe're going to say plot x1 and y1 there as well.\\n\\nAnd there we go.\\nThere is our formula for a line that fits these points.\\nNow note that this is a line,\\nit's not curve, it's a straight line.\\nBut what this would give us is,\\nyou could think about in the future\\nif someone said, \\\"Okay, I've got a value of x around 10.5,\\nwhat value of y would that be?\\\"\\nAnd you need just go over here to 10.5,\\ngo up and whatever is on that line,\\nthat's what this would predict.\\nThis is a quick introduction to linear regression.\\n\\nWe've done it in one dimension here.\\nWe just got a single value for x,\\nbut this generalizes to end dimension.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5903619\",\"duration\":258,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"scikit-learn\",\"fileName\":\"3806104_en_US_03_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":279,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to implement linear regression on a simple dataset, emphasizing foundational concepts.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7704971,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Okay, we just looked at the math\\nbehind linear regression,\\nand I know a lot of you probably like math,\\nbut I'm a lazy programmer,\\nand so I don't want to do the math.\\nAnd guess what?\\nAs long as we understand that Scikit-learn interface,\\nwhich is fit.\\nSo let's try this.\\nI'm going to say, okay, let's make an X1,\\nand I'm using some pandas code here.\\nGenerally, Scikit-learn wants X to be a data frame,\\nand if I just do this with a single square bracket,\\nthat X will be a column or a series, not a data frame.\\n\\nIf I do it with the double square brackets here,\\nthis is saying pass in a list of columns\\nthat you want in your data frame,\\nand I can do a list with just one thing in it.\\nSo X1 here is really a data frame,\\nand then Y1, this is going to be a series.\\nSo look at this.\\nI just import linear regression from Scikit-learn.\\nI make an instance of the linear regression class,\\nand I call fit.\\nWe've seen fit many times in this course already.\\n\\nIt's so easy once you understand how to use Scikit-learn,\\nwhen you have your data in the format\\nthat Scikit-learn likes,\\nyou generally just call fit with your data.\\nSo let's run that.\\nAnd this looks like it returns\\nthis linear regression object down here.\\nI'm going to now look at this thing called coef_.\\nWe talked about this already.\\nWhen things end in an underscore in Scikit-learn,\\nthose are things that are learned from fitting.\\n\\nSo they're not always the same.\\nSome models learn different things.\\nIn the case of linear regression, we learn a coefficient\\nand we learn an intercept.\\nSo here's our coefficient, which is 0.5.\\nThat should seem familiar.\\nWe actually saw that above.\\nAnd here's our intercept, which is 3,\\nwhich is what we also calculated above.\\nSo linear regression in Scikit-learn is giving us\\nthe same things that we saw above.\\nNow I'm going to do linear regression on Y2 and Y3,\\nwhich are two other parts of this\\nAnscombe's Quartet dataset,\\nand I'm just going to plot those here.\\n\\nSo now we have three plots.\\nIn the left plot is the first dataset.\\nThat's the one we saw before.\\nIn the second plot is the second dataset\\nfrom Anscombe's Quartet,\\nand you can see that the linear regression\\nis a straight line.\\nHowever, the dataset doesn't really look like it's straight.\\nIt actually looks like it's curved.\\nAnd in the third plot here,\\nyou can see the data looks like it's a straight line,\\nbut if you look carefully up above,\\nthere is an outlier up there.\\nSo this dataset, Anscombe's Quartet,\\nif you're not familiar with it,\\nwas a dataset created by a statistician\\nto drive home the importance of visualization.\\n\\nIf you look at the summary statistics of these data sets,\\nthey have the same summary statistics, same mean,\\nand same standard deviation.\\nBut if you visualize them, you can see\\nthat they are different data sets.\\nAnd people have actually taken this idea recently\\nand made fancier data sets that have like dinosaurs in them\\nand other shapes in them\\nthat have the same summary statistics.\\nBut when you visualize the data,\\nyou see that they are different.\\nSo you could think that for the first dataset on the left,\\nlinear regression might be an appropriate choice.\\n\\nFor the second one, it probably isn't.\\nIt just rubs me the wrong way to see\\nthat you've got these points\\nthat look like they're in a parabola,\\nand then you've got a line going through them.\\nAnd then in the third one, it makes me think,\\nokay, what's going on with that point up there above 12?\\nDo we need to figure out something to do with it?\\nIs it an outlier?\\nIs the data bad or is that really how it goes?\\nAnd our model needs to adapt to that somehow.\\n\\nHopefully, this combination of plotting is a great technique\\nthat you can use as well to start\\nto understand your models and diagnose them\\nand see if they make sense.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5906617\",\"duration\":374,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Real-world example\",\"fileName\":\"3806104_en_US_03_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":463,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to recognize and validate the core assumptions behind linear regression.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":12535691,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Tutor] Okay with that, what I'd like to do\\nis move on to a real-world example,\\nrather than playing with these toy datasets.\\nAnd we're going to use from that datasets library a dataset\\nof F-16 aircraft elevator information.\\nSo I'm not an expert in aircraft, so to speak,\\nbut this dataset is used to predict,\\nif an action is taken on the aircraft, what happens to that.\\nSo let's load the dataset here.\\n\\nAnd again, we need to jump through a little bit of hoops.\\nWe need to say, okay, we want the train set,\\nand we want to convert that to pandas,\\nand it looks something like that.\\nOkay, at the end here, you can see this goal.\\nIt's a numeric value that we might want to try and predict.\\nSo let's see if we can predict it.\\nSo what do we do?\\nWe're going to make an X and a y.\\nSo our X is going to be all of the columns except for Goal.\\nOur y is going to be the goal column.\\n\\nAnd we'll make a linear regression model,\\nand we'll call fit with X and y.\\nThat looks like it worked.\\nAgain, you need to be aware that LinearRegression\\nis an algorithm that needs numeric data,\\nand it can't have missing data.\\nSo looks like this data is actually pre-processed\\nin a way such that we're not running into those issues.\\nNow, another thing to be aware of\\nis that you might want to standardize your data as well.\\nIn this case, we did not do that,\\nbut you can see that the climbRate column\\nlooks like it has a different magnitude\\nthan some of these Time columns at the end.\\n\\nSo you might get a different result\\nif you standardize that data.\\nLet's look at the coefficient here.\\nIn this case, the coefficient is not a single value,\\nit is a NumPy array.\\nWhat this corresponds to is,\\neach of the features that we passed in,\\neach of these columns that came in here,\\nthis is the corresponding coefficient for that column.\\nSo to calculate the result of the prediction here,\\nif you have a row, you would take the first column,\\nmultiply it by this weight, the second column by this one,\\nsum these up, and then add the intercept as well.\\n\\nSo here is the intercept, intercept_.\\nOne of the things that I like to do\\nis I like to plot this to see what's going on here.\\nBecause these are weights, the magnitude is important,\\nand it tells us, the larger the magnitude,\\nthe more impact it might have on the data.\\nNow, be aware that we did not standardize the data here.\\nSo by looking at this, it looks like these SaTime3, Sa,\\nand SaTime2 are the bars that have the biggest magnitude.\\n\\nAgain, I'm not super concerned about the direction.\\nThe direction is important.\\nLike, if SaTime3 goes up,\\nthat's going to push the result higher.\\nIf SaTime2 goes up,\\nit's going to push the result in a negative direction.\\nThat's super useful to have direction.\\nBut also note that, like,\\nthese values in the middle without any magnitude\\nlook like they're not going to impact the model very much.\\nWe can also ask the model what the score is,\\nand scikit-learn has a general method for that.\\n\\nIt's the score method.\\nIn the case of regression models,\\nit reports what's called the R2,\\nor the coefficient of determination.\\nThis is the amount of variance in the answer\\nthat is explained by the features in the data.\\nGenerally, this is a score that is between 0 and 1.\\nThe closer it is to 1\\ngenerally indicates that a model is better.\\nNow, let me just make a comment here.\\nFolks often ask me, \\\"What number represents a good model?\\\"\\nAnd the answer to that is an unsatisfying, \\\"It depends.\\\"\\nIt might be the case that you have a score of 0.99,\\nand the model isn't sufficiently good for your use.\\n\\nIt might be the case that you have a model\\nthat has a score of 0.2,\\nand it works for your business case.\\nSo oftentimes you need to go into further analysis\\nand see if a model makes sense for your business.\\nBut what we can say is that,\\non a given dataset, if you have two models,\\nand one has a higher R2 score,\\nthat should be a better model.\\nNot that a higher score is of itself sufficient\\nfor a model to be put into production.\\n\\nNow, there are other metrics as well.\\nI'm going to import some of those here.\\nCommon ones include the mean_absolute_error.\\nSo that is taking all the errors,\\nbasically, here's the predicted value,\\nhere's the true value, what's the difference between those,\\nand take the absolute difference and take the mean of those.\\nWhy the mean absolute instead of just the mean error?\\nWell, you might have some errors that are positive\\nand some that are negative,\\nso theoretically it's possible to get a mean error of 0,\\nbut you have numbers that are way off.\\n\\nSo we're taking the absolute there.\\nAlternatively, you can say the mean_squared_error,\\nsquare those values so they're always positive,\\nso you don't have negative values canceling them out.\\nThis is also going to penalize outliers\\nor values that have really bad errors.\\nSo in the case here, you can see that\\nour mean_absolute_error is this value, 0.0019,\\nand our mean squared_error_is 8.4 times 10 to -6.\\n\\nIs that good or bad? I'm not sure.\\nAgain, I'm not a aircraft expert.\\nBut again, if those values go down,\\npresumably you have a better model.\\nNow, we can also ask the model to predict\\nsomething given a row.\\nSo here I'm saying,\\ngiven the first row of my data, make a prediction.\\nIt says, \\\"I'm going to predict 0.03.\\\"\\nIf I look at the real value of that, it's 0.031.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5901671\",\"duration\":364,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Assumptions\",\"fileName\":\"3806104_en_US_03_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":405,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to apply linear regression to a complex dataset, drawing insights and making predictions.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":13543944,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Okay, let's talk\\nabout some of the assumptions of linear regression.\\nIf you were to take a stats class, they would go over these.\\nI'm going to give a caveat,\\nand some statisticians might not like this,\\nbut my experience sample size one is that in the industry,\\na lot of people aren't covering these\\nbecause oftentimes they will use linear regression\\nas a base model and then use other models like XGBoost.\\nAnd if the XGBoost model performs better\\nor is good enough for their business,\\nor even if the linear regression model\\nworks in their business,\\nthey don't necessarily care\\nthat some of these assumptions are violated.\\n\\nSo let's talk about the assumptions here.\\nOne is that this assumes that there's a linear relationship\\nbetween the features and the target.\\nRemember the features are the columns\\nand the target is that label that we're trying to predict.\\nAnd we saw in the examples with the Anscombe's quartet,\\nwhere we had a non-linear relationship.\\nRemember that second model there?\\nI said it didn't sit well with me\\nbecause the scatter plot looked curved,\\nbut the model was square.\\n\\nThat second model didn't sit well with me\\nbecause the scatter plot looked like it was a parabola,\\nbut the model was a line.\\nNow, in practice, that model might be sufficiently good\\nfor someone to deploy in the real world,\\nprobably not, because that's pretty simplistic\\nand you probably wouldn't need a model for that.\\nBut again, you need to make sure\\nthat you evaluate a model from a business point of view\\nand see if it would make sense for your business\\nif it's going to save you money or make you money.\\n\\nOkay, another assumption is no multicollinearity.\\nThat means that the features,\\nthe columns are not correlated among themselves.\\nNow, in practice, they can be correlated,\\nbut what's going to happen is if you have two columns\\nthat are basically correlated\\nor highly correlated, they're the same thing.\\nThey're going to split the weights between them.\\nSo if you look at the weights\\nand you have multiple columns that are correlated,\\nit's going to screw up the interpretation of your model.\\n\\nSo you want to be careful with that.\\nHomoscedasticity, that's a big word,\\nbut basically what it says\\nis that when you look at the residuals,\\nwhich is the error of your prediction,\\nas you go through the values for that target,\\nare the errors constant or do they change?\\nIs the variance changing over time,\\nor is that variance constant\\nas you sweep through the various values?\\nSo this assumes that it will be constant over time,\\nbut if it's not, generally, that indicates\\nthat you might need to play around with your features\\nor use a better model\\nthat's better able to compensate for that.\\n\\nAlso, linear regression doesn't like outliers.\\nWe saw that with the third example in Anscombe's quartet.\\nIt kind of threw off the line\\nbecause of that outlier up there.\\nSo you might want to be aware of outliers in your data\\nbecause that might throw off your model.\\nAnd then again, I note here\\nthat you probably want to scale the features.\\nSo we didn't scale the features above.\\nLet's scale them now.\\nAgain, we've seen how to do this already.\\nWe're just going to use our standard scaler,\\nand now all of these columns should be on the same scale.\\n\\nIf we look at the summary statistics of those,\\nif you look at the mean,\\nthe mean should be close to zero.\\nAnd in this case, it's not zero\\nbecause of the impedance mismatch\\nbetween how computers represent numbers,\\nbut it's close enough for computers.\\nLike -1 x 10 to the -17\\nfor most people is pretty close to zero.\\nStandard deviation should be close to one as well.\\nAnd you can see that that looks like\\nthat is indeed the case.\\n\\nSo let's run our model now.\\nHere is our model.\\nIf we look at the scorer, the score is now 0.81.\\nLet's look at the score previously.\\nAnd the score previously was 0.8134.\\nSo it looks like the score didn't really change too much.\\nLet's look at our bar plot here of the features.\\nAnd you can see that the bar plot is different now.\\nSo before, SaTime was the most important,\\nand now we had diffRollRate\\nand absRoll bump up above that.\\n\\nSo I'm assuming\\nthat that is because we standardized the data\\nand that these were at a different scale before.\\nSo those weights were scaled differently,\\nbut now they're coming in as more important.\\nSo that is something to be aware of\\nif you want to understand what's going on with your model.\\nOkay, I'm going to now use what's called the XGBoost model.\\nThis is not part of scikit-learn,\\nbut it follows the scikit-learn interface.\\nThis is a super popular model\\nand it tends to do very well out of the box.\\n\\nYou can see that I call fit and score\\nso it has that same interface, but we got a 0.95 in there.\\nAgain, folks ask, so is 0.95 good enough?\\nI don't know if it's good enough,\\nbut what I do know is that this 0.95 score is a better model\\nthan a 0.81 score.\\nSo it's a better model.\\nIs it sufficiently good?\\nThat question needs a little bit more exploration to answer.\\n\\nOkay, in this video, we talked about some of the assumptions\\nof linear regression,\\nand we also saw that if we scaled the data,\\nin our case, the score did not change,\\nbut the interpretation of the weights did change.\\nWe also saw that we can use an XGBoost model\\nthat has the same interface,\\nand oftentimes we get a big performance\\nby using a model like XGBoost.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5906618\",\"duration\":34,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Develop a linear regression model\",\"fileName\":\"3806104_en_US_03_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":37,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to develop a linear regression model for a practical scenario, focusing on model tuning and interpretation.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":980931,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] It is challenge time again.\\nNow's your chance to make a model to predict\\nhow much Titanic passengers paid for their tickets\\nwith linear regression.\\nSome of the previous challenges might have been\\na little hard because we weren't aware of standardization\\nand other things that we can do with Scikit learn,\\nbut hopefully now we're getting a better feel for this.\\n\\nTry this out and see how your model does.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5906619\",\"duration\":127,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Develop a linear regression model\",\"fileName\":\"3806104_en_US_03_06_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":202,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Examine the best practices for the challenge's scenario, consolidating learning and refining techniques.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3984216,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] Okay, let's look at the solution here.\\nI think we've got our Titanic data right here.\\nOkay. So we're going to make a model\\nthat predicts this column here, fare,\\nfrom all of the numeric columns.\\nLet's see if we can do that.\\n\\\"Predict fare from numeric columns.\\\"\\nOkay, so here is our X.\\n\\nWell, I think we want to call tweak_titanic on that too.\\nSo we're going to call tweak_titanic on that,\\nand then we'll drop that.\\nAnd our Y is equal to,\\nand that should be the fare column.\\nAnd let's make a linear model.\\nSo we're going to call fit with X and Y.\\nOkay, that looks like it worked.\\nLet's see what our score is.\\nAnd our score is 0.38.\\n\\nOkay.\\nSo, is that a good model? Again, we don't know.\\nWe can look at other metrics and see if that makes sense,\\nbut it's a little bit hard to tell.\\nJust for fun, let's compare this with an xgboost model\\nand see how that does.\\nSo it's an XGBRegressor.\\nAnd we're going to say xgb.fit.\\nIt's the same interface here.\\nAnd we're going to say X and Y,\\nand then we will see the what the score is.\\n\\nAnd it looks like this model performs better.\\nFull disclosure, I still haven't shown you one thing,\\nwe'll talk about this later, which is splitting your data.\\nWe're evaluating our model on data that it's seen,\\nso we're evaluating our scoring here with all of the X,\\nand we're fitting it with all of that.\\nSo you could imagine a model that just memorizes that\\nand returns the result.\\nWhat we really want to do is split the data\\ninto a training set and an evaluation set.\\nWe'll see that later on in the course.\\nSo keep your eye out for that.\\n\\n\"}],\"name\":\"3. Linear Regression\",\"size\":47879165,\"urn\":\"urn:li:learningContentChapter:5909022\"},{\"duration\":737,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5904639\",\"duration\":79,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Logistic regression algorithm\",\"fileName\":\"3806104_en_US_04_01_VT\",\"demo\":true,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":129,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Grasp the foundational knowledge of logistic regression for binary classification tasks.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":2579617,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Narrator] In this video,\\nwe're going to talk about logistic regression.\\nOne thing to be aware of,\\nlogistic regression has the term regression in the name,\\nbut it is used for what is called classification,\\nwhere regression is we are predicting a numeric value,\\nin classification, we are predicting a categorical value.\\nIn this case, either a one or a zero\\nfor a positive or a negative class.\\nAnd it's called logistic regression\\nbecause it actually does curve fitting on a logistic curve,\\nand when you fit that curve, you get probabilities.\\n\\nAnd so basically, when the probability is above 0.5,\\nit's going to predict the positive case,\\nand below 0.5, it is going to predict the negative case.\\nI've got the formula written out right here\\nfor this function.\\nLet's plot it and see what it looks like,\\nand hopefully, that helps you get an understanding\\nof what's going to happen here.\\nSo in our data, and I'll show this in the next video,\\nwe'll have ones and zeros,\\nand we are going to try and fit this curve to that.\\n\\nAnd above this 0.5 value,\\nanything that's to the right of that,\\nwe will predict that as a positive,\\nand below, we will predict that as a negative.\\nIn the next video, we'll see an example of doing this.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5906620\",\"duration\":109,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Basic example\",\"fileName\":\"3806104_en_US_04_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":125,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to execute logistic regression on a basic dataset, emphasizing core concepts.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":3453785,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Let's look at a basic example\\nof logistic regression.\\nI'm going to make some data points here.\\nAgain, I'm just doing this in one dimension here.\\nI've got some X values,\\nand we're going to predict some Y values from that.\\nSo here's our training data.\\nWhen I plot this, I get something that looks like this.\\nSo the question is, if I give you an X value,\\ncan you predict whether it is a one or a zero?\\nSo we're going to use scikit-learn to fit this data here.\\nAgain, I need to put that X, even though it's one dimension,\\nit needs to be two dimensional.\\n\\nSo I'm going to stick it into a data frame.\\nThe Y can be a series, that's fine,\\nand it looks like it fit.\\nAt this point we have coef underscore,\\nwhich is the coefficient, and we have the intercept.\\nWe can plug those into our formula here\\nand see what happens when we do this.\\nSo here is our fitted plot.\\nI've got the original points as scatter plots.\\nI've got the curve fit on there.\\nAnd again, anything above this 0.5 or to the right of it,\\nso it looks like somewhere around here below zero,\\nwe are going to say that it is positive.\\n\\nEven though this would misclassify this\\nand it might misclassify that.\\nThat's okay, it will capture the other points.\\nAnd once we have this, again,\\nthis follows the scikit-learn interface.\\nIf we want to predict a value,\\nwe can pass in the value here.\\nAnd this says, if you pass in a negative 0.3,\\nwe're going to predict one.\\nAnd we can play around with this if we want to.\\nWe can say, okay, what about negative 0.43?\\nThat's a zero, right?\\nSo somewhere around that 0.3 is where it decides\\nto go up into positive versus negative.\\n\\n\"},{\"urn\":\"urn:li:learningContentVideo:5909020\",\"duration\":247,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Assumptions\",\"fileName\":\"3806104_en_US_04_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":305,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to implement logistic regression on a real-world dataset, deriving actionable insights, and outcomes.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":9001186,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Okay, we're going to use a dataset\\nfrom the datasets library.\\nAgain, this comes from openml.org,\\nand I've pasted a little bit from the website here,\\nbut this is tracking eye movements.\\nYou can see that features are in the columns.\\nEach assignment is a time sequence, 22 dimensions.\\nThe first column is the line number,\\nblah, blah, blah, blah, blah.\\nEach example represents a single word.\\nYou're asked to return the classification\\nof each read sentence.\\n\\nSo we're going to load that,\\nand once we've loaded that,\\nwe're going to stick that into Pandas\\nand see what it looks like.\\nOkay, so 7,000 rows, 21 columns,\\nand we have this label over here,\\nand that's what we are trying to predict which label.\\nSo let's set up our data.\\nI'm going to standardize our data.\\nAgain, this is a family of algorithms that fits this curve,\\nand generally with these curve type fitting algorithms,\\nwe want to have data that's on the same scale.\\n\\nSo I'm going to use the standard scaler, standardize that data.\\nAgain, that gives each column a mean value of zero\\nand a standard deviation of one.\\nWe're going to stick all of the columns except\\nfor label into X,\\nand we're going to stick the label column into the Y.\\nLet's standardize our data.\\nSo we'll use the standard scaler,\\nwe'll just call fit transform.\\nThat will give us our X scale data.\\nWe don't need to worry about scaling the Y data,\\nthat's fine.\\nLet's then call fit and score on that and see what happens.\\n\\nOkay, so we fit the data and we called score,\\nand we got a score of 0.56.\\nSo what does that 0.56 mean?\\nThis is not that R2 score that we saw in regression.\\nThis is the accuracy.\\nIt is the percent of classifications that is correct.\\nSo of all of those labels in there,\\nwe got 56% of them correct.\\nAgain, little rant about this.\\nIs 56 good or bad?\\nIt might be good, it might not be good.\\n\\nIt really depends on your business use case.\\nGenerally, a model that has a higher accuracy is better,\\nbut there are other metrics that might be important.\\nA simple example I like\\nto tell people about when we're talking about classification\\nis predicting fraud.\\nGenerally, instances of fraud are very rare,\\nso it's really trivial to make a model\\nthat gets high accuracy on fraud prediction.\\nYou can just predict not fraud\\nand you'll get 99.9% accurate.\\n\\nNow, is that model useful? Probably not.\\nIn fact, you probably don't even need a model to do that.\\nYou can just say nothing is fraud.\\nSo be aware of that accuracy number.\\nAgain, generally with these models,\\nthis gives us a baseline.\\nOur gut film allows us to compare models,\\nbut we want to dive in deeper\\nand often look at business metrics,\\ntie models to business metrics,\\nto see if they will work or not.\\nAnother nice thing about this model\\nis that it has those coefficients.\\n\\nI'm going to pull those out and stick those in a Panda series\\nand do a bar plot of them.\\nSimilar to the linear regression,\\nwe have both a direction and a magnitude.\\nThings that are positive are going to push things\\nto the positive prediction when those values go up.\\nSo this last sack, Lynn, if that goes up,\\nit's going to push more towards the positive.\\nWord no if that goes up,\\nthat's going to push us towards negative.\\nAnd you can see that word no and title no,\\nand regress dur tend to have\\na large impact on our model here.\\n\\nSo in this video I gave you a quick intro\\nto logistic regression with real world data.\\nMost of the concepts here shouldn't be new now\\nbecause we've been seeing the Scikit Learn interface a lot.\\nSo it's just pulling in the new model\\nand using our existing tools with the new model.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5906621\",\"duration\":12,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Construct a logistic regression model\",\"fileName\":\"3806104_en_US_04_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":20,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to construct a logistic regression model for a given real-world challenge, emphasizing predictive accuracy.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":341510,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] It's challenge time again.\\nYour challenge is to make a model\\nthat predicts whether...\\n\"},{\"urn\":\"urn:li:learningContentVideo:5909021\",\"duration\":290,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Construct a logistic regression model\",\"fileName\":\"3806104_en_US_04_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":470,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Review the optimal strategies for the challenge, reinforcing understanding, and pinpointing areas of enhancement.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":10160093,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] Okay, we're going to look at the solution\\nfor our logistic regression model here.\\nThe first thing that we need to do is load our data set.\\nSo I'm going to just find the Titanic code that I had up above,\\nand I'll just copy this whole cell.\\nLet's plop that in here\\nand make sure that that runs.\\nOkay, so here is our Titanic data set,\\nand what we're going to do is we're going to predict\\nthe survived column here from the other ones.\\n\\nSo we're going to import our logistic regression model,\\nand we'll drop any rows that have missing values.\\nSo let's actually explore that a little bit.\\nWe've seen this already, but I'll just review it here.\\nSo here's row I can say is NA,\\nthat's going to gimme this data frame of true false values.\\nI can sum that up to see the counts.\\nOkay, and so we do have missing values for age,\\nand we have a lot of missing values for cabin,\\nbut we're going to drop those.\\n\\nAnd we're going to drop these ones at the end here as well.\\nSo maybe I'll come down here\\nand I'll say that our Titanic_X is equal to,\\nwe'll take the raw.\\nAnd I'm going to select the columns that I want.\\nI'm going to do that using loc.\\nSo I'm going to say take all of the rows\\nand we'll take P class that looks good.\\nAge, siblings, and parents.\\nParents and children fair.\\nAnd we'll say drop NA after that.\\n\\nThat should get rid of the missing age ones.\\nAnd we'll do Titanic_X,\\nand we'll just look what that looks like after doing that.\\nOkay, that looks okay.\\nLet's standardize the data here.\\nSo we need to import our standard scaler,\\nand then we will make an instance of our standard scaler,\\nand then we're going to call fit transform on that.\\nOkay, let's look at X after we've done this.\\n\\nOkay, and that is a NumPy array.\\nIf I don't want a NumPy array,\\nI'm going to tell it to output a pandas data frame.\\nSo that is from sklearn.\\nI need to import the set config,\\nand we'll say transform output is pandas.\\nLet's run it again.\\nAnd I got to make sure I don't have a typo there.\\nLet's try it again.\\nOkay, so that's looking pretty good.\\n\\nLet's split our data.\\nI'll do this in the next cell here.\\nSo we're going to say we want survived\\nand we want to use the index\\nfrom X as the rows here.\\nAnd we'll we'll use loc, let's see if this works.\\nAnd let's just look at that after\\nwe've done that to validate it.\\nOkay, so I think we're pretty good.\\nIt says the length is 1045,\\nand it says the index is going to 1308.\\nThat looks like that's okay.\\n\\nSo let's split the data.\\nIt's going to be X_train and X_test,\\nand we will say train test split.\\nAnd it says that is not defined.\\nSo we got to load that.\\nSo you can see I'm relying pretty heavily\\non my AI to help me complete this.\\nDo I have to do that?\\nNo, I'm just kind of being lazy.\\nI have all this code up above,\\nbut I've just reloaded my notebook.\\nSo the existing environment was lost when I did that.\\nAnd can you use AI to do this? Possibly, right?\\nDepends on if you're in an environment\\nwhere AI is available.\\n\\nIf not, you can take the code above\\nand just adapt it so that it works.\\nI don't have problems with that.\\nSo let's see here.\\nWe need to make an instance of our logistic regression here.\\nSo let's do that.\\nI'm going to say LR is equal to logistic regression.\\nAnd then all we do is call fit\\nand we'll call lr.score.\\nLet's see what the score is.\\nThis is the percent accuracy.\\nOkay, it looks like we got 65% accurate after doing that.\\n\\nOkay, just for fun, I'm going to load the XG boost model\\nand let's do the same thing.\\nThis should be really easy.\\nAll we have to do is swap out this logistic regression\\nwith XG Boost Classifier, and the rest is the same\\nor call fit and we'll call score.\\nOkay, so in this case, it looks like our\\nlogistic regression model is actually\\ndoing better than our XG Boost Classifier.\\n\"}],\"name\":\"4. Logistic Regression\",\"size\":25536191,\"urn\":\"urn:li:learningContentChapter:5905604\"},{\"duration\":1154,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5906622\",\"duration\":252,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Decision tree algorithm\",\"fileName\":\"3806104_en_US_05_01_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":298,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to acquire foundational knowledge of decision trees for classification and regression tasks.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8457824,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this video\\nwe're going to look at decision trees.\\nLet's talk about the decision tree algorithm.\\nDecision trees are a great way to make models.\\nI really like decision trees.\\nThey have a few advantages of them.\\nOne is that you don't need to scale the data\\nbecause a decision tree is looking at\\nan individual column at a time.\\nIt's not comparing one column to another\\nat the same scale, so that doesn't matter.\\nAlso, decision trees can be intelligent\\nin that an implementation can deal with missing values\\nor categorical data, that depends on the implementation.\\n\\nFor example, XG Boost does that for us.\\nDecision tree is also explainable.\\nSo I went to the doctor and the doctor gave me a diagnosis\\nand actually showed me a decision tree\\nthat helped them lead to that diagnosis.\\nThese are some of the few advantages of a decision tree.\\nSo how do they work?\\nWhat they do is they look through all of the features,\\nall of the columns,\\nand then they decide at some threshold\\nin the value in that column,\\nwhere the best place to split that column would be\\nsuch that it best divides the positive\\nfrom the negative values.\\n\\nAnd I'm going to show an example here.\\nI need to load my anscombes data up above.\\nSo let me do that really quickly here\\nsince I restarted my notebook.\\nOkay, and let's make a decision tree.\\nSo this is a decision stump\\nand what that does is it makes one decision.\\nAnd let me visualize that here.\\nWe'll use plot tree, this is from Psychic Learn,\\nand this is what our stump looks like.\\nSo you can see for this set, this anscombes data,\\nwhat it's doing is it's saying if X is less than 0.8,\\nthen we're going to say the value is 5.79,\\notherwise the value is 8.927.\\n\\nSo it's made a single decision here\\nto give us one of two values.\\nLet's actually visualize that.\\nI'm going to plot the data and the predictions from the data\\nin a visualization.\\nAnd it's asking for numpy, and again,\\nbecause I restarted my notebook,\\nI don't have numpy here, so let me just throw that in here.\\nImportant numpy as np and let's try this again.\\nOkay, here's our anscombes data set.\\nYou can see those black dots there.\\nAnd here, the red line is the decision tree.\\n\\nBasically what it said is it can make one decision\\nbecause it's a stump\\nand it's saying that if it's less than 8.5,\\nit's this value down here.\\nIf it's greater than, it's that value above.\\nNow that doesn't look like a great fit to this line,\\nbut guess what?\\nWe can go deeper.\\nSo here I'm going to say max depth of two,\\nand we'll just plot this again.\\nAnd you can see that it's still doing the initial split\\nright here, but also now it has on the right hand side,\\nit can do another split\\nand on the left hand side it can do another split.\\n\\nSo you can imagine if we keep doing this,\\nwe can go up to max depth none.\\nThat means keep splitting\\nuntil you have pure nodes at the end\\nand we get something that looks like this.\\nYou can see it's actually matching every point on here,\\nand if you are close to one of these points in X,\\nyou will get that Y value.\\nNow this also demonstrates another feature\\nor maybe problem of a decision tree.\\n\\nIn that case, this looks like it's a little,\\nwhat we would call over fit.\\nIt's too complicated.\\nIt's going up and down.\\nAnd if you think about what's going on here with these dots,\\nwe probably would want some sort of smoother line\\ngoing on between there.\\nAnd our decision tree, because we've let it grow\\nas deep as it could, basically memorized the data.\\nWe call this over fitting in machine learning.\\nSo that's something to be aware of.\\nGenerally we want to tune these hyper parameters.\\nNow contrast that with our stump up here.\\n\\nOur stump was probably too simple,\\nso we would call this an under fit model.\\nThis is an over fit model,\\nand something like this might be an okay model.\\nThat is a good balance between those two.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5907649\",\"duration\":388,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Real-world example\",\"fileName\":\"3806104_en_US_05_02_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":614,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Discover the underlying assumptions and characteristics of decision tree algorithms.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":14515178,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] Okay, let's look at a realistic example.\\nWe're going to use the aircraft elevators dataset\\nthat we have up above.\\nLet me just load that again.\\nOkay.\\nAnd we're going to make our X and our Y.\\nLet me just load that again.\\nOkay.\\nI think I've loaded the data for elevator now.\\nLet's run this.\\nAnd it says I haven't.\\nLet's try and do that again.\\nOkay, I loaded the raw data,\\nbut I didn't pull off the pandas dataframe.\\n\\nI think that was my problem.\\nLet's go back down here and and run this now.\\nOkay, so what I've done is I've made a decision tree\\nand I've said it has a max depth of three.\\nSo max depth is a hyper parameter.\\nIt is a lever that controls the complexity of this model.\\nAnd I called fit with my X and my Y,\\nand I got my model that is trained.\\nNow, one of the nice things about a decision tree\\nis that we can look at the tree,\\nso let's look at this tree and see what's going on here.\\n\\nHere is three levels.\\nYou can see that the first thing that it's looking at\\nis this SA time one.\\nAnd then on the right hand side,\\nit's looking at SA time one again.\\nAnd then it's looking at it again.\\nThis is actually really cool if you think about it.\\nWhat it is doing is it can dive in on a single variable.\\nAnd this lets a decision tree capture non-linear behaviors.\\nIf you have a relationship with a column in X\\nthat is non-linear,\\ni.e., it might have like a bathtub curve shape,\\nor it might have like an elongated S or Z shape,\\na decision tree can capture that,\\nwhereas a linear model, like logistic regression\\nor linear regression, cannot capture that\\nunless you have encoded some non-linearity into the columns.\\n\\nSo this is actually a cool feature of decision trees.\\nLet's look at the score of our model.\\nIn this case, because this is a regressor,\\nthis is the R two score.\\nSo we are saying that 48% of the variance of the solution\\nis derived from the columns in there.\\nRemember, the R two, or coefficient of determination,\\nis generally a value between zero and one,\\nscores being close to one being better models.\\nAnd we had a linear regression model up above.\\n\\nBut again, because I've restarted my kernel,\\nI don't have it.\\nLet's just make that really quick here.\\nOkay, so it looks like I don't have that model around\\nbecause I restarted my kernel.\\nI'm going to, again, just find it here.\\nThis is the steps that you probably would have to do\\nif you came back to your kernel.\\nSo we just came back to our kernel\\nand we can see that we need this right here,\\nand we need to make this linear model down here,\\nwhich is this right here.\\nThe code is actually the same,\\nso it should be relatively easy to just throw that in here.\\n\\nLet's do that.\\nSo I'm going to say L-R-E-L-E-V is equal to linear regression.\\nAnd we're going to fit our model.\\nOkay, let's run that.\\nAnd our linear regression model is .81.\\nSo what is that telling us?\\nIt's telling us that our linear regression model\\nhas a higher coefficient of determination,\\nor it's a better model, than this decision tree.\\nNow, remember, up above here,\\nwe made a decision tree of depth three.\\n\\nMaybe that's not the appropriate depth.\\nAnd so, what I'm going to do is I'm going to loop over the depths\\nand I'm just going to make different depth trees.\\nI'll start from a stump and I'll go up to 19.\\nAnd I'll keep track of my score and plot my score over time.\\nOkay, so here we go.\\nWe can see that as I add more depth to this,\\nas I get closer to 19, my score gets better.\\nSo we might think,\\nokay, well why don't we just make the tree go to 19?\\nWell, if you look at this, we are fitting it\\nand we're scoring it on the same data,\\nso we're kind of cheating.\\n\\nAnd if you remember what we talked about\\njust in the previous video,\\na decision tree has the ability to memorize your data.\\nAnd so, what we want to do is we want to split our data\\ninto a training set and a testing set.\\nSo we're going to use train test split to do that\\nso that we evaluate it on data that it hasn't seen.\\nLet's do that and then we'll run this here.\\nWhat this is going to do is it's going to plot a visualization\\nof our score on the training data, which is this blue line,\\nand then a plot of our scores on the testing data,\\nwhich is the orange line.\\n\\nAnd this is pretty interesting.\\nWe call this a validation curve.\\nAs we're tweaking the values of a hyper parameter,\\nwe can look at how our model does.\\nAnd it is interesting\\nthat the blue line goes up close to one,\\nbut we're not really caring too much about that blue line,\\nwhat we're caring about is this orange line,\\nhow the model is doing on data that it hasn't seen before.\\nAnd it looks like somewhere around here, maybe 10 or 11,\\nit maxes out and then it starts going down.\\n\\nSo at this point, we could probably say,\\nto the left of this, the model is under fit,\\nand to the right of it, it is over-fitting.\\nWhat does that mean, over-fitting?\\nIt means that it's memorizing the noise basically\\nin the training data, such that when it sees data\\nthat it hasn't seen before,\\nit can't regularize or adapt to that,\\nand so it does poorly.\\nDown here on the left hand side, it is too simple,\\nso it's not able to explain very well\\ndata that it hasn't seen before.\\n\\nSo let's make a new model with depth 11\\nand look at what our score is,\\nand our score goes up to 70, or .7.\\nSo we can compare that\\nwith our model from linear regression\\nand we see that our linear regression model is .81.\\nSo really what we should do\\nis make a linear regression model\\nand evaluate it on data that it hasn't seen.\\nSo let's train it on the training data,\\nand then we'll evaluate it on data that it hasn't seen,\\nand this will give us\\na more apples to apples comparison here.\\n\\nAnd we're still seeing a score around .81.\\nSo in this case, it looks like linear regression\\nis doing a better job with this data than our decision tree.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5905602\",\"duration\":249,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Random Forest and XGBoost\",\"fileName\":\"3806104_en_US_05_03_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":401,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to implement a decision tree on a complex dataset, deriving insights, and predictions.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":7842487,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- [Instructor] In this video,\\nI want to talk about Random Forests and XGBoost.\\nWe saw in the previous video\\nthat our decision trees have the ability\\nto overfit and also underfit.\\nWe can make a stump that's too simple,\\nwe can make a decision tree that can go as deep as it wants,\\nbut that's memorizing,\\nit's getting too complicated.\\nSo one of the ways that we can remove that complication\\nor that tendency to memorize and make it adapt better\\nis to use a few tricks.\\nAnd one of those tricks is what is called doing bagging.\\n\\nThe idea behind bagging with a decision tree\\nis we're going to take a subset of the columns,\\nand we're going to train a model on a subset,\\nand then we'll train another model on a different subset\\nand repeat that multiple times.\\nAnd then we'll let each of those different trees\\nthat's looking at different aspects of the data\\nbasically vote,\\nand we'll take the vote or the aggregation of those.\\nWe actually call that a Random Forest.\\nIt's randomly looking at different subsets of columns.\\n\\nYou can also make it\\nlook at different subsets of features as well.\\nLet's run this.\\nI'm going to say let's import a Random Forest regressor.\\nThis is from the ensemble module.\\nIn scikit-learn, ensemble means combining a bunch of models.\\nSo you can think of this as making a bunch of small trees\\nthat look at different parts and then combining them.\\nSo in this case,\\na hyperparameter is the number of estimators,\\nthat's the number of trees,\\nand we're limiting the depth of all of them to 3.\\nIf we look at the score of this,\\nwe can see that the score of this is 0.528.\\n\\nNow, we can do the same thing here.\\nWe can say, okay,\\nlet's leave the number of estimators at 100,\\nand let's change the depth here\\nand plot that and look what happens when we plot it.\\nThis takes a while to run\\nbecause it's making 20 different models.\\nOkay, here's the result.\\nLet's look at this plot.\\nAgain, we see the blue line that's creeping up towards 1.\\nWe're actually not super concerned with the blue line.\\nWhat we're more concerned with is the orange line,\\nand it's a little bit hard to see here,\\nbut it looks, again, somewhere around 10, 11, 12.\\n\\nIt's sort of flattening out.\\nSo a rule of thumb when I'm making models\\nis if I have multiple models that are basically the same,\\nI will take the simpler model.\\nIn this case, if I have a depth that is shallower,\\nthat is a simpler model.\\nSo I'm probably going to go around here,\\n11, 12, maybe 13 for my model.\\nLet's try a depth of 13,\\nand look what that score looks like.\\nAnd we get a score of 0.82.\\n\\nSo, again, remember previously we had a score of 0.81\\nwith our linear regression model,\\nand now with our decision tree,\\nwe're getting a score of 0.82.\\nOkay, I'm also going to make a XGBoost regressor.\\nSo this is using another ensembling technique.\\nRandom Forest used what we call bagging.\\nXGBoost uses what we call boosting.\\nThe idea with this is that you make a decision tree,\\nand then you make another decision tree\\nthat corrects the previous decision tree's error.\\n\\nI like to compare this to golf.\\nIf you're thinking about golfing,\\na decision tree is like being able to hit the ball once,\\nand you're so far away from the hole\\nafter you hit the ball once.\\nA Random Forest is like you and five of your best friends\\nor 40 of your best friends\\nall get a chance to hit the ball once,\\nand you take the average of where your ball lands\\ncompared to where the hole is.\\nThat's probably going to be better than a single hit,\\ndepending on who your friends are.\\nA XGBoost regressor is like,\\nI hit the ball once, then I hit it again,\\nthen I hit it again,\\nand I can change my swings each time.\\n\\nIf you think about that analogy,\\nwhich is kind of what's going on here,\\nXGBoost should be able to do a better job.\\nLet's try it out and see.\\nAnd it looks like indeed here it does.\\nIt gets 0.87 on this model.\\nAgain, the nice thing about this,\\nthese are all using the same interface.\\nSo once you've got your data in a place\\nwhere it can work with a decision tree or linear.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5907650\",\"duration\":25,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Challenge: Design a decision tree model\",\"fileName\":\"3806104_en_US_05_04_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":29,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to design a decision tree model for a specific real-world problem, emphasizing depth, and feature selection.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":666605,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] Okay, now it's time\\nfor you to try out decision trees.\\nAgain, this should be relatively easy.\\nAll you have to do is swap out the previous code\\nand use a decision tree instead.\\nI want you to see\\nif you can determine the optimal depth of the tree\\nusing a validation curve, like we did up.\\n\"},{\"urn\":\"urn:li:learningContentVideo:5904640\",\"duration\":240,\"visible\":true,\"requiredForCertificateOfCompletion\":true,\"name\":\"Solution: Design a decision tree model\",\"fileName\":\"3806104_en_US_05_05_VT\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":443,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"Learn how to evaluate the best approach to the challenge, solidifying understanding, and improving modeling strategies.\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":8452753,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"(upbeat music)\\n- [Instructor] Okay, we're going to make a decision\\ntree classifier.\\nLet's load that module here.\\nSo I'm going to say from SK learn\\nand it's tree import decision tree classifier.\\nAnd let's just look at our raw data here.\\nHere's our raw data.\\nWe want to predict the survived column from the other ones.\\nSo I'm going to say Titanic X is equal to raw\\nand I want to get certain columns in here.\\n\\nI want P class that looks good, age looks good,\\nsiblings, parents ticket.\\nI don't have ticket in there.\\nI believe ticket is actually not numeric.\\nSo I'm okay with that.\\nAnd let's drop the missing values from that\\nand we'll look at Titanic X after doing that.\\nOkay, that looks okay, again,\\nbecause this is a decision tree, we don't need to\\nstandardize the data.\\n\\nIf we wanted to compare it to logistic regression,\\nwe might want to do that,\\nbut in this case, I'm not going to.\\nSo I'm going to say for our Y, we're going\\nto take the survived column.\\nSo I'm going to come down here and say pull off survived.\\nAnd then if I do this\\nbecause I dropped A on the X, I need\\nto have corresponding rows here.\\nSo I'm going to say loc here,\\nand I'm going to Titanic X index here in loc\\nto get those so that they're corresponding here.\\n\\nOkay, so I think that's pretty good.\\nI'm going to say DT is equal to,\\nand I'm going to say decision tree.\\nActually, I'm going to split my data first.\\nSo let's split our data, so that should be good.\\nWe'll run that, let's just look at what our data looks like\\nafter we've done that.\\nOkay, so that's looking like it split.\\nI'm going to come up here and take this code up here\\nand I'll copy this and paste it down here.\\n\\nSo instead of a random forest here,\\nI'm going to say decision tree classifier.\\nAnd we don't have an estimator.\\nThat's not a hyper parameter there this says RF here.\\nSo maybe I'll just change that to DT\\nand let's run this.\\nOkay, and here we go.\\nIt looks like our depth at this is, I believe this is three,\\nis probably giving us our best model here.\\n\\nNow, one of the things you might be concerned about is like,\\nwhy is this going up and down?\\nWhy isn't it monotonically increasing and decreasing?\\nThat's probably our train test split has different subsets\\nin it that perform better at different model sizes.\\nSo you could change where we split on,\\nwe said random state 42.\\nIf you change this to 43\\nand we reran this, this might look a little bit different.\\nOkay? And it looks like it's still at three the best here.\\n\\nAnother thing you can do is you can do what's called k-fold\\ncross validation.\\nBasically what that is doing is instead\\nof just reserving some chunk for testing,\\nit splits up the data into some number of chunks,\\ntypically something like five.\\nAnd you loop over and hold out each of those chunks\\nand then aggregate those values to see overall on all\\nof your data, how is your model performing\\nwith given hyper parameters?\\nAnd that's looking probably\\nfor this model, this decision tree.\\n\\nI would do a depth of three for our Titanic model.\\n\"}],\"name\":\"5. Decision Trees\",\"size\":39934847,\"urn\":\"urn:li:learningContentChapter:5902655\"},{\"duration\":86,\"entries\":[{\"urn\":\"urn:li:learningContentVideo:5902654\",\"duration\":86,\"visible\":true,\"requiredForCertificateOfCompletion\":null,\"name\":\"Next steps\",\"fileName\":\"3806104_en_US_06_01_LA30\",\"demo\":false,\"videoCreationMetadata\":{\"hasSlides\":false,\"assignedBy\":\"urn:li:member:-1\",\"assignedTo\":\"urn:li:member:-1\",\"editingNotes\":\"Editors, this is a workspace cam video. Please add URL overlay https://store.metasnake.com/xgboost at 00:51 and overlay https://www.linkedin.com/in/panela/ at 01:10. No need for additional visuals, some punch in would be sufficient if necessary.\",\"solutionVideo\":false,\"challengeVideo\":false,\"includesPickups\":false,\"graphicsIncluded\":false,\"rawDurationSeconds\":105,\"handoutGraphicsIncluded\":false,\"includesAlternateFootage\":false},\"description\":\"\",\"captionsStatus\":\"AVAILABLE\",\"cdnStatus\":\"AVAILABLE\",\"size\":5452973,\"solution\":false,\"welcomeContent\":null,\"challenge\":false,\"assetStatus\":\"COMPLETE\",\"transcript\":\"- Congratulations on completing the course.\\nYou might wonder about the next steps to solidify\\nand expand your newly acquired machine learning skills.\\nFirst and foremost, I can't emphasize enough\\nthe importance of practicing what you've learned.\\nMachine learning, like any skill,\\nrequires practice to deepen your understanding.\\nConsider working with data sets from your own projects,\\nor explore online repositories for data sets\\nto apply the algorithms we've discussed.\\nSecondly, for those looking to further their knowledge,\\nI recommend my book \\\"Effective XGBoost.\\\"\\nIt's designed to bridge the gap between theoretical concepts\\nand real-world application, offering advanced techniques,\\ncode examples, and practical exercises.\\n\\nThis book provides end-to-end coverage\\nof classification models with XGBoost.\\nAdditionally, I encourage you to connect with me\\non LinkedIn.\\nI regularly post updates, code snippets,\\nand insights on the latest in machine learning,\\nPython programming, and data science.\\nFinally, I urge you to apply these concepts and techniques\\nyou've learned to your own data projects.\\nHands-on application is crucial\\nfor mastering machine learning.\\nWhether it's predicting customer behavior,\\nanalyzing text data, forecasting trends,\\nthe skills you've gained in this course are valuable\\nand applicable across many domains.\\n\\nThanks for embarking on this journey with me.\\nI wish you all the best\\nin your future projects and endeavors.\\n\"}],\"name\":\"Conclusion\",\"size\":5452973,\"urn\":\"urn:li:learningContentChapter:5903621\"}],\"size\":250974119,\"duration\":7083,\"zeroBased\":false}]}"