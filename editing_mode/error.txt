(petroff) bianders@bianders-mn7180 ~/Brian_Code/Mentor/editing_mode % python 3_reACT_prompting.py
Generating curriculum for Data Science with Python
Model: claude-3-5-sonnet-20240620   Query: [Message(role='system', content="You are an experienced Learning and Development (L&D) professional with extensive experience across various industrie
Traceback (most recent call last):
  File "/Users/bianders/Brian_Code/Mentor/editing_mode/3_reACT_prompting.py", line 77, in <module>
    c = Mentor(topic)
        ^^^^^^^^^^^^^
  File "/Users/bianders/Brian_Code/Mentor/Mentor.py", line 305, in Mentor
    ideal_curriculum = lnd_curriculum(topic)
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/bianders/Brian_Code/Mentor/Mentor.py", line 235, in lnd_curriculum
    response = chain.run(messages=messages, input_variables={"topic": topic})
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/bianders/Brian_Code/Chain/chain/chain.py", line 49, in run
    result = self.run_messages(
             ^^^^^^^^^^^^^^^^^^
  File "/Users/bianders/Brian_Code/Chain/chain/chain.py", line 76, in run_messages
    result = self.model.query(messages, verbose=verbose)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/bianders/Brian_Code/Chain/model/model.py", line 106, in query
    return self._client.query(self.model, input, pydantic_model)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/bianders/Brian_Code/Chain/model/clients/anthropic_client.py", line 57, in query
    f"Input not recognized as a valid input type: {type:input}: {input}"
                                                  ^^^^^^^^^^^^
TypeError: unsupported format string passed to type.__format__
(petroff) bianders@bianders-mn7180 ~/Brian_Code/Mentor/editing_mode %


Look at my code below and help my troubleshoot the error above.

from Chain import Prompt, Model, Chain, MessageStore
from review_certificates import (
    review_curriculum,
    learner_progression,
    create_curriculum_text_for_review,
)
from Mentor import Mentor

Chain.message_store = MessageStore(log_file="log.json")

react_prompt = """
You are an AI agent tasked with improving a curriculum curation for a skill-based learning program. 
Your goal is to analyze the current curation, consider the critique provided, and make improvements 
by potentially adding, removing, or replacing courses.

Here is the current curation object:
<curation_object>
{{CURATION_OBJECT}}
</curation_object>

Here is the current curriculum (detailed table of contents for each course):
<curriculum>
{{CURRICULUM}}
</curriculum>

Here is a critique of the current curation:
<critique>
{{CRITIQUE}}
</critique>

You have access to the following function:

Get_alternative_courses(description: str) -> List[str]
This function takes a brief description (1-2 sentences) of desired course content and returns a list
of 5 alternative course titles with their table of contents.

To improve the curation, follow these steps using the ReACT framework:

1. Thought: Analyze the current curation, curriculum, and critique. Identify areas for improvement.
2. Action: Decide on a course of action (e.g., remove a course, add a new course, replace a course).
3. Function Call: If adding or replacing a course, use the Get_alternative_courses function to find 
options.
4. Observation: Review the results of the function call.
5. Repeat steps 1-4 as needed until you have made all necessary improvements.

For each step, use the following format:
<thought>Your thought process</thought>
<action>Your chosen action</action>
<function_call>Get_alternative_courses(description="Your description")</function_call>
<observation>Results of the function call</observation>

Once you have finished improving the curation, provide your final output in this format:
<improved_curation>
{
  "topic": "Updated topic if changed",
  "course_titles": [
    "Updated list of course titles"
  ]
}
</improved_curation>
<explanation>
Briefly explain the changes you made and why they improve the curation based on the critique and 
your analysis.
</explanation>
"""

"""
We need to parse the response to grab "<improved_curation>" or "Get_alternative_courses".

ReACT is not quite right here, how we define the fuction.

"""

if __name__ == "__main__":
    topic = "Data Science with Python"
    print("Generating curriculum for", topic)
    c = Mentor(topic)
    print("Reviewing curriculum for", topic)
    critique = review_curriculum(c, "Data Scientists")
    print("
=========
critique
=========
", critique)
    curriculum = create_curriculum_text_for_review(c)
    print("
=========
curriculum
=========
", curriculum)
    m = Model("gpt")
    p = Prompt(react_prompt)
    chain = Chain(p, m)
    response = chain.run(
        input_variables={
            "CURRICULUM": topic,
            "CURATION_OBJECT": c,
            "CRITIQUE": critique,
        }
    )
    print("
=========
response
=========
", response.content)

Here's another module referenced:

"""
A Chain is a convenience wrapper for models, prompts, parsers, messages, and response objects.
A chain needs to have at least a prompt and a model.
Chains are immutable, treat them like tuples.
This used to be a monolith, but now I've separated out various classes, and created Message as a new one.
"""

import time  # for timing our query calls (saved in Response object)

# The rest of our package.
from ..prompt.prompt import Prompt
from ..model.model import Model
from ..response.response import Response
from ..parser.parser import Parser
from ..message.message import Message
from ..message.messagestore import MessageStore


class Chain:
    """
    How we chain things together.
    Instantiate with:
    - a prompt (a string that is ready for jinja2 formatting),
    - a model (a name of a model (full list of accessible models in Model.models))
    - a parser (a function that takes a string and returns a string)
    """

    # Allow the possibility of a MessageStore to be stored at the class level.
    _message_store = None

    def __init__(self, prompt: Prompt, model: Model, parser: Parser = None):
        self.prompt = prompt
        self.model = model
        self.parser = parser
        self.input_schema = self.prompt.input_schema()  # this is a set
        # self.output_schema = {'result'}                   # could be useful to define this in future

    def run(self, input_variables: dict = None, verbose=True, messages=[]):
        """
        Input should be a dict with named variables that match the prompt.
        """
        # Render our prompt with the input_variables if variables are passed. Should throw a jinja error if it doesn't match.
        if input_variables:
            prompt = self.prompt.render(input_variables=input_variables)
        else:
            prompt = self.prompt.prompt_string
        # Route input; if string, if message
        if messages:
            result = self.run_messages(
                prompt=prompt, messages=messages, verbose=verbose
            )
        else:
            result = self.run_completion(prompt=prompt, verbose=verbose)
        return result

    def run_messages(self, prompt: str, messages, verbose=True):
        """
        Special version of Chain.run that takes a messages object.
        Converts input + prompt into a message object, appends to messages list, and runs to Model.chat.
        Input should be a dict with named variables that match the prompt.
        """
        # Add new query to messages list
        message = {"role": "user", "content": prompt}
        messages.append(message)
        # If we have class-level logging
        if Chain._message_store:
            Chain._message_store.add(messages)
        # Run our query
        time_start = time.time()
        if self.parser:
            result = self.model.query(
                messages, verbose=verbose, pydantic_model=self.parser.pydantic_model
            )
            # result = json.dumps(result.__dict__)
        else:
            result = self.model.query(messages, verbose=verbose)
        time_end = time.time()
        duration = time_end - time_start
        # Convert result to a string
        assistant_message = {"role": "assistant", "content": result}
        # If we have class-level logging
        if Chain._message_store:
            Chain._message_store.add(assistant_message)
        messages.append(assistant_message)
        # Return a response object
        response = Response(
            content=result,
            status="success",
            prompt=prompt,
            model=self.model.model,
            duration=duration,
            messages=messages,
            variables=input,
        )
        return response

    def run_completion(self, prompt: str, verbose=True):
        """
        Standard version of Chain.run which returns a string (i.e. a completion).
        Input should be a dict with named variables that match the prompt.
        """
        time_start = time.time()
        user_message = Message(role="user", content=prompt)
        # If we have class-level logging
        if Chain._message_store:
            Chain._message_store.add(user_message)
        if self.parser:
            result = self.model.query(
                prompt, verbose=verbose, pydantic_model=self.parser.pydantic_model
            )
        else:
            result = self.model.query(prompt, verbose=verbose)
        time_end = time.time()
        duration = time_end - time_start
        # Create a new messages object, to be passed to Response object.
        assistant_message = Message(role="assistant", content=result)
        # If we have class-level logging
        if Chain._message_store:
            Chain._message_store.add(assistant_message)
        new_messages_object = [user_message, assistant_message]
        response = Response(
            content=result,
            status="success",
            prompt=prompt,
            model=self.model.model,
            duration=duration,
            messages=new_messages_object,
            variables=input,
        )
        return response

    def __repr__(self) -> str:
        """
        Standard for all of my classes; changes how the object is represented when invoked in interpreter.
        """
        attributes = ", ".join(
            [f"{k}={repr(v)[:50]}" for k, v in self.__dict__.items()]
        )
        return f"{self.__class__.__name__}({attributes})"
        # Example output: Chain(prompt=Prompt(string='tell me about {{topic}}', format_in, model=Model(model='mistral'), parser=Parser(parser=<function Chain.<lambda> at 0x7f7c5a

And this code:

"""
Modularized version of Model class.

NEXT BIG THING TO DO:
- [x] lazy load should happen on model object initialization, not on query.
- 
"""

from pathlib import Path
import importlib
import json
import itertools

dir_path = Path(__file__).resolve().parent


class Model:
	# Some class variables: models, context sizes, clients
	# Load models from the JSON file. Why classmethod and property?
	# Because models is a class-level variable (Model.models, not model.models).
	# We want it to dynamically load the models from the models file everytime you access the attribute, because Ollama models can change.
	@classmethod
	@property
	def models(cls):
		with open(dir_path / "clients/models.json") as f:
			return json.load(f)

	# Store lazy-loaded client instances at the class level
	_clients = {}

	def __init__(self, model: str = "gpt-4o"):
		self.model = self._validate_model(model)
		self._client_type: tuple[str, str] = self._get_client_type(self.model)
		# Add client loading logic
		self._client = self.__class__._get_client(self._client_type)

	def _validate_model(cls, model: str) -> str:
		"""
		This is where you can put in any model aliases you want to support.
		"""
		# Load our aliases from aliases.json
		with open(dir_path / "aliases.json", "r") as f:
			aliases = json.load(f)
		# Check data quality.
		for value in aliases.values():
			if value not in list(itertools.chain.from_iterable(cls.models.values())):
				raise ValueError(
					f"WARNING: This model declared in aliases.json is not available: {value}."
				)
		# Assign models based on aliases
		if model in aliases.keys():
			model = aliases[model]
		elif model in list(
			itertools.chain.from_iterable(cls.models.values())
		):  # any other model we support (flattened the list)
			model = model
		else:
			ValueError(
				f"WARNING: Model not found locally: {model}. This may cause errors."
			)
		return model

	def _get_client_type(self, model: str) -> str:
		"""
		Setting client_type for Model object is necessary for loading the correct client in the query functions.
		Returns a tuple with client type (which informs the module title) and the client class name (which is used to instantiate the client).
		"""
		model_list = self.__class__.models
		if model in model_list["openai"]:
			return "openai", "OpenAIClient"
		elif model in model_list["anthropic"]:
			return "anthropic", "AnthropicClient"
		elif model in model_list["google"]:
			return "google", "GoogleClient"
		elif model in model_list["ollama"]:
			return "ollama", "OllamaClient"
		elif model in model_list["groq"]:
			return "groq", "GroqClient"
		elif model in model_list["testing"]:
			return "testing", "TestingClient"
		else:
			raise ValueError(f"Model {model} not found in models")

	@classmethod
	def _get_client(cls, client_type: str):
		# print(f"client type: {client_type}")
		if client_type[0] not in cls._clients:
			try:
				module = importlib.import_module(
					f"Chain.model.clients.{client_type[0].lower()}_client"
				)
				client_class = getattr(module, f"{client_type[1]}")
				cls._clients[client_type[0]] = client_class()
			except ImportError as e:
				raise ImportError(f"Failed to import {client_type} client: {str(e)}")
		return cls._clients[client_type[0]]

	def query(
		self,
		input: "str | list",
		verbose: bool = True,
		pydantic_model: "BaseModel" = None,
	) -> "BaseModel | str":
		if verbose:
			print(f"Model: {self.model}   Query: " + self.pretty(str(input)))
		return self._client.query(self.model, input, pydantic_model)

	async def query_async(
		self,
		input: "str | list",
		verbose: bool = True,
		pydantic_model: "BaseModel" = None,
	) -> "BaseModel | str":
		if self._client is None:
			self._client = self._get_client(self._client_type)
		if verbose:
			print(f"Model: {self.model}   Query: " + self.pretty(str(input)))
		return await self._client.query_async(self.model, input, pydantic_model)

	def pretty(self, user_input):
		pretty = user_input.replace("
", " ").replace("	", " ").strip()
		return pretty[:150]

	def __repr__(self):
		attributes = ", ".join(
			[f"{k}={repr(v)[:50]}" for k, v in self.__dict__.items()]
		)
		return f"{self.__class__.__name__}({attributes})"

And finally this:

"""
Client subclass for Anthropic models.
"""

from .client import Client
from ...message.message import Message
from anthropic import Anthropic
import instructor
from pydantic import BaseModel
import os
from dotenv import load_dotenv
from pathlib import Path

root_dir = Path(__file__).resolve().parent.parent.parent
load_dotenv(dotenv_path=root_dir / ".env")


class AnthropicClient(Client):
    def __init__(self):
        self._client = self._initialize_client()

    def _initialize_client(self):
        """
        We use the Instructor library by default, as this offers a great interface for doing function calling and working with pydantic objects.
        """
        anthropic_client = Anthropic(api_key=self._get_api_key())
        return instructor.from_anthropic(anthropic_client)

    def _get_api_key(self):
        if os.getenv("ANTHROPIC_API_KEY") is None:
            raise ValueError("No ANTHROPIC_API_KEY found in environment variables")
        else:
            return os.getenv("ANTHROPIC_API_KEY")

    def query(
        self, model: str, input: "str | list", pydantic_model: BaseModel = None
    ) -> "str | BaseModel":
        """
        Handles all synchronous requests from Anthropic's models.
        Possibilities:
        - pydantic object not provided, input is string -> return string
        - pydantic object provided, input is string -> return pydantic object
        Anthropic is quirky about system messsages (The Messages API accepts a top-level "system" parameter, not "system" as an input message role.)
        """
        # Anthropic requires a system variable
        system = ""
        if isinstance(input, str):
            input = [{"role": "user", "content": input}]
        elif isinstance(input, Message):
            input = input
            # This is anthropic quirk; we remove the system message and set it as a query parameter.
            if input[0]["role"] == "system":
                system = input[0]["content"]
                input = input[1:]
        else:
            raise ValueError(
                f"Input not recognized as a valid input type: {type:input}: {input}"
            )
        # set max_tokens based on model
        if model == "claude-3-5-sonnet-20240620":
            max_tokens = 8192
        else:
            max_tokens = 4096
        # call our client
        response = self._client.chat.completions.create(
            # model = self.model,
            model=model,
            max_tokens=max_tokens,
            max_retries=0,
            system=system,
            messages=input,
            response_model=pydantic_model,
        )
        # only two possibilities here
        if pydantic_model:
            return response
        else:
            return response.content[0].text

    async def query_async(
        self, model: str, input: "str | list", pydantic_model: "BaseModel" = None
    ) -> "BaseModel | str":
        # Implement asynchronous query logic here
        # This would be similar to the synchronous version but using async calls
        pass
